------- JOB Configuration ---------
scontrol show job 24297793
JobId=24297793 JobName=g4
   UserId=shaima0d(174988) GroupId=g-shaima0d(1174988) MCS_label=N/A
   Priority=9694 Nice=0 Account=a100_training_acc QOS=a100_training_qos
   JobState=RUNNING Reason=None Dependency=(null)
   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=00:00:01 TimeLimit=00:15:00 TimeMin=N/A
   SubmitTime=2023-03-17T14:27:50 EligibleTime=2023-03-17T14:27:50
   AccrueTime=2023-03-17T14:27:50
   StartTime=2023-03-17T14:27:51 EndTime=2023-03-17T14:42:51 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2023-03-17T14:27:51 Scheduler=Main
   Partition=a100_training AllocNode:Sid=login510-27:152451
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=gpu108-16-r
   BatchHost=gpu108-16-r
   NumNodes=1 NumCPUs=16 NumTasks=4 CPUs/Task=4 ReqB:S:C:T=0:0:*:*
   TRES=cpu=16,mem=32G,node=1,billing=16,gres/gpu=4
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
   MinCPUsNode=4 MinMemoryCPU=2G MinTmpDiskNode=0
   Features=(a100)&el7 DelayBoot=00:00:00
   Reservation=A100
   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
   Command=/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/G4N1.slurm
   WorkDir=/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed
   StdErr=/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/slurm-24297793.out
   StdIn=/dev/null
   StdOut=/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/slurm-24297793.out
   Power=
   TresPerJob=gres:gpu:4
   TresPerNode=gres:gpu:4
   

------- GPU Configuration ---------
nvidia-smi -L
GPU 0: NVIDIA A100-SXM4-80GB (UUID: GPU-db3b8555-6232-72c7-07c7-c31184289fd8)
GPU 1: NVIDIA A100-SXM4-80GB (UUID: GPU-82dd53a9-3f96-eb73-875f-3320460f81ee)
GPU 2: NVIDIA A100-SXM4-80GB (UUID: GPU-caebc372-ac54-a639-d9ac-103582552abd)
GPU 3: NVIDIA A100-SXM4-80GB (UUID: GPU-54e23bbb-3354-0380-105b-ac45ed7325d3)
------- NVLink Configuration ------
nvidia-smi topo -m
	[4mGPU0	GPU1	GPU2	GPU3	mlx5_0	mlx5_1	CPU Affinity	NUMA Affinity[0m
GPU0	 X 	NV4	NV4	NV4	SYS	SYS	0-15	0-1
GPU1	NV4	 X 	NV4	NV4	PHB	SYS	0-15	0-1
GPU2	NV4	NV4	 X 	NV4	SYS	SYS	0-15	0-1
GPU3	NV4	NV4	NV4	 X 	SYS	PHB	0-15	0-1
mlx5_0	SYS	PHB	SYS	SYS	 X 	SYS		
mlx5_1	SYS	SYS	SYS	PHB	SYS	 X 		

Legend:

  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing at most a single PCIe bridge
  NV#  = Connection traversing a bonded set of # NVLinks
------- Infiniband Configuration --
ibv_devinfo
hca_id:	mlx5_0
	transport:			InfiniBand (0)
	fw_ver:				20.34.1002
	node_guid:			88e9:a4ff:ff1a:ae08
	sys_image_guid:			88e9:a4ff:ff1a:ae08
	vendor_id:			0x02c9
	vendor_part_id:			4123
	hw_ver:				0x0
	board_id:			MT_0000000451
	phys_port_cnt:			1
		port:	1
			state:			PORT_ACTIVE (4)
			max_mtu:		4096 (5)
			active_mtu:		4096 (5)
			sm_lid:			1
			port_lid:		222
			port_lmc:		0x00
			link_layer:		InfiniBand

hca_id:	mlx5_1
	transport:			InfiniBand (0)
	fw_ver:				20.34.1002
	node_guid:			88e9:a4ff:ff1a:9e74
	sys_image_guid:			88e9:a4ff:ff1a:9e74
	vendor_id:			0x02c9
	vendor_part_id:			4123
	hw_ver:				0x0
	board_id:			MT_0000000451
	phys_port_cnt:			1
		port:	1
			state:			PORT_ACTIVE (4)
			max_mtu:		4096 (5)
			active_mtu:		4096 (5)
			sm_lid:			1
			port_lid:		214
			port_lmc:		0x00
			link_layer:		InfiniBand

Loading module for CUDA 11.7.0
CUDA 11.7.0 is now loaded
GNU 11.1.0 is now loaded
Loading module for nccl-2.17.1.1
nccl-2.17.1.1 modules now loaded
Loading module for gdrcopy 2.0_cuda11.7.0
gdrcopy 2.0_cuda11.7.0 modules now loaded
Loading module for ucx-gpu 1.14.0
ucx-gpu 1.14.0 modules now loaded
Loading module for OPENMPI 4.1.4
OPENMPI 4.1.4 modules now loaded
Loading module for pytorch-1.13.1_cuda11.7.0
pytorch-1.13.1_cuda11.7.0 modules now loaded
Loading module for deepspeed-0.8.3
deepspeed-0.8.3 modules now loaded
Loading module for apex-22.03
apex-22.03 modules now loaded
Currently Loaded Modulefiles:
  1) dl/2023             5) nccl/2.17.1.1       9) pytorch/1.13.1
  2) python/3.9.16       6) gdrcopy/2.0        10) deepspeed/0.8.3
  3) cuda/11.7.0         7) ucx/1.14.0         11) apex/22.03
  4) gcc/11.1.0          8) openmpi-gpu/4.1.4
######################################################################
#     ___       __          _          ____        __                #
#    /   | ____/ /___ ___  (_)___     / __ \____  / /_  __           #
#   / /| |/ __  / __ `__ \/ / __ \   / / / / __ \/ / / / /           #
#  / ___ / /_/ / / / / / / / / / /  / /_/ / / / / / /_/ /            #
# /_/  |_\__,_/_/ /_/ /_/_/_/ /_/   \____/_/ /_/_/\__, /             #
#                                                /____/              #
#                                                                    #
# Access granted to Administers Only.                                #
#                                                                    #
# Please be aware that jobs maybe running.                           #
#                                                                    #
#                                             - Your Ibex Admin Team #
#                                              ibex@hpc.kaust.edu.sa #
#                              https://kaust-ibex.slack.com #general #
######################################################################
[2023-03-17 14:27:55,387] [INFO] [runner.py:454:main] Using IP address of 10.109.8.92 for node gpu108-16-r
[2023-03-17 14:27:55,387] [INFO] [runner.py:550:main] cmd = /sw/csgv/dl/apps/python/3.9.16/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJncHUxMDgtMTYtciI6IFswLCAxLCAyLCAzXX0= --master_addr=10.109.8.92 --master_port=29500 --no_local_rank --enable_each_rank_log=None pretrain_gpt.py --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 1 --hidden-size 12288 --num-attention-heads 96 --seq-length 1024 --loss-scale 15 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 16 --train-iters 50 --lr 6.0e-5 --min-lr 6.0e-6 --lr-decay-style cosine --log-interval 1 --eval-iters 40 --eval-interval 1000 --data-path /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document --vocab-file /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json --merge-file /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt --save-interval 1000 --split 98,2,0 --clip-grad 1.0 --weight-decay 0.1 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.006 --fp16 --checkpoint-activations --tensorboard-dir ds_z_off-none_stage_3_nl1_hs12288_mb4_seq1024_gb16_nodes1 --deepspeed-activation-checkpointing --zero-stage=3 --deepspeed_config=ds_config.json --no-pipeline-parallel --deepspeed --exit-interval 5000
[2023-03-17 14:27:56,998] [INFO] [launch.py:135:main] 0 NCCL_SOCKET_IFNAME=ib0
[2023-03-17 14:27:56,998] [INFO] [launch.py:135:main] 0 NCCL_TREE_THRESHOLD=0
[2023-03-17 14:27:56,998] [INFO] [launch.py:135:main] 0 NCCL_HOME=/sw/csgv/dl/apps/nccl/2.17.1.1
[2023-03-17 14:27:56,998] [INFO] [launch.py:135:main] 0 NCCL_TOPO_DUMP_FILE=./nccl_dump.log.g4
[2023-03-17 14:27:56,998] [INFO] [launch.py:135:main] 0 NCCL_DEBUG=INFO
[2023-03-17 14:27:56,998] [INFO] [launch.py:135:main] 0 NCCL_NET_GDR_LEVEL=4
[2023-03-17 14:27:56,998] [INFO] [launch.py:142:main] WORLD INFO DICT: {'gpu108-16-r': [0, 1, 2, 3]}
[2023-03-17 14:27:56,998] [INFO] [launch.py:148:main] nnodes=1, num_local_procs=4, node_rank=0
[2023-03-17 14:27:56,998] [INFO] [launch.py:161:main] global_rank_mapping=defaultdict(<class 'list'>, {'gpu108-16-r': [0, 1, 2, 3]})
[2023-03-17 14:27:56,998] [INFO] [launch.py:162:main] dist_world_size=4
[2023-03-17 14:27:56,998] [INFO] [launch.py:164:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m[93m [WARNING] [0m async_io: please install the libaio-devel package with yum

fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.

async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... sparse_attn[92m[OKAY][0m
 ............ [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
using world size: 4, data-parallel-size: 4, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  data_parallel_size .............................. 4
  data_path ....................................... ['/ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 40
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 5000
  expert_interval ................................. 2
  ffn_hidden_size ................................. 49152
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 16
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 12288
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 128
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 15.0
  loss_scale_window ............................... 1000
  lr .............................................. 6e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ None
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 96
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 1
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. ds_z_off-none_stage_3_nl1_hs12288_mb4_seq1024_gb16_nodes1
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 50
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 4
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 3
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 1
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
[2023-03-17 14:28:00,475] [INFO] [comm.py:652:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
> setting tensorboard ...
Hi, I am 2 and pinning GPU 2
Hi, I am 0 and pinning GPU 0
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
Hi, I am 1 and pinning GPU 1
Hi, I am 3 and pinning GPU 3
> setting random seeds to 1234 ...
[2023-03-17 14:28:01,607] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> compiling dataset index builder ...
make: Entering directory `/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/megatron/data'
make: Nothing to be done for `default'.
make: Leaving directory `/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/megatron/data'
>>> done with dataset index builder. Compilation time: 0.032 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (4) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (4) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (4) as the number of workers...
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
gpu108-16-r:60335:60335 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-16-r:60335:60335 [0] NCCL INFO Bootstrap : Using ib0:10.109.136.92<0>
gpu108-16-r:60335:60335 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu108-16-r:60335:60335 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu108-16-r:60335:60335 [0] NCCL INFO cudaDriverVersion 11080
NCCL version 2.17.1+cuda11.7
gpu108-16-r:60338:60338 [3] NCCL INFO cudaDriverVersion 11080
gpu108-16-r:60338:60338 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-16-r:60338:60338 [3] NCCL INFO Bootstrap : Using ib0:10.109.136.92<0>
gpu108-16-r:60338:60338 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu108-16-r:60338:60338 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu108-16-r:60337:60337 [2] NCCL INFO cudaDriverVersion 11080
gpu108-16-r:60336:60336 [1] NCCL INFO cudaDriverVersion 11080
gpu108-16-r:60337:60337 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-16-r:60336:60336 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-16-r:60337:60337 [2] NCCL INFO Bootstrap : Using ib0:10.109.136.92<0>
gpu108-16-r:60336:60336 [1] NCCL INFO Bootstrap : Using ib0:10.109.136.92<0>
gpu108-16-r:60337:60337 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu108-16-r:60337:60337 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu108-16-r:60336:60336 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu108-16-r:60336:60336 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu108-16-r:60335:60614 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-16-r:60335:60614 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.136.92<0>
gpu108-16-r:60335:60614 [0] NCCL INFO Using network IB
gpu108-16-r:60336:60616 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-16-r:60337:60618 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-16-r:60338:60619 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-16-r:60336:60616 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.136.92<0>
gpu108-16-r:60336:60616 [1] NCCL INFO Using network IB
gpu108-16-r:60337:60618 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.136.92<0>
gpu108-16-r:60337:60618 [2] NCCL INFO Using network IB
gpu108-16-r:60338:60619 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.136.92<0>
gpu108-16-r:60338:60619 [3] NCCL INFO Using network IB
gpu108-16-r:60337:60618 [2] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu108-16-r:60337:60618 [2] NCCL INFO Setting affinity for GPU 2 to ffff
gpu108-16-r:60335:60614 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:60338:60619 [3] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu108-16-r:60338:60619 [3] NCCL INFO Setting affinity for GPU 3 to ffff
gpu108-16-r:60335:60614 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu108-16-r:60336:60616 [1] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 00/24 :    0   1   2   3
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 01/24 :    0   1   3   2
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 02/24 :    0   2   3   1
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 03/24 :    0   2   1   3
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 04/24 :    0   3   1   2
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 05/24 :    0   3   2   1
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 06/24 :    0   1   2   3
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 07/24 :    0   1   3   2
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 08/24 :    0   2   3   1
gpu108-16-r:60338:60619 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 0/-1/-1->3->1 [3] 0/-1/-1->3->1 [4] 1/-1/-1->3->0 [5] 1/-1/-1->3->0 [6] 2/-1/-1->3->-1 [7] 2/-1/-1->3->-1 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] 0/-1/-1->3->1 [11] 0/-1/-1->3->1 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] 0/-1/-1->3->1 [15] 0/-1/-1->3->1 [16] 1/-1/-1->3->0 [17] 1/-1/-1->3->0 [18] 2/-1/-1->3->-1 [19] 2/-1/-1->3->-1 [20] -1/-1/-1->3->2 [21] -1/-1/-1->3->2 [22] 0/-1/-1->3->1 [23] 0/-1/-1->3->1
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 09/24 :    0   2   1   3
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 10/24 :    0   3   1   2
gpu108-16-r:60336:60616 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 3/-1/-1->1->-1 [3] 3/-1/-1->1->-1 [4] -1/-1/-1->1->3 [5] -1/-1/-1->1->3 [6] 0/-1/-1->1->2 [7] 0/-1/-1->1->2 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 3/-1/-1->1->-1 [11] 3/-1/-1->1->-1 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 3/-1/-1->1->-1 [15] 3/-1/-1->1->-1 [16] -1/-1/-1->1->3 [17] -1/-1/-1->1->3 [18] 0/-1/-1->1->2 [19] 0/-1/-1->1->2 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 3/-1/-1->1->-1 [23] 3/-1/-1->1->-1
gpu108-16-r:60337:60618 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] -1/-1/-1->2->0 [3] -1/-1/-1->2->0 [4] 0/-1/-1->2->-1 [5] 0/-1/-1->2->-1 [6] 1/-1/-1->2->3 [7] 1/-1/-1->2->3 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] -1/-1/-1->2->0 [11] -1/-1/-1->2->0 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] -1/-1/-1->2->0 [15] -1/-1/-1->2->0 [16] 0/-1/-1->2->-1 [17] 0/-1/-1->2->-1 [18] 1/-1/-1->2->3 [19] 1/-1/-1->2->3 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] -1/-1/-1->2->0 [23] -1/-1/-1->2->0
gpu108-16-r:60338:60619 [3] NCCL INFO P2P Chunksize set to 524288
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 11/24 :    0   3   2   1
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 12/24 :    0   1   2   3
gpu108-16-r:60336:60616 [1] NCCL INFO P2P Chunksize set to 524288
gpu108-16-r:60337:60618 [2] NCCL INFO P2P Chunksize set to 524288
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 13/24 :    0   1   3   2
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 14/24 :    0   2   3   1
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 15/24 :    0   2   1   3
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 16/24 :    0   3   1   2
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 17/24 :    0   3   2   1
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 18/24 :    0   1   2   3
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 19/24 :    0   1   3   2
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 20/24 :    0   2   3   1
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 21/24 :    0   2   1   3
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 22/24 :    0   3   1   2
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 23/24 :    0   3   2   1
gpu108-16-r:60335:60614 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 2/-1/-1->0->3 [3] 2/-1/-1->0->3 [4] 3/-1/-1->0->2 [5] 3/-1/-1->0->2 [6] -1/-1/-1->0->1 [7] -1/-1/-1->0->1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 2/-1/-1->0->3 [11] 2/-1/-1->0->3 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 2/-1/-1->0->3 [15] 2/-1/-1->0->3 [16] 3/-1/-1->0->2 [17] 3/-1/-1->0->2 [18] -1/-1/-1->0->1 [19] -1/-1/-1->0->1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 2/-1/-1->0->3 [23] 2/-1/-1->0->3
gpu108-16-r:60335:60614 [0] NCCL INFO P2P Chunksize set to 524288
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 00/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 00/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 00/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 00/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 04/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 02/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 01/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 06/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 06/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 06/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 06/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 07/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 10/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 08/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 09/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 12/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 12/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 12/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 12/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 13/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 16/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 14/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 15/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 18/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 18/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 18/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 18/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 19/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 22/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 20/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 21/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 01/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 02/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 02/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 01/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 03/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 03/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 04/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 04/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 07/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 08/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 08/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 07/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 09/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 09/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 10/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 10/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 13/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 14/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 14/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 13/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 15/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 16/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 15/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 16/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 19/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 20/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 20/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 19/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 21/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 21/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 22/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 22/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 02/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 01/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 03/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 04/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 05/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 05/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 05/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 05/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 08/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 07/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 10/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 09/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 11/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 11/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 11/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 11/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 14/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 13/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 16/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 15/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 17/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 17/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 17/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 17/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 20/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 19/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 22/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 21/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 23/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 23/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 23/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 23/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Connected all rings
gpu108-16-r:60336:60616 [1] NCCL INFO Connected all rings
gpu108-16-r:60335:60614 [0] NCCL INFO Connected all rings
gpu108-16-r:60337:60618 [2] NCCL INFO Connected all rings
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 01/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 07/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 08/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 02/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 09/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 04/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 13/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 05/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 01/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 10/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 19/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 07/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 11/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 20/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 08/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 09/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 14/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 21/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 09/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 13/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 20/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 16/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 19/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 21/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 17/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 21/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 22/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 23/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 02/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 04/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 05/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 10/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 04/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 02/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 11/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 05/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 03/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 14/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 10/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 05/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 16/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 11/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 11/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 05/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 17/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 16/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 11/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 14/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 22/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 17/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 15/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 15/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 23/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 22/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 17/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 17/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 23/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 23/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 23/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 00/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 06/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 08/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 09/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 12/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 00/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 18/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 01/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 06/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 02/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 20/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 00/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 01/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 07/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 03/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Channel 21/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 14/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 06/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 08/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 07/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Channel 15/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 12/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 09/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 13/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 18/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 12/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 19/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 13/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60337:60618 [2] NCCL INFO Channel 20/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 18/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 19/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60338:60619 [3] NCCL INFO Connected all trees
gpu108-16-r:60338:60619 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
gpu108-16-r:60338:60619 [3] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
gpu108-16-r:60336:60616 [1] NCCL INFO Channel 21/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60335:60614 [0] NCCL INFO Connected all trees
gpu108-16-r:60335:60614 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
gpu108-16-r:60335:60614 [0] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
gpu108-16-r:60337:60618 [2] NCCL INFO Connected all trees
gpu108-16-r:60337:60618 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
gpu108-16-r:60337:60618 [2] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
gpu108-16-r:60336:60616 [1] NCCL INFO Connected all trees
gpu108-16-r:60336:60616 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
gpu108-16-r:60336:60616 [1] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
gpu108-16-r:60336:60616 [1] NCCL INFO comm 0x38771a30 rank 1 nranks 4 cudaDev 1 busId 46000 commId 0x1f0938419a5ecd02 - Init COMPLETE
gpu108-16-r:60338:60619 [3] NCCL INFO comm 0x38ac6ed0 rank 3 nranks 4 cudaDev 3 busId c7000 commId 0x1f0938419a5ecd02 - Init COMPLETE
gpu108-16-r:60337:60618 [2] NCCL INFO comm 0x3a2d5e50 rank 2 nranks 4 cudaDev 2 busId 85000 commId 0x1f0938419a5ecd02 - Init COMPLETE
gpu108-16-r:60335:60614 [0] NCCL INFO comm 0x3ab91db0 rank 0 nranks 4 cudaDev 0 busId 7000 commId 0x1f0938419a5ecd02 - Init COMPLETE
>>> done with compiling and loading fused kernels. Compilation time: 7.009 seconds
time to initialize megatron (seconds): -55.349
[after megatron is initialized] datetime: 2023-03-17 14:28:08 
building GPT model ...
[2023-03-17 14:28:08,693] [INFO] [utils.py:829:see_memory_usage] Before Building Model
[2023-03-17 14:28:08,694] [INFO] [utils.py:830:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2023-03-17 14:28:08,694] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 34.36 GB, percent = 6.8%
gpu108-16-r:60335:60669 [0] NCCL INFO Using network IB
gpu108-16-r:60337:60670 [2] NCCL INFO Using network IB
gpu108-16-r:60336:60672 [1] NCCL INFO Using network IB
gpu108-16-r:60338:60671 [3] NCCL INFO Using network IB
gpu108-16-r:60337:60670 [2] NCCL INFO Setting affinity for GPU 2 to ffff
gpu108-16-r:60335:60669 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:60338:60671 [3] NCCL INFO Setting affinity for GPU 3 to ffff
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 00/24 :    0   1   2   3
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 01/24 :    0   1   3   2
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 02/24 :    0   2   3   1
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 03/24 :    0   2   1   3
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 04/24 :    0   3   1   2
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 05/24 :    0   3   2   1
gpu108-16-r:60338:60671 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 0/-1/-1->3->1 [3] 0/-1/-1->3->1 [4] 1/-1/-1->3->0 [5] 1/-1/-1->3->0 [6] 2/-1/-1->3->-1 [7] 2/-1/-1->3->-1 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] 0/-1/-1->3->1 [11] 0/-1/-1->3->1 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] 0/-1/-1->3->1 [15] 0/-1/-1->3->1 [16] 1/-1/-1->3->0 [17] 1/-1/-1->3->0 [18] 2/-1/-1->3->-1 [19] 2/-1/-1->3->-1 [20] -1/-1/-1->3->2 [21] -1/-1/-1->3->2 [22] 0/-1/-1->3->1 [23] 0/-1/-1->3->1
gpu108-16-r:60337:60670 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] -1/-1/-1->2->0 [3] -1/-1/-1->2->0 [4] 0/-1/-1->2->-1 [5] 0/-1/-1->2->-1 [6] 1/-1/-1->2->3 [7] 1/-1/-1->2->3 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] -1/-1/-1->2->0 [11] -1/-1/-1->2->0 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] -1/-1/-1->2->0 [15] -1/-1/-1->2->0 [16] 0/-1/-1->2->-1 [17] 0/-1/-1->2->-1 [18] 1/-1/-1->2->3 [19] 1/-1/-1->2->3 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] -1/-1/-1->2->0 [23] -1/-1/-1->2->0
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 06/24 :    0   1   2   3
gpu108-16-r:60338:60671 [3] NCCL INFO P2P Chunksize set to 524288
gpu108-16-r:60337:60670 [2] NCCL INFO P2P Chunksize set to 524288
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 07/24 :    0   1   3   2
gpu108-16-r:60336:60672 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 3/-1/-1->1->-1 [3] 3/-1/-1->1->-1 [4] -1/-1/-1->1->3 [5] -1/-1/-1->1->3 [6] 0/-1/-1->1->2 [7] 0/-1/-1->1->2 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 3/-1/-1->1->-1 [11] 3/-1/-1->1->-1 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 3/-1/-1->1->-1 [15] 3/-1/-1->1->-1 [16] -1/-1/-1->1->3 [17] -1/-1/-1->1->3 [18] 0/-1/-1->1->2 [19] 0/-1/-1->1->2 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 3/-1/-1->1->-1 [23] 3/-1/-1->1->-1
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 08/24 :    0   2   3   1
gpu108-16-r:60336:60672 [1] NCCL INFO P2P Chunksize set to 524288
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 09/24 :    0   2   1   3
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 10/24 :    0   3   1   2
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 11/24 :    0   3   2   1
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 12/24 :    0   1   2   3
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 13/24 :    0   1   3   2
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 14/24 :    0   2   3   1
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 15/24 :    0   2   1   3
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 16/24 :    0   3   1   2
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 17/24 :    0   3   2   1
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 18/24 :    0   1   2   3
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 19/24 :    0   1   3   2
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 20/24 :    0   2   3   1
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 21/24 :    0   2   1   3
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 22/24 :    0   3   1   2
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 23/24 :    0   3   2   1
gpu108-16-r:60335:60669 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 2/-1/-1->0->3 [3] 2/-1/-1->0->3 [4] 3/-1/-1->0->2 [5] 3/-1/-1->0->2 [6] -1/-1/-1->0->1 [7] -1/-1/-1->0->1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 2/-1/-1->0->3 [11] 2/-1/-1->0->3 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 2/-1/-1->0->3 [15] 2/-1/-1->0->3 [16] 3/-1/-1->0->2 [17] 3/-1/-1->0->2 [18] -1/-1/-1->0->1 [19] -1/-1/-1->0->1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 2/-1/-1->0->3 [23] 2/-1/-1->0->3
gpu108-16-r:60335:60669 [0] NCCL INFO P2P Chunksize set to 524288
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 00/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 00/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 00/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 00/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 01/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 04/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 02/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 06/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 06/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 06/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 06/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 07/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 09/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 10/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 08/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 12/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 12/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 12/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 12/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 16/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 13/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 15/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 14/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 18/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 18/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 18/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 18/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 22/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 21/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 19/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 20/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 02/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 01/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 02/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 01/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 03/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 04/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 04/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 03/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 08/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 08/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 07/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 07/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 09/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 09/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 10/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 10/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 14/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 13/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 14/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 13/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 15/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 16/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 16/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 15/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 20/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 19/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 20/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 19/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 21/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 21/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 22/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 22/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 02/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 03/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 01/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 04/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 05/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 05/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 05/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 05/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 08/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 09/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 07/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 10/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 11/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 11/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 11/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 11/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 14/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 15/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 13/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 16/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 17/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 17/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 17/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 17/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 20/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 21/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 19/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 22/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 23/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 23/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 23/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 23/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Connected all rings
gpu108-16-r:60336:60672 [1] NCCL INFO Connected all rings
gpu108-16-r:60338:60671 [3] NCCL INFO Connected all rings
gpu108-16-r:60335:60669 [0] NCCL INFO Connected all rings
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 01/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 07/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 08/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 02/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 04/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 09/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 01/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 05/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 13/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 07/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 19/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 10/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 09/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 20/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 11/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 08/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 13/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 21/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 14/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 09/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 19/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 20/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 16/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 21/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 17/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 21/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 22/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 23/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 02/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 04/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 05/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 10/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 11/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 02/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 04/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 05/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 03/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 14/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 05/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 16/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 10/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 11/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 11/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 17/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 05/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 14/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 22/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 11/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 16/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 15/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 23/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 17/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 15/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 17/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 17/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 22/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 23/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 23/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 23/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 00/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 06/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 08/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 09/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 12/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 00/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 01/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 18/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 06/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 20/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 02/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 00/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 01/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 07/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 03/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Channel 21/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 06/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 14/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 08/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 07/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 12/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60335:60669 [0] NCCL INFO Channel 15/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 09/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 13/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 12/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 18/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 13/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 19/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 18/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60337:60670 [2] NCCL INFO Channel 20/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 19/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60338:60671 [3] NCCL INFO Connected all trees
gpu108-16-r:60338:60671 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
gpu108-16-r:60338:60671 [3] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
gpu108-16-r:60336:60672 [1] NCCL INFO Channel 21/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:60336:60672 [1] NCCL INFO Connected all trees
gpu108-16-r:60336:60672 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
gpu108-16-r:60336:60672 [1] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
gpu108-16-r:60335:60669 [0] NCCL INFO Connected all trees
gpu108-16-r:60335:60669 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
gpu108-16-r:60335:60669 [0] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
gpu108-16-r:60337:60670 [2] NCCL INFO Connected all trees
gpu108-16-r:60337:60670 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
gpu108-16-r:60337:60670 [2] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
gpu108-16-r:60335:60669 [0] NCCL INFO comm 0x36c60410 rank 0 nranks 4 cudaDev 0 busId 7000 commId 0x2f4c1da9784f1632 - Init COMPLETE
gpu108-16-r:60337:60670 [2] NCCL INFO comm 0x3a631730 rank 2 nranks 4 cudaDev 2 busId 85000 commId 0x2f4c1da9784f1632 - Init COMPLETE
gpu108-16-r:60336:60672 [1] NCCL INFO comm 0x38acd0f0 rank 1 nranks 4 cudaDev 1 busId 46000 commId 0x2f4c1da9784f1632 - Init COMPLETE
gpu108-16-r:60338:60671 [3] NCCL INFO comm 0x38e236f0 rank 3 nranks 4 cudaDev 3 busId c7000 commId 0x2f4c1da9784f1632 - Init COMPLETE
[2023-03-17 14:28:13,249] [INFO] [partition_parameters.py:415:__exit__] finished initializing model with 2.44B parameters
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[2023-03-17 14:28:13,284] [INFO] [utils.py:829:see_memory_usage] After Building Model
[2023-03-17 14:28:13,285] [INFO] [utils.py:830:see_memory_usage] MA 1.14 GB         Max_MA 2.26 GB         CA 4.04 GB         Max_CA 4 GB 
[2023-03-17 14:28:13,285] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 34.55 GB, percent = 6.9%
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2442842112
> learning rate decay style: cosine
DeepSpeed is enabled.
[2023-03-17 14:28:13,286] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed info: version=0.8.3+bbfd0a6, git-hash=bbfd0a6, git-branch=master
[2023-03-17 14:28:13,288] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-03-17 14:28:13,288] [INFO] [logging.py:93:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-03-17 14:28:13,288] [INFO] [logging.py:93:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-03-17 14:28:13,289] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2023-03-17 14:28:13,289] [INFO] [utils.py:55:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'apex.optimizers.fused_adam.FusedAdam'>
[2023-03-17 14:28:13,289] [INFO] [logging.py:93:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer
[2023-03-17 14:28:13,323] [INFO] [utils.py:829:see_memory_usage] Stage 3 initialize beginning
[2023-03-17 14:28:13,324] [INFO] [utils.py:830:see_memory_usage] MA 1.14 GB         Max_MA 1.14 GB         CA 4.04 GB         Max_CA 4 GB 
[2023-03-17 14:28:13,324] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 34.56 GB, percent = 6.9%
[2023-03-17 14:28:13,325] [INFO] [stage3.py:113:__init__] Reduce bucket size 90000000
[2023-03-17 14:28:13,325] [INFO] [stage3.py:114:__init__] Prefetch bucket size 50000000
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Emitting ninja build file /home/shaima0d/.cache/torch_extensions/py39_cu117/utils/build.ninja...
Building extension module utils...
Using envvar MAX_JOBS (4) as the number of workers...
ninja: no work to do.
Loading extension module utils...
Loading extension module utils...
Time to load utils op: 0.10335803031921387 seconds
Time to load utils op: 0.1514122486114502 seconds
[2023-03-17 14:28:13,459] [INFO] [utils.py:829:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2023-03-17 14:28:13,460] [INFO] [utils.py:830:see_memory_usage] MA 1.14 GB         Max_MA 1.14 GB         CA 4.04 GB         Max_CA 4 GB 
[2023-03-17 14:28:13,460] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 34.55 GB, percent = 6.9%
Parameter Offload: Total persistent parameters: 184320 in 10 params
Loading extension module utils...
Loading extension module utils...
Time to load utils op: 0.2072293758392334 secondsTime to load utils op: 0.20717549324035645 seconds

[2023-03-17 14:28:13,485] [INFO] [utils.py:829:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2023-03-17 14:28:13,485] [INFO] [utils.py:830:see_memory_usage] MA 1.14 GB         Max_MA 1.14 GB         CA 4.04 GB         Max_CA 4 GB 
[2023-03-17 14:28:13,486] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 34.55 GB, percent = 6.9%
[2023-03-17 14:28:13,510] [INFO] [utils.py:829:see_memory_usage] Before creating fp16 partitions
[2023-03-17 14:28:13,510] [INFO] [utils.py:830:see_memory_usage] MA 1.14 GB         Max_MA 1.14 GB         CA 4.04 GB         Max_CA 4 GB 
[2023-03-17 14:28:13,511] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 34.55 GB, percent = 6.9%
[2023-03-17 14:28:14,587] [INFO] [utils.py:829:see_memory_usage] After creating fp16 partitions: 2
[2023-03-17 14:28:14,588] [INFO] [utils.py:830:see_memory_usage] MA 1.14 GB         Max_MA 1.14 GB         CA 1.14 GB         Max_CA 4 GB 
[2023-03-17 14:28:14,588] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 34.56 GB, percent = 6.9%
[2023-03-17 14:28:14,612] [INFO] [utils.py:829:see_memory_usage] Before creating fp32 partitions
[2023-03-17 14:28:14,613] [INFO] [utils.py:830:see_memory_usage] MA 1.14 GB         Max_MA 1.14 GB         CA 1.14 GB         Max_CA 1 GB 
[2023-03-17 14:28:14,613] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 34.56 GB, percent = 6.9%
[2023-03-17 14:28:14,641] [INFO] [utils.py:829:see_memory_usage] After creating fp32 partitions
[2023-03-17 14:28:14,642] [INFO] [utils.py:830:see_memory_usage] MA 3.41 GB         Max_MA 4.55 GB         CA 4.55 GB         Max_CA 5 GB 
[2023-03-17 14:28:14,642] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 34.56 GB, percent = 6.9%
[2023-03-17 14:28:14,667] [INFO] [utils.py:829:see_memory_usage] Before initializing optimizer states
[2023-03-17 14:28:14,667] [INFO] [utils.py:830:see_memory_usage] MA 3.41 GB         Max_MA 3.41 GB         CA 4.55 GB         Max_CA 5 GB 
[2023-03-17 14:28:14,668] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 34.56 GB, percent = 6.9%
[2023-03-17 14:28:14,687] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | init_optimizer_state: 15.54
[2023-03-17 14:28:14,710] [INFO] [utils.py:829:see_memory_usage] After initializing optimizer states
[2023-03-17 14:28:14,711] [INFO] [utils.py:830:see_memory_usage] MA 7.96 GB         Max_MA 10.24 GB         CA 11.38 GB         Max_CA 11 GB 
[2023-03-17 14:28:14,711] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 34.56 GB, percent = 6.9%
[2023-03-17 14:28:14,711] [INFO] [stage3.py:376:_setup_for_real_optimizer] optimizer state initialized
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...Time to load utils op: 0.0012035369873046875 seconds

Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...Time to load utils op: 0.001650094985961914 seconds

Loading extension module utils...
Time to load utils op: 0.0020415782928466797 seconds
[2023-03-17 14:28:14,768] [INFO] [utils.py:829:see_memory_usage] After initializing ZeRO optimizer
[2023-03-17 14:28:14,769] [INFO] [utils.py:830:see_memory_usage] MA 9.27 GB         Max_MA 11.57 GB         CA 18.21 GB         Max_CA 18 GB 
[2023-03-17 14:28:14,769] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 34.56 GB, percent = 6.9%
[2023-03-17 14:28:14,769] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[2023-03-17 14:28:14,770] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-03-17 14:28:14,770] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x2b3a65c584f0>
[2023-03-17 14:28:14,770] [INFO] [logging.py:93:log_dist] [Rank 0] step=0, skipped=0, lr=[5.9999999999999995e-05, 5.9999999999999995e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:14,770] [INFO] [config.py:1018:print] DeepSpeedEngine configuration:
[2023-03-17 14:28:14,770] [INFO] [config.py:1022:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-03-17 14:28:14,770] [INFO] [config.py:1022:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 16, 'thread_count': 2, 'single_submit': False, 'overlap_events': True}
[2023-03-17 14:28:14,770] [INFO] [config.py:1022:print]   amp_enabled .................. False
[2023-03-17 14:28:14,770] [INFO] [config.py:1022:print]   amp_params ................... False
[2023-03-17 14:28:14,770] [INFO] [config.py:1022:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-03-17 14:28:14,770] [INFO] [config.py:1022:print]   bfloat16_enabled ............. False
[2023-03-17 14:28:14,770] [INFO] [config.py:1022:print]   checkpoint_parallel_write_pipeline  False
[2023-03-17 14:28:14,770] [INFO] [config.py:1022:print]   checkpoint_tag_validation_enabled  True
[2023-03-17 14:28:14,770] [INFO] [config.py:1022:print]   checkpoint_tag_validation_fail  False
[2023-03-17 14:28:14,770] [INFO] [config.py:1022:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x2b3a65c58a00>
[2023-03-17 14:28:14,770] [INFO] [config.py:1022:print]   communication_data_type ...... None
[2023-03-17 14:28:14,770] [INFO] [config.py:1022:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-03-17 14:28:14,770] [INFO] [config.py:1022:print]   curriculum_enabled_legacy .... False
[2023-03-17 14:28:14,770] [INFO] [config.py:1022:print]   curriculum_params_legacy ..... False
[2023-03-17 14:28:14,770] [INFO] [config.py:1022:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-03-17 14:28:14,770] [INFO] [config.py:1022:print]   data_efficiency_enabled ...... False
[2023-03-17 14:28:14,770] [INFO] [config.py:1022:print]   dataloader_drop_last ......... False
[2023-03-17 14:28:14,770] [INFO] [config.py:1022:print]   disable_allgather ............ False
[2023-03-17 14:28:14,770] [INFO] [config.py:1022:print]   dump_state ................... False
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   dynamic_loss_scale_args ...... {'init_scale': 32768, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   eigenvalue_enabled ........... False
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   eigenvalue_gas_boundary_resolution  1
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   eigenvalue_layer_num ......... 0
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   eigenvalue_max_iter .......... 100
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   eigenvalue_stability ......... 1e-06
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   eigenvalue_tol ............... 0.01
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   eigenvalue_verbose ........... False
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   elasticity_enabled ........... False
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   fp16_auto_cast ............... False
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   fp16_enabled ................. True
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   fp16_master_weights_and_gradients  False
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   global_rank .................. 0
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   grad_accum_dtype ............. None
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   gradient_accumulation_steps .. 1
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   gradient_clipping ............ 1
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   gradient_predivide_factor .... 1.0
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   initial_dynamic_scale ........ 32768
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   load_universal_checkpoint .... False
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   loss_scale ................... 0
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   memory_breakdown ............. False
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   optimizer_legacy_fusion ...... False
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   optimizer_name ............... None
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   optimizer_params ............. None
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   pld_enabled .................. False
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   pld_params ................... False
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   prescale_gradients ........... False
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   scheduler_name ............... None
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   scheduler_params ............. None
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   sparse_attention ............. None
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   sparse_gradients_enabled ..... False
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   steps_per_print .............. 1
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   train_batch_size ............. 16
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   train_micro_batch_size_per_gpu  4
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   use_node_local_storage ....... False
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   wall_clock_breakdown ......... True
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   world_size ................... 4
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   zero_allow_untested_optimizer  False
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=90000000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=sys.maxsize max_live_parameters=3000000000 max_reuse_distance=3000000000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2023-03-17 14:28:14,771] [INFO] [config.py:1022:print]   zero_enabled ................. True
[2023-03-17 14:28:14,772] [INFO] [config.py:1022:print]   zero_force_ds_cpu_optimizer .. True
[2023-03-17 14:28:14,772] [INFO] [config.py:1022:print]   zero_optimization_stage ...... 3
[2023-03-17 14:28:14,772] [INFO] [config.py:1007:print_user_config]   json = {
    "train_batch_size": 16, 
    "train_micro_batch_size_per_gpu": 4, 
    "steps_per_print": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 3, 
        "stage3_max_live_parameters": 3.000000e+09, 
        "stage3_max_reuse_distance": 3.000000e+09, 
        "stage3_param_persistence_threshold": 1.000000e+05, 
        "stage3_prefetch_bucket_size": 5.000000e+07, 
        "contiguous_gradients": true, 
        "overlap_comm": true, 
        "reduce_bucket_size": 9.000000e+07, 
        "sub_group_size": 1.000000e+09, 
        "offload_optimizer": {
            "device": "none", 
            "buffer_count": 4, 
            "pipeline_read": false, 
            "pipeline_write": false, 
            "pin_memory": true
        }
    }, 
    "gradient_clipping": 1, 
    "fp16": {
        "enabled": true, 
        "initial_scale_power": 15, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "wall_clock_breakdown": true, 
    "zero_allow_untested_optimizer": false, 
    "aio": {
        "block_size": 1.048576e+06, 
        "queue_depth": 16, 
        "single_submit": false, 
        "overlap_events": true, 
        "thread_count": 2
    }
}
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.000827789306640625 seconds
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-03-17 14:28:14 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      800
    validation: 640
    test:       640
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.000789 seconds
    number of documents: 17868
 > dataset split:
    train:
     document indices in [0, 17511) total of 17511 documents
    validation:
     document indices in [17511, 17868) total of 357 documents
    test:
     document indices in [17868, 17868) total of 0 documents
NCCL version 2.17.1+cuda11.7NCCL version 2.17.1+cuda11.7

NCCL version 2.17.1+cuda11.7
gpu108-16-r:60336:60695 [1] NCCL INFO Using network IB
gpu108-16-r:60335:60697 [0] NCCL INFO Using network IB
gpu108-16-r:60338:60696 [3] NCCL INFO Using network IB
gpu108-16-r:60337:60698 [2] NCCL INFO Using network IB
gpu108-16-r:60336:60695 [1] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 00/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 01/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 02/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 03/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 04/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 05/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 06/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 07/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 08/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 09/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 10/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 11/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 12/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 13/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 14/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 15/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 16/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 17/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 18/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 19/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 20/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 21/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 22/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 23/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 24/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 25/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 26/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 27/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 28/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 29/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 30/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Channel 31/32 :    0
gpu108-16-r:60336:60695 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-16-r:60336:60695 [1] NCCL INFO P2P Chunksize set to 131072
gpu108-16-r:60336:60695 [1] NCCL INFO Connected all rings
gpu108-16-r:60336:60695 [1] NCCL INFO Connected all trees
gpu108-16-r:60336:60695 [1] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-16-r:60335:60697 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:60338:60696 [3] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:60338:60696 [3] NCCL INFO Setting affinity for GPU 3 to ffff
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 00/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 01/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 02/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 03/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 04/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 05/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 06/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 07/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 08/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 09/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 10/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 11/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 12/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 13/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 14/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 15/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 16/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 17/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 18/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 19/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 20/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 21/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 22/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 23/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 24/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 25/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 26/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 27/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 28/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 29/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 30/32 :    0
gpu108-16-r:60335:60697 [0] NCCL INFO Channel 31/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:60335:60697 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-16-r:60335:60697 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 00/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 01/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 02/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 03/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 04/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 05/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 06/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 07/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 08/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 09/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 10/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 11/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 12/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 13/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 14/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 15/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 16/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 17/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 18/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 19/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 20/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 21/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 22/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 23/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 24/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 25/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 26/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 27/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 28/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 29/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 30/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Channel 31/32 :    0
gpu108-16-r:60338:60696 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-16-r:60338:60696 [3] NCCL INFO P2P Chunksize set to 131072
gpu108-16-r:60335:60697 [0] NCCL INFO Connected all rings
gpu108-16-r:60335:60697 [0] NCCL INFO Connected all trees
gpu108-16-r:60335:60697 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-16-r:60338:60696 [3] NCCL INFO Connected all rings
gpu108-16-r:60338:60696 [3] NCCL INFO Connected all trees
gpu108-16-r:60338:60696 [3] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-16-r:60337:60698 [2] NCCL INFO Setting affinity for GPU 2 to ffff
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 00/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 01/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 02/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 03/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 04/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 05/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 06/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 07/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 08/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 09/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 10/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 11/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 12/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 13/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 14/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 15/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 16/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 17/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 18/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 19/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 20/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 21/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 22/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 23/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 24/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 25/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 26/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 27/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 28/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 29/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 30/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Channel 31/32 :    0
gpu108-16-r:60337:60698 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-16-r:60337:60698 [2] NCCL INFO P2P Chunksize set to 131072
gpu108-16-r:60337:60698 [2] NCCL INFO Connected all rings
gpu108-16-r:60337:60698 [2] NCCL INFO Connected all trees
gpu108-16-r:60337:60698 [2] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-16-r:60336:60695 [1] NCCL INFO comm 0x38b17c00 rank 0 nranks 1 cudaDev 1 busId 46000 commId 0x1225f2ac897a1e8c - Init COMPLETE
gpu108-16-r:60338:60696 [3] NCCL INFO comm 0x38e6ef80 rank 0 nranks 1 cudaDev 3 busId c7000 commId 0xa1c42cf87e0a1a80 - Init COMPLETE
gpu108-16-r:60335:60697 [0] NCCL INFO comm 0x36c89fe0 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0xec9c77c3ccdd5b29 - Init COMPLETE
gpu108-16-r:60337:60698 [2] NCCL INFO comm 0x3a67bb70 rank 0 nranks 1 cudaDev 2 busId 85000 commId 0xbe3283236b1d4eae - Init COMPLETE
 > loading doc-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_train_indexmap_800ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_train_indexmap_800ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_train_indexmap_800ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.001 seconds
    total number of samples: 1544006
    total number of epochs: 1
 > loading doc-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_valid_indexmap_640ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_valid_indexmap_640ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_valid_indexmap_640ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.001 seconds
    total number of samples: 31426
    total number of epochs: 1
> finished creating GPT datasets ...
gpu108-16-r:60335:60715 [0] NCCL INFO Using network IB
gpu108-16-r:60337:60712 [2] NCCL INFO Using network IB
gpu108-16-r:60336:60714 [1] NCCL INFO Using network IB
gpu108-16-r:60338:60713 [3] NCCL INFO Using network IB
gpu108-16-r:60337:60712 [2] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:60338:60713 [3] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:60337:60712 [2] NCCL INFO Setting affinity for GPU 2 to ffff
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 00/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 01/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 02/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 03/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 04/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 05/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 06/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 07/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 08/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 09/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 10/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 11/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 12/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 13/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 14/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 15/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 16/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 17/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 18/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 19/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 20/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 21/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 22/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 23/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 24/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 25/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 26/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 27/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 28/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 29/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 30/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Channel 31/32 :    0
gpu108-16-r:60337:60712 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-16-r:60337:60712 [2] NCCL INFO P2P Chunksize set to 131072
gpu108-16-r:60335:60715 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:60338:60713 [3] NCCL INFO Setting affinity for GPU 3 to ffff
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 00/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 01/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 02/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 03/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 04/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 05/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 06/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 07/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 08/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 09/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 10/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 11/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 12/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 13/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 14/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 15/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 16/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 17/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 18/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 19/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 20/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 21/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 22/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 23/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 24/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 25/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 26/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 27/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 28/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 29/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 30/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Channel 31/32 :    0
gpu108-16-r:60338:60713 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-16-r:60338:60713 [3] NCCL INFO P2P Chunksize set to 131072
gpu108-16-r:60337:60712 [2] NCCL INFO Connected all rings
gpu108-16-r:60337:60712 [2] NCCL INFO Connected all trees
gpu108-16-r:60337:60712 [2] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-16-r:60338:60713 [3] NCCL INFO Connected all rings
gpu108-16-r:60338:60713 [3] NCCL INFO Connected all trees
gpu108-16-r:60338:60713 [3] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 00/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 01/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 02/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 03/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 04/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 05/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 06/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 07/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 08/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 09/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 10/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 11/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 12/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 13/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 14/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 15/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 16/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 17/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 18/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 19/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 20/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 21/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 22/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 23/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 24/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 25/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 26/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 27/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 28/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 29/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 30/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Channel 31/32 :    0
gpu108-16-r:60335:60715 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-16-r:60335:60715 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-16-r:60336:60714 [1] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:60335:60715 [0] NCCL INFO Connected all rings
gpu108-16-r:60335:60715 [0] NCCL INFO Connected all trees
gpu108-16-r:60335:60715 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 00/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 01/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 02/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 03/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 04/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 05/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 06/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 07/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 08/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 09/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 10/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 11/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 12/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 13/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 14/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 15/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 16/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 17/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 18/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 19/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 20/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 21/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 22/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 23/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 24/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 25/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 26/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 27/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 28/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 29/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 30/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Channel 31/32 :    0
gpu108-16-r:60336:60714 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-16-r:60336:60714 [1] NCCL INFO P2P Chunksize set to 131072
gpu108-16-r:60336:60714 [1] NCCL INFO Connected all rings
gpu108-16-r:60336:60714 [1] NCCL INFO Connected all trees
gpu108-16-r:60336:60714 [1] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-16-r:60338:60713 [3] NCCL INFO comm 0x38e5cc80 rank 0 nranks 1 cudaDev 3 busId c7000 commId 0x397a345a56f7ea24 - Init COMPLETE
gpu108-16-r:60335:60715 [0] NCCL INFO comm 0x36c78e30 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0xa667ac7de7c66b71 - Init COMPLETE
gpu108-16-r:60337:60712 [2] NCCL INFO comm 0x3a669870 rank 0 nranks 1 cudaDev 2 busId 85000 commId 0xf29e254c413f09a8 - Init COMPLETE
gpu108-16-r:60336:60714 [1] NCCL INFO comm 0x38b05900 rank 0 nranks 1 cudaDev 1 busId 46000 commId 0xa8564b317f1a56bb - Init COMPLETE
[after dataloaders are built] datetime: 2023-03-17 14:28:15 time (ms) | model-and-optimizer-setup: 6101.53 | train/valid/test-data-iterators-setup: 908.18

done with setup ...
training ...
[before the start of training step] datetime: 2023-03-17 14:28:15 
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
[2023-03-17 14:28:15,703] [INFO] [checkpointing.py:553:forward] Activation Checkpointing Information
[2023-03-17 14:28:15,703] [INFO] [checkpointing.py:554:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-03-17 14:28:15,703] [INFO] [checkpointing.py:557:forward] ----contiguous Memory Checkpointing False with 1 total layers
[2023-03-17 14:28:15,703] [INFO] [checkpointing.py:560:forward] ----Synchronization False
[2023-03-17 14:28:15,703] [INFO] [checkpointing.py:561:forward] ----Profiling time in checkpointing False
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
gpu108-16-r:60335:61583 [0] NCCL INFO Using network IB
gpu108-16-r:60336:61580 [1] NCCL INFO Using network IB
gpu108-16-r:60337:61581 [2] NCCL INFO Using network IB
gpu108-16-r:60338:61582 [3] NCCL INFO Using network IB
gpu108-16-r:60335:61583 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:60336:61580 [1] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 00/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 01/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 02/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 03/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 04/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 05/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 06/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 07/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 08/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 09/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 10/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 11/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 12/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 13/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 14/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 15/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 16/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 17/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 18/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 19/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 20/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 21/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 22/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 23/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 24/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 25/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 26/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 27/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 28/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 29/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 30/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Channel 31/32 :    0
gpu108-16-r:60335:61583 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-16-r:60335:61583 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-16-r:60338:61582 [3] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:60335:61583 [0] NCCL INFO Connected all rings
gpu108-16-r:60335:61583 [0] NCCL INFO Connected all trees
gpu108-16-r:60335:61583 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 00/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 01/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 02/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 03/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 04/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 05/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 06/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 07/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 08/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 09/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 10/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 11/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 12/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 13/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 14/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 15/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 16/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 17/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 18/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 19/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 20/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 21/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 22/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 23/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 24/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 25/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 26/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 27/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 28/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 29/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 30/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Channel 31/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-16-r:60336:61580 [1] NCCL INFO P2P Chunksize set to 131072
gpu108-16-r:60337:61581 [2] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:60338:61582 [3] NCCL INFO Setting affinity for GPU 3 to ffff
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 00/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 01/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 02/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 03/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 04/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 05/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 06/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 07/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 08/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 09/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 10/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 11/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 12/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 13/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 14/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 15/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 16/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 17/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 18/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 19/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 20/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 21/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 22/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 23/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 24/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 25/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 26/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Connected all rings
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 27/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 28/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO Connected all trees
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 29/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 30/32 :    0
gpu108-16-r:60336:61580 [1] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-16-r:60338:61582 [3] NCCL INFO Channel 31/32 :    0
gpu108-16-r:60338:61582 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-16-r:60338:61582 [3] NCCL INFO P2P Chunksize set to 131072
gpu108-16-r:60338:61582 [3] NCCL INFO Connected all rings
gpu108-16-r:60338:61582 [3] NCCL INFO Connected all trees
gpu108-16-r:60338:61582 [3] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-16-r:60337:61581 [2] NCCL INFO Setting affinity for GPU 2 to ffff
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 00/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 01/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 02/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 03/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 04/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 05/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 06/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 07/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 08/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 09/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 10/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 11/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 12/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 13/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 14/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 15/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 16/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 17/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 18/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 19/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 20/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 21/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 22/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 23/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 24/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 25/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 26/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 27/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 28/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 29/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 30/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Channel 31/32 :    0
gpu108-16-r:60337:61581 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-16-r:60337:61581 [2] NCCL INFO P2P Chunksize set to 131072
gpu108-16-r:60337:61581 [2] NCCL INFO Connected all rings
gpu108-16-r:60337:61581 [2] NCCL INFO Connected all trees
gpu108-16-r:60337:61581 [2] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-16-r:60336:61580 [1] NCCL INFO comm 0x6da496b0 rank 0 nranks 1 cudaDev 1 busId 46000 commId 0xb58c79db42d2357d - Init COMPLETE
gpu108-16-r:60338:61582 [3] NCCL INFO comm 0x6e3df980 rank 0 nranks 1 cudaDev 3 busId c7000 commId 0xab532b190ac13695 - Init COMPLETE
gpu108-16-r:60335:61583 [0] NCCL INFO comm 0x704373e0 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0x5c9a486aac754e50 - Init COMPLETE
gpu108-16-r:60337:61581 [2] NCCL INFO comm 0x6faaa890 rank 0 nranks 1 cudaDev 2 busId 85000 commId 0xd13cda83fb858564 - Init COMPLETE
[2023-03-17 14:28:17,320] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 27.86
[2023-03-17 14:28:17,321] [INFO] [logging.py:93:log_dist] [Rank 0] step=1, skipped=0, lr=[5.9946721667563326e-05, 5.9946721667563326e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:17,321] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1060.72 | backward_microstep: 384.41 | backward_inner_microstep: 375.86 | backward_allreduce_microstep: 8.46 | step_microstep: 181.61
[2023-03-17 14:28:17,321] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1060.71 | backward: 384.41 | backward_inner: 375.87 | backward_allreduce: 8.46 | step: 181.61
[Rank 0] (after 1 iterations) memory (MB) | allocated: 9492.21875 | max allocated: 18381.111328125 | reserved: 28110.0 | max reserved: 28110.0
 iteration        1/      50 | consumed samples:           16 | consumed tokens:        16384 | elapsed time per iteration (ms): 1631.2 | learning rate: 5.995E-05 | global batch size:    16 | lm loss: 1.103198E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.809 | TFLOPs: 46.22 |
time (ms) | forward-compute: 1063.00 | backward-compute: 384.50 | backward-embedding-all-reduce: 0.01 | optimizer: 181.70 | batch-generator: 3.41
[2023-03-17 14:28:17,737] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.93
[2023-03-17 14:28:17,737] [INFO] [logging.py:93:log_dist] [Rank 0] step=2, skipped=0, lr=[5.97870969354909e-05, 5.97870969354909e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:17,737] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 103.76 | backward_microstep: 274.14 | backward_inner_microstep: 252.40 | backward_allreduce_microstep: 21.68 | step_microstep: 33.56
[2023-03-17 14:28:17,738] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 103.74 | backward: 274.13 | backward_inner: 252.40 | backward_allreduce: 21.69 | step: 33.56
 iteration        2/      50 | consumed samples:           32 | consumed tokens:        32768 | elapsed time per iteration (ms): 416.1 | learning rate: 5.979E-05 | global batch size:    16 | lm loss: 9.677216E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.452 | TFLOPs: 181.18 |
time (ms) | forward-compute: 106.38 | backward-compute: 274.22 | backward-embedding-all-reduce: 0.01 | optimizer: 33.64 | batch-generator: 1.07
[2023-03-17 14:28:18,151] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.99
[2023-03-17 14:28:18,151] [INFO] [logging.py:93:log_dist] [Rank 0] step=3, skipped=0, lr=[5.95217557696746e-05, 5.95217557696746e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:18,152] [INFO] [timer.py:198:stop] epoch=0/micro_step=3/global_step=3, RunningAvgSamplesPerSec=38.794995817515336, CurrSamplesPerSec=38.794995817515336, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:18,152] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 105.04 | backward_microstep: 272.11 | backward_inner_microstep: 254.11 | backward_allreduce_microstep: 17.93 | step_microstep: 33.52
[2023-03-17 14:28:18,152] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 105.02 | backward: 272.11 | backward_inner: 254.11 | backward_allreduce: 17.94 | step: 33.52
 iteration        3/      50 | consumed samples:           48 | consumed tokens:        49152 | elapsed time per iteration (ms): 414.2 | learning rate: 5.952E-05 | global batch size:    16 | lm loss: 1.058213E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.633 | TFLOPs: 182.03 |
time (ms) | forward-compute: 107.34 | backward-compute: 272.20 | backward-embedding-all-reduce: 0.01 | optimizer: 33.30 | batch-generator: 1.28
[2023-03-17 14:28:18,564] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.96
[2023-03-17 14:28:18,565] [INFO] [logging.py:93:log_dist] [Rank 0] step=4, skipped=0, lr=[5.9151745350473036e-05, 5.9151745350473036e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:18,565] [INFO] [timer.py:198:stop] epoch=0/micro_step=4/global_step=4, RunningAvgSamplesPerSec=38.848782995395496, CurrSamplesPerSec=38.90271952641181, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:18,565] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 103.25 | backward_microstep: 271.37 | backward_inner_microstep: 252.44 | backward_allreduce_microstep: 18.85 | step_microstep: 33.97
[2023-03-17 14:28:18,565] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 103.24 | backward: 271.37 | backward_inner: 252.44 | backward_allreduce: 18.85 | step: 33.97
 iteration        4/      50 | consumed samples:           64 | consumed tokens:        65536 | elapsed time per iteration (ms): 413.4 | learning rate: 5.915E-05 | global batch size:    16 | lm loss: 1.150413E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.704 | TFLOPs: 182.37 |
time (ms) | forward-compute: 106.83 | backward-compute: 271.43 | backward-embedding-all-reduce: 0.01 | optimizer: 33.77 | batch-generator: 0.87
[2023-03-17 14:28:18,978] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.95
[2023-03-17 14:28:18,978] [INFO] [logging.py:93:log_dist] [Rank 0] step=5, skipped=0, lr=[5.8678525939969144e-05, 5.8678525939969144e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:18,979] [INFO] [timer.py:198:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=38.871390342324254, CurrSamplesPerSec=38.91668406378014, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:18,979] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 103.49 | backward_microstep: 272.57 | backward_inner_microstep: 252.11 | backward_allreduce_microstep: 20.40 | step_microstep: 33.50
[2023-03-17 14:28:18,979] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 103.48 | backward: 272.57 | backward_inner: 252.10 | backward_allreduce: 20.41 | step: 33.51
 iteration        5/      50 | consumed samples:           80 | consumed tokens:        81920 | elapsed time per iteration (ms): 413.5 | learning rate: 5.868E-05 | global batch size:    16 | lm loss: 1.060220E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.694 | TFLOPs: 182.32 |
time (ms) | forward-compute: 106.31 | backward-compute: 272.64 | backward-embedding-all-reduce: 0.01 | optimizer: 33.29 | batch-generator: 0.96
[2023-03-17 14:28:19,394] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 23.03
[2023-03-17 14:28:19,394] [INFO] [logging.py:93:log_dist] [Rank 0] step=6, skipped=0, lr=[5.810396511898279e-05, 5.810396511898279e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:19,395] [INFO] [timer.py:198:stop] epoch=0/micro_step=6/global_step=6, RunningAvgSamplesPerSec=38.819361245549345, CurrSamplesPerSec=38.66410630014939, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:19,395] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 104.08 | backward_microstep: 273.72 | backward_inner_microstep: 253.54 | backward_allreduce_microstep: 20.13 | step_microstep: 34.27
[2023-03-17 14:28:19,395] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 104.06 | backward: 273.72 | backward_inner: 253.53 | backward_allreduce: 20.14 | step: 34.28
 iteration        6/      50 | consumed samples:           96 | consumed tokens:        98304 | elapsed time per iteration (ms): 416.1 | learning rate: 5.810E-05 | global batch size:    16 | lm loss: 1.089130E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.456 | TFLOPs: 181.20 |
time (ms) | forward-compute: 107.15 | backward-compute: 273.75 | backward-embedding-all-reduce: 0.01 | optimizer: 33.94 | batch-generator: 1.11
[2023-03-17 14:28:19,808] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.91
[2023-03-17 14:28:19,808] [INFO] [logging.py:93:log_dist] [Rank 0] step=7, skipped=0, lr=[5.743033041658253e-05, 5.743033041658253e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:19,808] [INFO] [timer.py:198:stop] epoch=0/micro_step=7/global_step=7, RunningAvgSamplesPerSec=38.833837894366006, CurrSamplesPerSec=38.89185262456354, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:19,809] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 103.27 | backward_microstep: 272.49 | backward_inner_microstep: 252.87 | backward_allreduce_microstep: 19.55 | step_microstep: 33.80
[2023-03-17 14:28:19,809] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 103.26 | backward: 272.48 | backward_inner: 252.87 | backward_allreduce: 19.55 | step: 33.80
 iteration        7/      50 | consumed samples:          112 | consumed tokens:       114688 | elapsed time per iteration (ms): 413.7 | learning rate: 5.743E-05 | global batch size:    16 | lm loss: 1.014043E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.674 | TFLOPs: 182.23 |
time (ms) | forward-compute: 106.30 | backward-compute: 272.61 | backward-embedding-all-reduce: 0.02 | optimizer: 33.56 | batch-generator: 1.18
[2023-03-17 14:28:20,223] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.95
[2023-03-17 14:28:20,223] [INFO] [logging.py:93:log_dist] [Rank 0] step=8, skipped=0, lr=[5.666028036118432e-05, 5.666028036118432e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:20,223] [INFO] [timer.py:198:stop] epoch=0/micro_step=8/global_step=8, RunningAvgSamplesPerSec=38.81969620513837, CurrSamplesPerSec=38.749141972884956, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:20,224] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 104.59 | backward_microstep: 272.47 | backward_inner_microstep: 256.02 | backward_allreduce_microstep: 16.39 | step_microstep: 33.34
[2023-03-17 14:28:20,224] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 104.57 | backward: 272.47 | backward_inner: 256.02 | backward_allreduce: 16.40 | step: 33.35
 iteration        8/      50 | consumed samples:          128 | consumed tokens:       131072 | elapsed time per iteration (ms): 415.0 | learning rate: 5.666E-05 | global batch size:    16 | lm loss: 9.358145E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.551 | TFLOPs: 181.64 |
time (ms) | forward-compute: 108.06 | backward-compute: 272.54 | backward-embedding-all-reduce: 0.01 | optimizer: 33.15 | batch-generator: 2.08
[2023-03-17 14:28:20,639] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.90
[2023-03-17 14:28:20,639] [INFO] [logging.py:93:log_dist] [Rank 0] step=9, skipped=0, lr=[5.579685398855441e-05, 5.579685398855441e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:20,639] [INFO] [timer.py:198:stop] epoch=0/micro_step=9/global_step=9, RunningAvgSamplesPerSec=38.79535465312227, CurrSamplesPerSec=38.64994399090032, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:20,640] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 104.59 | backward_microstep: 274.38 | backward_inner_microstep: 252.84 | backward_allreduce_microstep: 21.44 | step_microstep: 33.41
[2023-03-17 14:28:20,640] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 104.58 | backward: 274.37 | backward_inner: 252.86 | backward_allreduce: 21.44 | step: 33.41
 iteration        9/      50 | consumed samples:          144 | consumed tokens:       147456 | elapsed time per iteration (ms): 416.0 | learning rate: 5.580E-05 | global batch size:    16 | lm loss: 8.845327E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.461 | TFLOPs: 181.22 |
time (ms) | forward-compute: 107.03 | backward-compute: 274.46 | backward-embedding-all-reduce: 0.01 | optimizer: 33.20 | batch-generator: 0.86
[2023-03-17 14:28:21,054] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.94
[2023-03-17 14:28:21,054] [INFO] [logging.py:93:log_dist] [Rank 0] step=10, skipped=0, lr=[5.4843458848123576e-05, 5.4843458848123576e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:21,054] [INFO] [timer.py:198:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=38.789714973713465, CurrSamplesPerSec=38.750283082221166, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:21,055] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 103.91 | backward_microstep: 272.87 | backward_inner_microstep: 252.99 | backward_allreduce_microstep: 19.80 | step_microstep: 33.40
[2023-03-17 14:28:21,055] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 103.89 | backward: 272.87 | backward_inner: 253.00 | backward_allreduce: 19.80 | step: 33.40
 iteration       10/      50 | consumed samples:          160 | consumed tokens:       163840 | elapsed time per iteration (ms): 414.9 | learning rate: 5.484E-05 | global batch size:    16 | lm loss: 8.200310E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.560 | TFLOPs: 181.69 |
time (ms) | forward-compute: 107.50 | backward-compute: 272.98 | backward-embedding-all-reduce: 0.01 | optimizer: 33.18 | batch-generator: 0.83
[2023-03-17 14:28:21,467] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.87
[2023-03-17 14:28:21,467] [INFO] [logging.py:93:log_dist] [Rank 0] step=11, skipped=0, lr=[5.380385755494631e-05, 5.380385755494631e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:21,468] [INFO] [timer.py:198:stop] epoch=0/micro_step=11/global_step=11, RunningAvgSamplesPerSec=38.80525265956592, CurrSamplesPerSec=38.93000370105242, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:21,468] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 103.50 | backward_microstep: 272.77 | backward_inner_microstep: 253.02 | backward_allreduce_microstep: 19.69 | step_microstep: 33.47
[2023-03-17 14:28:21,468] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 103.49 | backward: 272.77 | backward_inner: 253.01 | backward_allreduce: 19.70 | step: 33.47
 iteration       11/      50 | consumed samples:          176 | consumed tokens:       180224 | elapsed time per iteration (ms): 413.2 | learning rate: 5.380E-05 | global batch size:    16 | lm loss: 8.098831E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.718 | TFLOPs: 182.44 |
time (ms) | forward-compute: 105.84 | backward-compute: 272.83 | backward-embedding-all-reduce: 0.01 | optimizer: 33.26 | batch-generator: 0.93
[2023-03-17 14:28:21,882] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 23.00
[2023-03-17 14:28:21,882] [INFO] [logging.py:93:log_dist] [Rank 0] step=12, skipped=0, lr=[5.2682152940378117e-05, 5.2682152940378117e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:21,883] [INFO] [timer.py:198:stop] epoch=0/micro_step=12/global_step=12, RunningAvgSamplesPerSec=38.80095559160953, CurrSamplesPerSec=38.762324762272776, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:21,883] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 104.21 | backward_microstep: 273.46 | backward_inner_microstep: 256.10 | backward_allreduce_microstep: 17.30 | step_microstep: 33.36
[2023-03-17 14:28:21,883] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 104.20 | backward: 273.46 | backward_inner: 256.09 | backward_allreduce: 17.31 | step: 33.37
 iteration       12/      50 | consumed samples:          192 | consumed tokens:       196608 | elapsed time per iteration (ms): 414.9 | learning rate: 5.268E-05 | global batch size:    16 | lm loss: 8.132516E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.562 | TFLOPs: 181.70 |
time (ms) | forward-compute: 106.91 | backward-compute: 273.50 | backward-embedding-all-reduce: 0.01 | optimizer: 33.22 | batch-generator: 0.97
[2023-03-17 14:28:22,297] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.92
[2023-03-17 14:28:22,297] [INFO] [logging.py:93:log_dist] [Rank 0] step=13, skipped=0, lr=[5.14827718600746e-05, 5.14827718600746e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:22,298] [INFO] [timer.py:198:stop] epoch=0/micro_step=13/global_step=13, RunningAvgSamplesPerSec=38.795852140739974, CurrSamplesPerSec=38.74489137275052, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:22,298] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 102.90 | backward_microstep: 274.51 | backward_inner_microstep: 254.81 | backward_allreduce_microstep: 19.61 | step_microstep: 33.45
[2023-03-17 14:28:22,298] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 102.88 | backward: 274.50 | backward_inner: 254.82 | backward_allreduce: 19.61 | step: 33.46
 iteration       13/      50 | consumed samples:          208 | consumed tokens:       212992 | elapsed time per iteration (ms): 415.0 | learning rate: 5.148E-05 | global batch size:    16 | lm loss: 7.965299E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.554 | TFLOPs: 181.66 |
time (ms) | forward-compute: 105.89 | backward-compute: 274.58 | backward-embedding-all-reduce: 0.01 | optimizer: 33.25 | batch-generator: 0.94
[2023-03-17 14:28:22,714] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.97
[2023-03-17 14:28:22,714] [INFO] [logging.py:93:log_dist] [Rank 0] step=14, skipped=0, lr=[5.021044772321462e-05, 5.021044772321462e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:22,714] [INFO] [timer.py:198:stop] epoch=0/micro_step=14/global_step=14, RunningAvgSamplesPerSec=38.7789260426768, CurrSamplesPerSec=38.59370907867534, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:22,714] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 104.52 | backward_microstep: 274.74 | backward_inner_microstep: 255.27 | backward_allreduce_microstep: 19.41 | step_microstep: 33.43
[2023-03-17 14:28:22,715] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 104.51 | backward: 274.74 | backward_inner: 255.27 | backward_allreduce: 19.42 | step: 33.43
 iteration       14/      50 | consumed samples:          224 | consumed tokens:       229376 | elapsed time per iteration (ms): 416.7 | learning rate: 5.021E-05 | global batch size:    16 | lm loss: 7.624642E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.395 | TFLOPs: 180.91 |
time (ms) | forward-compute: 107.20 | backward-compute: 274.81 | backward-embedding-all-reduce: 0.01 | optimizer: 33.25 | batch-generator: 1.37
[2023-03-17 14:28:23,131] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.93
[2023-03-17 14:28:23,131] [INFO] [logging.py:93:log_dist] [Rank 0] step=15, skipped=0, lr=[4.887020181189677e-05, 4.887020181189677e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:23,131] [INFO] [timer.py:198:stop] epoch=0/micro_step=15/global_step=15, RunningAvgSamplesPerSec=38.763888629080185, CurrSamplesPerSec=38.584345104316334, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:23,131] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 103.14 | backward_microstep: 275.20 | backward_inner_microstep: 253.88 | backward_allreduce_microstep: 21.27 | step_microstep: 33.48
[2023-03-17 14:28:23,132] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 103.13 | backward: 275.20 | backward_inner: 253.88 | backward_allreduce: 21.27 | step: 33.48
 iteration       15/      50 | consumed samples:          240 | consumed tokens:       245760 | elapsed time per iteration (ms): 416.9 | learning rate: 4.887E-05 | global batch size:    16 | lm loss: 7.426762E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.378 | TFLOPs: 180.83 |
time (ms) | forward-compute: 107.07 | backward-compute: 275.29 | backward-embedding-all-reduce: 0.01 | optimizer: 33.28 | batch-generator: 0.86
[2023-03-17 14:28:23,547] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.90
[2023-03-17 14:28:23,548] [INFO] [logging.py:93:log_dist] [Rank 0] step=16, skipped=0, lr=[4.74673234644329e-05, 4.74673234644329e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:23,548] [INFO] [timer.py:198:stop] epoch=0/micro_step=16/global_step=16, RunningAvgSamplesPerSec=38.75070342374616, CurrSamplesPerSec=38.580108400959375, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:23,548] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 103.69 | backward_microstep: 275.49 | backward_inner_microstep: 252.12 | backward_allreduce_microstep: 23.29 | step_microstep: 33.51
[2023-03-17 14:28:23,548] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 103.68 | backward: 275.49 | backward_inner: 252.13 | backward_allreduce: 23.29 | step: 33.51
 iteration       16/      50 | consumed samples:          256 | consumed tokens:       262144 | elapsed time per iteration (ms): 416.7 | learning rate: 4.747E-05 | global batch size:    16 | lm loss: 7.504765E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.394 | TFLOPs: 180.91 |
time (ms) | forward-compute: 106.61 | backward-compute: 275.56 | backward-embedding-all-reduce: 0.01 | optimizer: 33.30 | batch-generator: 0.98
[2023-03-17 14:28:23,963] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.90
[2023-03-17 14:28:23,963] [INFO] [logging.py:93:log_dist] [Rank 0] step=17, skipped=0, lr=[4.6007349200746303e-05, 4.6007349200746303e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:23,964] [INFO] [timer.py:198:stop] epoch=0/micro_step=17/global_step=17, RunningAvgSamplesPerSec=38.74699418467692, CurrSamplesPerSec=38.69513929851692, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:23,964] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 102.92 | backward_microstep: 273.95 | backward_inner_microstep: 253.76 | backward_allreduce_microstep: 20.10 | step_microstep: 33.44
[2023-03-17 14:28:23,964] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 102.91 | backward: 273.94 | backward_inner: 253.77 | backward_allreduce: 20.10 | step: 33.44
 iteration       17/      50 | consumed samples:          272 | consumed tokens:       278528 | elapsed time per iteration (ms): 415.7 | learning rate: 4.601E-05 | global batch size:    16 | lm loss: 7.583458E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.489 | TFLOPs: 181.36 |
time (ms) | forward-compute: 107.20 | backward-compute: 274.04 | backward-embedding-all-reduce: 0.01 | optimizer: 33.22 | batch-generator: 1.09
[2023-03-17 14:28:24,379] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.91
[2023-03-17 14:28:24,379] [INFO] [logging.py:93:log_dist] [Rank 0] step=18, skipped=0, lr=[4.4496040872256956e-05, 4.4496040872256956e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:24,379] [INFO] [timer.py:198:stop] epoch=0/micro_step=18/global_step=18, RunningAvgSamplesPerSec=38.74435452117495, CurrSamplesPerSec=38.704802683486136, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:24,379] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 102.78 | backward_microstep: 272.84 | backward_inner_microstep: 252.04 | backward_allreduce_microstep: 20.72 | step_microstep: 33.43
[2023-03-17 14:28:24,379] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 102.77 | backward: 272.84 | backward_inner: 252.05 | backward_allreduce: 20.72 | step: 33.44
 iteration       18/      50 | consumed samples:          288 | consumed tokens:       294912 | elapsed time per iteration (ms): 415.4 | learning rate: 4.450E-05 | global batch size:    16 | lm loss: 7.614600E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.513 | TFLOPs: 181.47 |
time (ms) | forward-compute: 107.90 | backward-compute: 272.89 | backward-embedding-all-reduce: 0.01 | optimizer: 33.28 | batch-generator: 1.02
[2023-03-17 14:28:24,795] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.96
[2023-03-17 14:28:24,795] [INFO] [logging.py:93:log_dist] [Rank 0] step=19, skipped=0, lr=[4.293936292248631e-05, 4.293936292248631e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:24,796] [INFO] [timer.py:198:stop] epoch=0/micro_step=19/global_step=19, RunningAvgSamplesPerSec=38.74121923672798, CurrSamplesPerSec=38.69112360671257, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:24,796] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 101.12 | backward_microstep: 274.06 | backward_inner_microstep: 252.77 | backward_allreduce_microstep: 21.20 | step_microstep: 33.41
[2023-03-17 14:28:24,796] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 101.11 | backward: 274.05 | backward_inner: 252.78 | backward_allreduce: 21.20 | step: 33.41
 iteration       19/      50 | consumed samples:          304 | consumed tokens:       311296 | elapsed time per iteration (ms): 417.1 | learning rate: 4.294E-05 | global batch size:    16 | lm loss: 7.292959E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.360 | TFLOPs: 180.75 |
time (ms) | forward-compute: 107.79 | backward-compute: 274.17 | backward-embedding-all-reduce: 0.01 | optimizer: 33.16 | batch-generator: 0.86
[2023-03-17 14:28:25,210] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.90
[2023-03-17 14:28:25,210] [INFO] [logging.py:93:log_dist] [Rank 0] step=20, skipped=0, lr=[4.134345884812357e-05, 4.134345884812357e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:25,211] [INFO] [timer.py:198:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=38.74161954308182, CurrSamplesPerSec=38.74842601702627, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:25,211] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 102.64 | backward_microstep: 272.62 | backward_inner_microstep: 250.85 | backward_allreduce_microstep: 21.69 | step_microstep: 33.65
[2023-03-17 14:28:25,211] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 102.63 | backward: 272.62 | backward_inner: 250.86 | backward_allreduce: 21.69 | step: 33.66
 iteration       20/      50 | consumed samples:          320 | consumed tokens:       327680 | elapsed time per iteration (ms): 414.3 | learning rate: 4.134E-05 | global batch size:    16 | lm loss: 7.315308E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.620 | TFLOPs: 181.97 |
time (ms) | forward-compute: 106.89 | backward-compute: 272.68 | backward-embedding-all-reduce: 0.01 | optimizer: 33.46 | batch-generator: 1.16
[2023-03-17 14:28:25,624] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.98
[2023-03-17 14:28:25,625] [INFO] [logging.py:93:log_dist] [Rank 0] step=21, skipped=0, lr=[3.971462695345109e-05, 3.971462695345109e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:25,625] [INFO] [timer.py:198:stop] epoch=0/micro_step=21/global_step=21, RunningAvgSamplesPerSec=38.74550829951108, CurrSamplesPerSec=38.815639653354445, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:25,625] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 101.47 | backward_microstep: 273.22 | backward_inner_microstep: 252.43 | backward_allreduce_microstep: 20.71 | step_microstep: 33.50
[2023-03-17 14:28:25,625] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 101.46 | backward: 273.22 | backward_inner: 252.43 | backward_allreduce: 20.72 | step: 33.50
 iteration       21/      50 | consumed samples:          336 | consumed tokens:       344064 | elapsed time per iteration (ms): 414.5 | learning rate: 3.971E-05 | global batch size:    16 | lm loss: 7.191701E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.605 | TFLOPs: 181.90 |
time (ms) | forward-compute: 106.60 | backward-compute: 273.32 | backward-embedding-all-reduce: 0.01 | optimizer: 33.29 | batch-generator: 1.31
[2023-03-17 14:28:26,041] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.90
[2023-03-17 14:28:26,041] [INFO] [logging.py:93:log_dist] [Rank 0] step=22, skipped=0, lr=[3.805929549381457e-05, 3.805929549381457e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:26,042] [INFO] [timer.py:198:stop] epoch=0/micro_step=22/global_step=22, RunningAvgSamplesPerSec=38.73782513648301, CurrSamplesPerSec=38.59242181753774, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:26,042] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 102.55 | backward_microstep: 275.32 | backward_inner_microstep: 252.34 | backward_allreduce_microstep: 22.90 | step_microstep: 33.63
[2023-03-17 14:28:26,042] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 102.54 | backward: 275.32 | backward_inner: 252.35 | backward_allreduce: 22.90 | step: 33.63
 iteration       22/      50 | consumed samples:          352 | consumed tokens:       360448 | elapsed time per iteration (ms): 416.6 | learning rate: 3.806E-05 | global batch size:    16 | lm loss: 7.809849E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.402 | TFLOPs: 180.94 |
time (ms) | forward-compute: 106.53 | backward-compute: 275.38 | backward-embedding-all-reduce: 0.01 | optimizer: 33.43 | batch-generator: 0.92
[2023-03-17 14:28:26,459] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 23.03
[2023-03-17 14:28:26,460] [INFO] [logging.py:93:log_dist] [Rank 0] step=23, skipped=0, lr=[3.638399730623622e-05, 3.638399730623622e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:26,460] [INFO] [timer.py:198:stop] epoch=0/micro_step=23/global_step=23, RunningAvgSamplesPerSec=38.72845810360213, CurrSamplesPerSec=38.54206416894242, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:26,460] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 103.43 | backward_microstep: 274.92 | backward_inner_microstep: 255.04 | backward_allreduce_microstep: 19.79 | step_microstep: 34.17
[2023-03-17 14:28:26,460] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 103.42 | backward: 274.92 | backward_inner: 255.05 | backward_allreduce: 19.79 | step: 34.17
 iteration       23/      50 | consumed samples:          368 | consumed tokens:       376832 | elapsed time per iteration (ms): 418.1 | learning rate: 3.638E-05 | global batch size:    16 | lm loss: 7.397505E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.272 | TFLOPs: 180.33 |
time (ms) | forward-compute: 107.87 | backward-compute: 275.02 | backward-embedding-all-reduce: 0.01 | optimizer: 33.96 | batch-generator: 1.78
[2023-03-17 14:28:26,873] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.88
[2023-03-17 14:28:26,873] [INFO] [logging.py:93:log_dist] [Rank 0] step=24, skipped=0, lr=[3.469534402729146e-05, 3.469534402729146e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:26,874] [INFO] [timer.py:198:stop] epoch=0/micro_step=24/global_step=24, RunningAvgSamplesPerSec=38.735203897041245, CurrSamplesPerSec=38.877410399695044, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:26,874] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 103.57 | backward_microstep: 272.31 | backward_inner_microstep: 252.15 | backward_allreduce_microstep: 20.10 | step_microstep: 33.60
[2023-03-17 14:28:26,874] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 103.56 | backward: 272.30 | backward_inner: 252.14 | backward_allreduce: 20.11 | step: 33.61
 iteration       24/      50 | consumed samples:          384 | consumed tokens:       393216 | elapsed time per iteration (ms): 413.8 | learning rate: 3.470E-05 | global batch size:    16 | lm loss: 7.125862E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.666 | TFLOPs: 182.19 |
time (ms) | forward-compute: 106.68 | backward-compute: 272.38 | backward-embedding-all-reduce: 0.01 | optimizer: 33.40 | batch-generator: 0.94
[2023-03-17 14:28:27,289] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.97
[2023-03-17 14:28:27,290] [INFO] [logging.py:93:log_dist] [Rank 0] step=25, skipped=0, lr=[3.3e-05, 3.3e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:27,290] [INFO] [timer.py:198:stop] epoch=0/micro_step=25/global_step=25, RunningAvgSamplesPerSec=38.73086614696423, CurrSamplesPerSec=38.63568083658517, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:27,290] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 104.35 | backward_microstep: 275.08 | backward_inner_microstep: 255.66 | backward_allreduce_microstep: 19.36 | step_microstep: 33.46
[2023-03-17 14:28:27,290] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 104.34 | backward: 275.07 | backward_inner: 255.65 | backward_allreduce: 19.37 | step: 33.47
 iteration       25/      50 | consumed samples:          400 | consumed tokens:       409600 | elapsed time per iteration (ms): 416.3 | learning rate: 3.300E-05 | global batch size:    16 | lm loss: 7.563158E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.430 | TFLOPs: 181.07 |
time (ms) | forward-compute: 106.68 | backward-compute: 275.14 | backward-embedding-all-reduce: 0.01 | optimizer: 33.30 | batch-generator: 0.92
[2023-03-17 14:28:27,705] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.88
[2023-03-17 14:28:27,706] [INFO] [logging.py:93:log_dist] [Rank 0] step=26, skipped=0, lr=[3.1304655972708536e-05, 3.1304655972708536e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:27,706] [INFO] [timer.py:198:stop] epoch=0/micro_step=26/global_step=26, RunningAvgSamplesPerSec=38.72831362635778, CurrSamplesPerSec=38.669698370094466, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:27,706] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 103.51 | backward_microstep: 273.52 | backward_inner_microstep: 252.49 | backward_allreduce_microstep: 20.95 | step_microstep: 33.82
[2023-03-17 14:28:27,706] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 103.50 | backward: 273.52 | backward_inner: 252.50 | backward_allreduce: 20.95 | step: 33.82
 iteration       26/      50 | consumed samples:          416 | consumed tokens:       425984 | elapsed time per iteration (ms): 415.8 | learning rate: 3.130E-05 | global batch size:    16 | lm loss: 7.447141E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.476 | TFLOPs: 181.29 |
time (ms) | forward-compute: 107.27 | backward-compute: 273.64 | backward-embedding-all-reduce: 0.01 | optimizer: 33.57 | batch-generator: 1.09
[2023-03-17 14:28:28,121] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.94
[2023-03-17 14:28:28,122] [INFO] [logging.py:93:log_dist] [Rank 0] step=27, skipped=0, lr=[2.961600269376378e-05, 2.961600269376378e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:28,122] [INFO] [timer.py:198:stop] epoch=0/micro_step=27/global_step=27, RunningAvgSamplesPerSec=38.72545341170038, CurrSamplesPerSec=38.65693477748701, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:28,122] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 103.56 | backward_microstep: 274.03 | backward_inner_microstep: 254.58 | backward_allreduce_microstep: 19.38 | step_microstep: 33.42
[2023-03-17 14:28:28,122] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 103.54 | backward: 274.02 | backward_inner: 254.58 | backward_allreduce: 19.38 | step: 33.42
 iteration       27/      50 | consumed samples:          432 | consumed tokens:       442368 | elapsed time per iteration (ms): 416.0 | learning rate: 2.962E-05 | global batch size:    16 | lm loss: 7.453723E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.457 | TFLOPs: 181.20 |
time (ms) | forward-compute: 107.43 | backward-compute: 274.08 | backward-embedding-all-reduce: 0.01 | optimizer: 33.23 | batch-generator: 0.88
[2023-03-17 14:28:28,538] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.95
[2023-03-17 14:28:28,539] [INFO] [logging.py:93:log_dist] [Rank 0] step=28, skipped=0, lr=[2.7940704506185428e-05, 2.7940704506185428e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:28,539] [INFO] [timer.py:198:stop] epoch=0/micro_step=28/global_step=28, RunningAvgSamplesPerSec=38.71942624528559, CurrSamplesPerSec=38.569354459574484, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:28,539] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 103.94 | backward_microstep: 274.52 | backward_inner_microstep: 255.18 | backward_allreduce_microstep: 19.29 | step_microstep: 33.66
[2023-03-17 14:28:28,539] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 103.93 | backward: 274.52 | backward_inner: 255.17 | backward_allreduce: 19.30 | step: 33.67
 iteration       28/      50 | consumed samples:          448 | consumed tokens:       458752 | elapsed time per iteration (ms): 417.2 | learning rate: 2.794E-05 | global batch size:    16 | lm loss: 7.428530E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.354 | TFLOPs: 180.72 |
time (ms) | forward-compute: 107.75 | backward-compute: 274.61 | backward-embedding-all-reduce: 0.01 | optimizer: 33.44 | batch-generator: 0.96
[2023-03-17 14:28:28,955] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.88
[2023-03-17 14:28:28,955] [INFO] [logging.py:93:log_dist] [Rank 0] step=29, skipped=0, lr=[2.6285373046548923e-05, 2.6285373046548923e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:28,956] [INFO] [timer.py:198:stop] epoch=0/micro_step=29/global_step=29, RunningAvgSamplesPerSec=38.71449405737189, CurrSamplesPerSec=38.58669676529795, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:28,956] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 103.58 | backward_microstep: 274.51 | backward_inner_microstep: 254.05 | backward_allreduce_microstep: 20.37 | step_microstep: 33.46
[2023-03-17 14:28:28,956] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 103.57 | backward: 274.50 | backward_inner: 254.06 | backward_allreduce: 20.37 | step: 33.47
 iteration       29/      50 | consumed samples:          464 | consumed tokens:       475136 | elapsed time per iteration (ms): 416.7 | learning rate: 2.629E-05 | global batch size:    16 | lm loss: 7.864111E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.397 | TFLOPs: 180.92 |
time (ms) | forward-compute: 107.55 | backward-compute: 274.55 | backward-embedding-all-reduce: 0.01 | optimizer: 33.28 | batch-generator: 1.23
[2023-03-17 14:28:29,371] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.98
[2023-03-17 14:28:29,371] [INFO] [logging.py:93:log_dist] [Rank 0] step=30, skipped=0, lr=[2.465654115187642e-05, 2.465654115187642e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:29,371] [INFO] [timer.py:198:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=38.713661305109746, CurrSamplesPerSec=38.6911905280794, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:29,372] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 104.91 | backward_microstep: 273.89 | backward_inner_microstep: 255.88 | backward_allreduce_microstep: 17.94 | step_microstep: 33.81
[2023-03-17 14:28:29,372] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 104.88 | backward: 273.89 | backward_inner: 255.89 | backward_allreduce: 17.94 | step: 33.81
 iteration       30/      50 | consumed samples:          480 | consumed tokens:       491520 | elapsed time per iteration (ms): 415.7 | learning rate: 2.466E-05 | global batch size:    16 | lm loss: 7.320786E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.490 | TFLOPs: 181.36 |
time (ms) | forward-compute: 106.81 | backward-compute: 273.99 | backward-embedding-all-reduce: 0.01 | optimizer: 33.60 | batch-generator: 0.88
[2023-03-17 14:28:29,786] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.86
[2023-03-17 14:28:29,786] [INFO] [logging.py:93:log_dist] [Rank 0] step=31, skipped=0, lr=[2.3060637077513695e-05, 2.3060637077513695e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:29,786] [INFO] [timer.py:198:stop] epoch=0/micro_step=31/global_step=31, RunningAvgSamplesPerSec=38.714936065334236, CurrSamplesPerSec=38.75066346693521, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:29,787] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 104.76 | backward_microstep: 273.03 | backward_inner_microstep: 252.95 | backward_allreduce_microstep: 20.00 | step_microstep: 33.57
[2023-03-17 14:28:29,787] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 104.75 | backward: 273.02 | backward_inner: 252.96 | backward_allreduce: 20.00 | step: 33.57
 iteration       31/      50 | consumed samples:          496 | consumed tokens:       507904 | elapsed time per iteration (ms): 414.9 | learning rate: 2.306E-05 | global batch size:    16 | lm loss: 7.409903E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.563 | TFLOPs: 181.70 |
time (ms) | forward-compute: 107.15 | backward-compute: 273.09 | backward-embedding-all-reduce: 0.01 | optimizer: 33.37 | batch-generator: 1.67
[2023-03-17 14:28:30,202] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 23.01
[2023-03-17 14:28:30,203] [INFO] [logging.py:93:log_dist] [Rank 0] step=32, skipped=0, lr=[2.150395912774304e-05, 2.150395912774304e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:30,203] [INFO] [timer.py:198:stop] epoch=0/micro_step=32/global_step=32, RunningAvgSamplesPerSec=38.711592255912244, CurrSamplesPerSec=38.61487241513043, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:30,203] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 103.95 | backward_microstep: 274.98 | backward_inner_microstep: 256.66 | backward_allreduce_microstep: 18.25 | step_microstep: 33.59
[2023-03-17 14:28:30,203] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 103.94 | backward: 274.98 | backward_inner: 256.66 | backward_allreduce: 18.26 | step: 33.59
 iteration       32/      50 | consumed samples:          512 | consumed tokens:       524288 | elapsed time per iteration (ms): 416.5 | learning rate: 2.150E-05 | global batch size:    16 | lm loss: 7.499984E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.413 | TFLOPs: 181.00 |
time (ms) | forward-compute: 106.79 | backward-compute: 275.09 | backward-embedding-all-reduce: 0.01 | optimizer: 33.35 | batch-generator: 0.84
[2023-03-17 14:28:30,619] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.89
[2023-03-17 14:28:30,619] [INFO] [logging.py:93:log_dist] [Rank 0] step=33, skipped=0, lr=[1.999265079925368e-05, 1.999265079925368e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:30,619] [INFO] [timer.py:198:stop] epoch=0/micro_step=33/global_step=33, RunningAvgSamplesPerSec=38.709435591390644, CurrSamplesPerSec=38.64484720889762, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:30,619] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 105.66 | backward_microstep: 274.75 | backward_inner_microstep: 256.07 | backward_allreduce_microstep: 18.60 | step_microstep: 33.42
[2023-03-17 14:28:30,619] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 105.65 | backward: 274.75 | backward_inner: 256.07 | backward_allreduce: 18.60 | step: 33.42
 iteration       33/      50 | consumed samples:          528 | consumed tokens:       540672 | elapsed time per iteration (ms): 416.1 | learning rate: 1.999E-05 | global batch size:    16 | lm loss: 7.177189E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.451 | TFLOPs: 181.17 |
time (ms) | forward-compute: 106.79 | backward-compute: 274.82 | backward-embedding-all-reduce: 0.01 | optimizer: 33.23 | batch-generator: 1.50
[2023-03-17 14:28:31,032] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.96
[2023-03-17 14:28:31,032] [INFO] [logging.py:93:log_dist] [Rank 0] step=34, skipped=0, lr=[1.853267653556708e-05, 1.853267653556708e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:31,033] [INFO] [timer.py:198:stop] epoch=0/micro_step=34/global_step=34, RunningAvgSamplesPerSec=38.714761240508636, CurrSamplesPerSec=38.880586318274645, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:31,033] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 104.07 | backward_microstep: 272.53 | backward_inner_microstep: 255.61 | backward_allreduce_microstep: 16.87 | step_microstep: 33.58
[2023-03-17 14:28:31,033] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 104.06 | backward: 272.53 | backward_inner: 255.60 | backward_allreduce: 16.88 | step: 33.58
 iteration       34/      50 | consumed samples:          544 | consumed tokens:       557056 | elapsed time per iteration (ms): 413.6 | learning rate: 1.853E-05 | global batch size:    16 | lm loss: 7.401935E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.686 | TFLOPs: 182.28 |
time (ms) | forward-compute: 106.32 | backward-compute: 272.64 | backward-embedding-all-reduce: 0.01 | optimizer: 33.32 | batch-generator: 0.88
[2023-03-17 14:28:31,448] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.96
[2023-03-17 14:28:31,449] [INFO] [logging.py:93:log_dist] [Rank 0] step=35, skipped=0, lr=[1.712979818810323e-05, 1.712979818810323e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:31,449] [INFO] [timer.py:198:stop] epoch=0/micro_step=35/global_step=35, RunningAvgSamplesPerSec=38.714922319205456, CurrSamplesPerSec=38.72007754532132, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:31,449] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 104.42 | backward_microstep: 274.06 | backward_inner_microstep: 254.11 | backward_allreduce_microstep: 19.87 | step_microstep: 33.45
[2023-03-17 14:28:31,449] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 104.41 | backward: 274.06 | backward_inner: 254.12 | backward_allreduce: 19.87 | step: 33.46
 iteration       35/      50 | consumed samples:          560 | consumed tokens:       573440 | elapsed time per iteration (ms): 416.3 | learning rate: 1.713E-05 | global batch size:    16 | lm loss: 7.335943E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.438 | TFLOPs: 181.11 |
time (ms) | forward-compute: 107.55 | backward-compute: 274.12 | backward-embedding-all-reduce: 0.01 | optimizer: 33.25 | batch-generator: 0.77
[2023-03-17 14:28:31,865] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.95
[2023-03-17 14:28:31,865] [INFO] [logging.py:93:log_dist] [Rank 0] step=36, skipped=0, lr=[1.5789552276785377e-05, 1.5789552276785377e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:31,865] [INFO] [timer.py:198:stop] epoch=0/micro_step=36/global_step=36, RunningAvgSamplesPerSec=38.712601498146405, CurrSamplesPerSec=38.63617019316685, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:31,865] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 104.34 | backward_microstep: 273.61 | backward_inner_microstep: 253.84 | backward_allreduce_microstep: 19.69 | step_microstep: 33.56
[2023-03-17 14:28:31,865] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 104.32 | backward: 273.61 | backward_inner: 253.85 | backward_allreduce: 19.69 | step: 33.56
 iteration       36/      50 | consumed samples:          576 | consumed tokens:       589824 | elapsed time per iteration (ms): 416.2 | learning rate: 1.579E-05 | global batch size:    16 | lm loss: 7.132134E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.444 | TFLOPs: 181.14 |
time (ms) | forward-compute: 107.85 | backward-compute: 273.71 | backward-embedding-all-reduce: 0.01 | optimizer: 33.34 | batch-generator: 1.17
[2023-03-17 14:28:32,282] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.96
[2023-03-17 14:28:32,283] [INFO] [logging.py:93:log_dist] [Rank 0] step=37, skipped=0, lr=[1.4517228139925405e-05, 1.4517228139925405e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:32,283] [INFO] [timer.py:198:stop] epoch=0/micro_step=37/global_step=37, RunningAvgSamplesPerSec=38.706599438193166, CurrSamplesPerSec=38.503630969354354, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:32,283] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 105.94 | backward_microstep: 274.98 | backward_inner_microstep: 256.55 | backward_allreduce_microstep: 18.35 | step_microstep: 33.48
[2023-03-17 14:28:32,283] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 105.93 | backward: 274.98 | backward_inner: 256.56 | backward_allreduce: 18.35 | step: 33.49
 iteration       37/      50 | consumed samples:          592 | consumed tokens:       606208 | elapsed time per iteration (ms): 417.7 | learning rate: 1.452E-05 | global batch size:    16 | lm loss: 7.274084E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.301 | TFLOPs: 180.47 |
time (ms) | forward-compute: 108.07 | backward-compute: 275.03 | backward-embedding-all-reduce: 0.01 | optimizer: 33.32 | batch-generator: 1.81
[2023-03-17 14:28:32,700] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.92
[2023-03-17 14:28:32,700] [INFO] [logging.py:93:log_dist] [Rank 0] step=38, skipped=0, lr=[1.3317847059621894e-05, 1.3317847059621894e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:32,701] [INFO] [timer.py:198:stop] epoch=0/micro_step=38/global_step=38, RunningAvgSamplesPerSec=38.70066099610379, CurrSamplesPerSec=38.49395736167094, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:32,701] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 104.33 | backward_microstep: 274.76 | backward_inner_microstep: 254.10 | backward_allreduce_microstep: 20.59 | step_microstep: 33.43
[2023-03-17 14:28:32,701] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 104.32 | backward: 274.76 | backward_inner: 254.10 | backward_allreduce: 20.59 | step: 33.43
 iteration       38/      50 | consumed samples:          608 | consumed tokens:       622592 | elapsed time per iteration (ms): 417.8 | learning rate: 1.332E-05 | global batch size:    16 | lm loss: 7.421062E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.295 | TFLOPs: 180.44 |
time (ms) | forward-compute: 108.36 | backward-compute: 274.82 | backward-embedding-all-reduce: 0.01 | optimizer: 33.25 | batch-generator: 1.10
[2023-03-17 14:28:33,117] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.95
[2023-03-17 14:28:33,117] [INFO] [logging.py:93:log_dist] [Rank 0] step=39, skipped=0, lr=[1.2196142445053694e-05, 1.2196142445053694e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:33,118] [INFO] [timer.py:198:stop] epoch=0/micro_step=39/global_step=39, RunningAvgSamplesPerSec=38.69676812222642, CurrSamplesPerSec=38.55714436737865, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:33,118] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 104.03 | backward_microstep: 275.70 | backward_inner_microstep: 255.53 | backward_allreduce_microstep: 20.08 | step_microstep: 33.58
[2023-03-17 14:28:33,118] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 104.01 | backward: 275.70 | backward_inner: 255.54 | backward_allreduce: 20.08 | step: 33.58
 iteration       39/      50 | consumed samples:          624 | consumed tokens:       638976 | elapsed time per iteration (ms): 417.1 | learning rate: 1.220E-05 | global batch size:    16 | lm loss: 7.180298E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.364 | TFLOPs: 180.77 |
time (ms) | forward-compute: 106.64 | backward-compute: 275.79 | backward-embedding-all-reduce: 0.01 | optimizer: 33.37 | batch-generator: 0.95
[2023-03-17 14:28:33,533] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 23.01
[2023-03-17 14:28:33,533] [INFO] [logging.py:93:log_dist] [Rank 0] step=40, skipped=0, lr=[1.1156541151876422e-05, 1.1156541151876422e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:33,533] [INFO] [timer.py:198:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=38.69665303140597, CurrSamplesPerSec=38.692395152269924, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:33,534] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 104.60 | backward_microstep: 274.11 | backward_inner_microstep: 254.67 | backward_allreduce_microstep: 19.38 | step_microstep: 33.48
[2023-03-17 14:28:33,534] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 104.58 | backward: 274.10 | backward_inner: 254.67 | backward_allreduce: 19.39 | step: 33.49
 iteration       40/      50 | consumed samples:          640 | consumed tokens:       655360 | elapsed time per iteration (ms): 415.6 | learning rate: 1.116E-05 | global batch size:    16 | lm loss: 7.219608E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.494 | TFLOPs: 181.38 |
time (ms) | forward-compute: 106.81 | backward-compute: 274.14 | backward-embedding-all-reduce: 0.01 | optimizer: 33.33 | batch-generator: 1.11
[2023-03-17 14:28:33,949] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.88
[2023-03-17 14:28:33,949] [INFO] [logging.py:93:log_dist] [Rank 0] step=41, skipped=0, lr=[1.0203146011445599e-05, 1.0203146011445599e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:33,949] [INFO] [timer.py:198:stop] epoch=0/micro_step=41/global_step=41, RunningAvgSamplesPerSec=38.69663996252826, CurrSamplesPerSec=38.6961433517159, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:33,949] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 103.67 | backward_microstep: 274.93 | backward_inner_microstep: 254.19 | backward_allreduce_microstep: 20.68 | step_microstep: 33.43
[2023-03-17 14:28:33,949] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 103.66 | backward: 274.92 | backward_inner: 254.19 | backward_allreduce: 20.68 | step: 33.43
 iteration       41/      50 | consumed samples:          656 | consumed tokens:       671744 | elapsed time per iteration (ms): 415.6 | learning rate: 1.020E-05 | global batch size:    16 | lm loss: 7.564805E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.495 | TFLOPs: 181.38 |
time (ms) | forward-compute: 106.11 | backward-compute: 275.04 | backward-embedding-all-reduce: 0.01 | optimizer: 33.19 | batch-generator: 0.82
[2023-03-17 14:28:34,365] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 23.01
[2023-03-17 14:28:34,365] [INFO] [logging.py:93:log_dist] [Rank 0] step=42, skipped=0, lr=[9.33971963881569e-06, 9.33971963881569e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:34,366] [INFO] [timer.py:198:stop] epoch=0/micro_step=42/global_step=42, RunningAvgSamplesPerSec=38.694663507058316, CurrSamplesPerSec=38.61773891066325, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:34,366] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 104.59 | backward_microstep: 274.04 | backward_inner_microstep: 256.67 | backward_allreduce_microstep: 17.31 | step_microstep: 33.44
[2023-03-17 14:28:34,366] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 104.58 | backward: 274.04 | backward_inner: 256.66 | backward_allreduce: 17.32 | step: 33.44
 iteration       42/      50 | consumed samples:          672 | consumed tokens:       688128 | elapsed time per iteration (ms): 416.6 | learning rate: 9.340E-06 | global batch size:    16 | lm loss: 7.299363E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.410 | TFLOPs: 180.98 |
time (ms) | forward-compute: 107.91 | backward-compute: 274.09 | backward-embedding-all-reduce: 0.01 | optimizer: 33.24 | batch-generator: 1.41
[2023-03-17 14:28:34,782] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.92
[2023-03-17 14:28:34,782] [INFO] [logging.py:93:log_dist] [Rank 0] step=43, skipped=0, lr=[8.569669583417477e-06, 8.569669583417477e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:34,783] [INFO] [timer.py:198:stop] epoch=0/micro_step=43/global_step=43, RunningAvgSamplesPerSec=38.69147617066263, CurrSamplesPerSec=38.564411875580404, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:34,783] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 104.32 | backward_microstep: 274.25 | backward_inner_microstep: 253.16 | backward_allreduce_microstep: 21.01 | step_microstep: 33.53
[2023-03-17 14:28:34,783] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 104.30 | backward: 274.25 | backward_inner: 253.17 | backward_allreduce: 21.01 | step: 33.53
 iteration       43/      50 | consumed samples:          688 | consumed tokens:       704512 | elapsed time per iteration (ms): 417.1 | learning rate: 8.570E-06 | global batch size:    16 | lm loss: 7.553797E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.364 | TFLOPs: 180.77 |
time (ms) | forward-compute: 108.09 | backward-compute: 274.33 | backward-embedding-all-reduce: 0.01 | optimizer: 33.32 | batch-generator: 1.16
[2023-03-17 14:28:35,198] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 23.01
[2023-03-17 14:28:35,198] [INFO] [logging.py:93:log_dist] [Rank 0] step=44, skipped=0, lr=[7.896034881017213e-06, 7.896034881017213e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:35,198] [INFO] [timer.py:198:stop] epoch=0/micro_step=44/global_step=44, RunningAvgSamplesPerSec=38.69193358325517, CurrSamplesPerSec=38.71069681587448, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:35,198] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 104.12 | backward_microstep: 273.83 | backward_inner_microstep: 255.31 | backward_allreduce_microstep: 18.45 | step_microstep: 33.44
[2023-03-17 14:28:35,199] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 104.11 | backward: 273.82 | backward_inner: 255.31 | backward_allreduce: 18.45 | step: 33.44
 iteration       44/      50 | consumed samples:          704 | consumed tokens:       720896 | elapsed time per iteration (ms): 415.4 | learning rate: 7.896E-06 | global batch size:    16 | lm loss: 7.077693E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.513 | TFLOPs: 181.47 |
time (ms) | forward-compute: 106.95 | backward-compute: 273.91 | backward-embedding-all-reduce: 0.01 | optimizer: 33.24 | batch-generator: 0.87
[2023-03-17 14:28:35,616] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.99
[2023-03-17 14:28:35,617] [INFO] [logging.py:93:log_dist] [Rank 0] step=45, skipped=0, lr=[7.3214740600308545e-06, 7.3214740600308545e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:35,617] [INFO] [timer.py:198:stop] epoch=0/micro_step=45/global_step=45, RunningAvgSamplesPerSec=38.6864708048899, CurrSamplesPerSec=38.45841881703546, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:35,617] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 104.26 | backward_microstep: 275.78 | backward_inner_microstep: 255.79 | backward_allreduce_microstep: 19.93 | step_microstep: 33.45
[2023-03-17 14:28:35,617] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 104.25 | backward: 275.78 | backward_inner: 255.79 | backward_allreduce: 19.94 | step: 33.45
 iteration       45/      50 | consumed samples:          720 | consumed tokens:       737280 | elapsed time per iteration (ms): 418.6 | learning rate: 7.321E-06 | global batch size:    16 | lm loss: 7.380528E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.218 | TFLOPs: 180.08 |
time (ms) | forward-compute: 108.31 | backward-compute: 275.83 | backward-embedding-all-reduce: 0.01 | optimizer: 33.27 | batch-generator: 1.27
[2023-03-17 14:28:36,033] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.91
[2023-03-17 14:28:36,033] [INFO] [logging.py:93:log_dist] [Rank 0] step=46, skipped=0, lr=[6.848254649526961e-06, 6.848254649526961e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:36,034] [INFO] [timer.py:198:stop] epoch=0/micro_step=46/global_step=46, RunningAvgSamplesPerSec=38.68390861081721, CurrSamplesPerSec=38.57405441384666, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:36,034] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 104.40 | backward_microstep: 274.63 | backward_inner_microstep: 253.60 | backward_allreduce_microstep: 20.97 | step_microstep: 33.42
[2023-03-17 14:28:36,034] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 104.39 | backward: 274.62 | backward_inner: 253.59 | backward_allreduce: 20.98 | step: 33.42
 iteration       46/      50 | consumed samples:          736 | consumed tokens:       753664 | elapsed time per iteration (ms): 416.8 | learning rate: 6.848E-06 | global batch size:    16 | lm loss: 7.448695E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.385 | TFLOPs: 180.86 |
time (ms) | forward-compute: 107.53 | backward-compute: 274.71 | backward-embedding-all-reduce: 0.01 | optimizer: 33.22 | batch-generator: 1.08
[2023-03-17 14:28:36,451] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 23.33
[2023-03-17 14:28:36,451] [INFO] [logging.py:93:log_dist] [Rank 0] step=47, skipped=0, lr=[6.478244230325408e-06, 6.478244230325408e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:36,451] [INFO] [timer.py:198:stop] epoch=0/micro_step=47/global_step=47, RunningAvgSamplesPerSec=38.68028639176964, CurrSamplesPerSec=38.52157755734279, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:36,451] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 104.77 | backward_microstep: 274.40 | backward_inner_microstep: 255.18 | backward_allreduce_microstep: 19.15 | step_microstep: 33.93
[2023-03-17 14:28:36,452] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 104.75 | backward: 274.39 | backward_inner: 255.18 | backward_allreduce: 19.16 | step: 33.94
 iteration       47/      50 | consumed samples:          752 | consumed tokens:       770048 | elapsed time per iteration (ms): 417.5 | learning rate: 6.478E-06 | global batch size:    16 | lm loss: 7.286203E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.322 | TFLOPs: 180.57 |
time (ms) | forward-compute: 107.93 | backward-compute: 274.42 | backward-embedding-all-reduce: 0.01 | optimizer: 33.79 | batch-generator: 1.16
[2023-03-17 14:28:36,866] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.91
[2023-03-17 14:28:36,867] [INFO] [logging.py:93:log_dist] [Rank 0] step=48, skipped=0, lr=[6.2129030645091e-06, 6.2129030645091e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:36,867] [INFO] [timer.py:198:stop] epoch=0/micro_step=48/global_step=48, RunningAvgSamplesPerSec=38.68031496548408, CurrSamplesPerSec=38.68160082632814, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:36,867] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 103.78 | backward_microstep: 273.95 | backward_inner_microstep: 253.30 | backward_allreduce_microstep: 20.57 | step_microstep: 33.62
[2023-03-17 14:28:36,867] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 103.76 | backward: 273.95 | backward_inner: 253.31 | backward_allreduce: 20.56 | step: 33.63
 iteration       48/      50 | consumed samples:          768 | consumed tokens:       786432 | elapsed time per iteration (ms): 415.6 | learning rate: 6.213E-06 | global batch size:    16 | lm loss: 7.830332E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.495 | TFLOPs: 181.38 |
time (ms) | forward-compute: 106.96 | backward-compute: 274.03 | backward-embedding-all-reduce: 0.01 | optimizer: 33.42 | batch-generator: 0.95
[2023-03-17 14:28:37,283] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.93
[2023-03-17 14:28:37,283] [INFO] [logging.py:93:log_dist] [Rank 0] step=49, skipped=0, lr=[6.053278332436668e-06, 6.053278332436668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:37,283] [INFO] [timer.py:198:stop] epoch=0/micro_step=49/global_step=49, RunningAvgSamplesPerSec=38.67892595316023, CurrSamplesPerSec=38.615139047925936, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:37,283] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 104.06 | backward_microstep: 273.95 | backward_inner_microstep: 254.17 | backward_allreduce_microstep: 19.73 | step_microstep: 33.51
[2023-03-17 14:28:37,284] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 104.04 | backward: 273.94 | backward_inner: 254.16 | backward_allreduce: 19.74 | step: 33.51
 iteration       49/      50 | consumed samples:          784 | consumed tokens:       802816 | elapsed time per iteration (ms): 416.4 | learning rate: 6.053E-06 | global batch size:    16 | lm loss: 7.231093E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.427 | TFLOPs: 181.06 |
time (ms) | forward-compute: 107.72 | backward-compute: 274.01 | backward-embedding-all-reduce: 0.01 | optimizer: 33.31 | batch-generator: 0.77
[2023-03-17 14:28:37,701] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 22.99
[2023-03-17 14:28:37,701] [INFO] [logging.py:93:log_dist] [Rank 0] step=50, skipped=0, lr=[6e-06, 6e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:28:37,701] [INFO] [timer.py:198:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=38.67461365342578, CurrSamplesPerSec=38.47301454443304, MemAllocated=9.27GB, MaxMemAllocated=20.22GB
[2023-03-17 14:28:37,702] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 104.11 | backward_microstep: 276.45 | backward_inner_microstep: 255.08 | backward_allreduce_microstep: 21.31 | step_microstep: 33.54
[2023-03-17 14:28:37,702] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 104.10 | backward: 276.45 | backward_inner: 255.08 | backward_allreduce: 21.32 | step: 33.54
 iteration       50/      50 | consumed samples:          800 | consumed tokens:       819200 | elapsed time per iteration (ms): 417.9 | learning rate: 6.000E-06 | global batch size:    16 | lm loss: 7.284102E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 38.285 | TFLOPs: 180.39 |
time (ms) | forward-compute: 106.86 | backward-compute: 276.55 | backward-embedding-all-reduce: 0.01 | optimizer: 33.30 | batch-generator: 0.88
[after training is done] datetime: 2023-03-17 14:28:37 
------------------------------------------------------------------------------------------------------------------
 validation loss at the end of training for val data | lm loss value: 7.441185E+00 | lm loss PPL: 1.704769E+03 | 
------------------------------------------------------------------------------------------------------------------
gpu108-16-r:60336:60630 [1] NCCL INFO [Service thread] Connection closed by localRank 1
gpu108-16-r:60335:60629 [0] NCCL INFO [Service thread] Connection closed by localRank 0
gpu108-16-r:60338:60628 [3] NCCL INFO [Service thread] Connection closed by localRank 3
gpu108-16-r:60337:60627 [2] NCCL INFO [Service thread] Connection closed by localRank 2
gpu108-16-r:60336:60336 [1] NCCL INFO comm 0x38771a30 rank 1 nranks 4 cudaDev 1 busId 46000 - Abort COMPLETE
gpu108-16-r:60335:60335 [0] NCCL INFO comm 0x3ab91db0 rank 0 nranks 4 cudaDev 0 busId 7000 - Abort COMPLETE
gpu108-16-r:60338:60338 [3] NCCL INFO comm 0x38ac6ed0 rank 3 nranks 4 cudaDev 3 busId c7000 - Abort COMPLETE
gpu108-16-r:60337:60337 [2] NCCL INFO comm 0x3a2d5e50 rank 2 nranks 4 cudaDev 2 busId 85000 - Abort COMPLETE
[2023-03-17 14:28:44,050] [INFO] [launch.py:350:main] Process 60337 exits successfully.
[2023-03-17 14:28:44,050] [INFO] [launch.py:350:main] Process 60338 exits successfully.
[2023-03-17 14:28:44,050] [INFO] [launch.py:350:main] Process 60335 exits successfully.
[2023-03-17 14:28:44,050] [INFO] [launch.py:350:main] Process 60336 exits successfully.
