------- JOB Configuration ---------
scontrol show job 24295994
JobId=24295994 JobName=g4
   UserId=shaima0d(174988) GroupId=g-shaima0d(1174988) MCS_label=N/A
   Priority=9694 Nice=0 Account=a100_training_acc QOS=a100_training_qos
   JobState=RUNNING Reason=None Dependency=(null)
   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=00:00:01 TimeLimit=00:15:00 TimeMin=N/A
   SubmitTime=2023-03-17T11:16:12 EligibleTime=2023-03-17T11:16:12
   AccrueTime=2023-03-17T11:16:12
   StartTime=2023-03-17T11:16:12 EndTime=2023-03-17T11:31:13 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2023-03-17T11:16:12 Scheduler=Main
   Partition=a100_training AllocNode:Sid=login510-22:43766
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=gpu108-23-r
   BatchHost=gpu108-23-r
   NumNodes=1 NumCPUs=16 NumTasks=4 CPUs/Task=4 ReqB:S:C:T=0:0:*:*
   TRES=cpu=16,mem=32G,node=1,billing=16,gres/gpu=4
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
   MinCPUsNode=4 MinMemoryCPU=2G MinTmpDiskNode=0
   Features=(a100)&el7 DelayBoot=00:00:00
   Reservation=A100
   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
   Command=/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/test.slurm
   WorkDir=/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed
   StdErr=/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/slurm-24295994.out
   StdIn=/dev/null
   StdOut=/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/slurm-24295994.out
   Power=
   TresPerJob=gres:gpu:4
   TresPerNode=gres:gpu:4
   

------- GPU Configuration ---------
nvidia-smi -L
GPU 0: NVIDIA A100-SXM4-80GB (UUID: GPU-b213bccc-3ee2-861c-2151-d700ecb7a781)
GPU 1: NVIDIA A100-SXM4-80GB (UUID: GPU-d0b85b20-df35-2628-d05c-c0db3be5d985)
GPU 2: NVIDIA A100-SXM4-80GB (UUID: GPU-da281192-0921-6a9a-bec4-99f49b455612)
GPU 3: NVIDIA A100-SXM4-80GB (UUID: GPU-7767a19d-0002-95f4-a3be-2ef4043f3f1f)
------- NVLink Configuration ------
nvidia-smi topo -m
	[4mGPU0	GPU1	GPU2	GPU3	mlx5_0	mlx5_1	CPU Affinity	NUMA Affinity[0m
GPU0	 X 	NV4	NV4	NV4	SYS	SYS	0-15	0-1
GPU1	NV4	 X 	NV4	NV4	PHB	SYS	0-15	0-1
GPU2	NV4	NV4	 X 	NV4	SYS	SYS	0-15	0-1
GPU3	NV4	NV4	NV4	 X 	SYS	PHB	0-15	0-1
mlx5_0	SYS	PHB	SYS	SYS	 X 	SYS		
mlx5_1	SYS	SYS	SYS	PHB	SYS	 X 		

Legend:

  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing at most a single PCIe bridge
  NV#  = Connection traversing a bonded set of # NVLinks
------- Infiniband Configuration --
ibv_devinfo
hca_id:	mlx5_0
	transport:			InfiniBand (0)
	fw_ver:				20.34.1002
	node_guid:			88e9:a4ff:ff1a:6ec8
	sys_image_guid:			88e9:a4ff:ff1a:6ec8
	vendor_id:			0x02c9
	vendor_part_id:			4123
	hw_ver:				0x0
	board_id:			MT_0000000451
	phys_port_cnt:			1
		port:	1
			state:			PORT_ACTIVE (4)
			max_mtu:		4096 (5)
			active_mtu:		4096 (5)
			sm_lid:			1
			port_lid:		664
			port_lmc:		0x00
			link_layer:		InfiniBand

hca_id:	mlx5_1
	transport:			InfiniBand (0)
	fw_ver:				20.34.1002
	node_guid:			88e9:a4ff:ff1a:6ec4
	sys_image_guid:			88e9:a4ff:ff1a:6ec4
	vendor_id:			0x02c9
	vendor_part_id:			4123
	hw_ver:				0x0
	board_id:			MT_0000000451
	phys_port_cnt:			1
		port:	1
			state:			PORT_ACTIVE (4)
			max_mtu:		4096 (5)
			active_mtu:		4096 (5)
			sm_lid:			1
			port_lid:		692
			port_lmc:		0x00
			link_layer:		InfiniBand

Loading module for CUDA 11.7.0
CUDA 11.7.0 is now loaded
GNU 11.1.0 is now loaded
Loading module for nccl-2.17.1.1
nccl-2.17.1.1 modules now loaded
Loading module for gdrcopy 2.0_cuda11.7.0
gdrcopy 2.0_cuda11.7.0 modules now loaded
Loading module for ucx-gpu 1.14.0
ucx-gpu 1.14.0 modules now loaded
Loading module for OPENMPI 4.1.4
OPENMPI 4.1.4 modules now loaded
Loading module for pytorch-1.13.1_cuda11.7.0
pytorch-1.13.1_cuda11.7.0 modules now loaded
Loading module for deepspeed-0.8.3
deepspeed-0.8.3 modules now loaded
Loading module for apex-22.03
apex-22.03 modules now loaded
Currently Loaded Modulefiles:
  1) dl/2023             5) nccl/2.17.1.1       9) pytorch/1.13.1
  2) python/3.9.16       6) gdrcopy/2.0        10) deepspeed/0.8.3
  3) cuda/11.7.0         7) ucx/1.14.0         11) apex/22.03
  4) gcc/11.1.0          8) openmpi-gpu/4.1.4
######################################################################
#     ___       __          _          ____        __                #
#    /   | ____/ /___ ___  (_)___     / __ \____  / /_  __           #
#   / /| |/ __  / __ `__ \/ / __ \   / / / / __ \/ / / / /           #
#  / ___ / /_/ / / / / / / / / / /  / /_/ / / / / / /_/ /            #
# /_/  |_\__,_/_/ /_/ /_/_/_/ /_/   \____/_/ /_/_/\__, /             #
#                                                /____/              #
#                                                                    #
# Access granted to Administers Only.                                #
#                                                                    #
# Please be aware that jobs maybe running.                           #
#                                                                    #
#                                             - Your Ibex Admin Team #
#                                              ibex@hpc.kaust.edu.sa #
#                              https://kaust-ibex.slack.com #general #
######################################################################
[2023-03-17 11:16:15,837] [INFO] [runner.py:454:main] Using IP address of 10.109.8.134 for node gpu108-23-r
[2023-03-17 11:16:15,837] [INFO] [runner.py:550:main] cmd = /sw/csgv/dl/apps/python/3.9.16/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJncHUxMDgtMjMtciI6IFswLCAxLCAyLCAzXX0= --master_addr=10.109.8.134 --master_port=29500 --enable_each_rank_log=None pretrain_gpt.py --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 1 --hidden-size 1024 --num-attention-heads 32 --seq-length 1024 --loss-scale 15 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 16 --train-iters 50 --lr 6.0e-5 --min-lr 6.0e-6 --lr-decay-style cosine --log-interval 1 --eval-iters 40 --eval-interval 1000 --data-path /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document --vocab-file /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json --merge-file /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt --save-interval 1000 --split 98,2,0 --clip-grad 1.0 --weight-decay 0.1 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.006 --fp16 --checkpoint-activations --tensorboard-dir ds_z_off-none_stage_3_nl1_hs1024_mb4_seq1024_gb16_nodes1 --deepspeed-activation-checkpointing --zero-stage=3 --deepspeed_config=ds_config.json --no-pipeline-parallel --deepspeed --no-masked-softmax-fusion --no-bias-dropout-fusion --no-bias-gelu-fusion --exit-interval 5000
[2023-03-17 11:16:17,483] [INFO] [launch.py:135:main] 0 NCCL_SOCKET_IFNAME=ib0
[2023-03-17 11:16:17,483] [INFO] [launch.py:135:main] 0 NCCL_TREE_THRESHOLD=0
[2023-03-17 11:16:17,483] [INFO] [launch.py:135:main] 0 NCCL_HOME=/sw/csgv/dl/apps/nccl/2.17.1.1
[2023-03-17 11:16:17,483] [INFO] [launch.py:135:main] 0 NCCL_TOPO_DUMP_FILE=./nccl_dump.log.g4
[2023-03-17 11:16:17,483] [INFO] [launch.py:135:main] 0 NCCL_DEBUG=INFO
[2023-03-17 11:16:17,483] [INFO] [launch.py:135:main] 0 NCCL_NET_GDR_LEVEL=4
[2023-03-17 11:16:17,483] [INFO] [launch.py:142:main] WORLD INFO DICT: {'gpu108-23-r': [0, 1, 2, 3]}
[2023-03-17 11:16:17,483] [INFO] [launch.py:148:main] nnodes=1, num_local_procs=4, node_rank=0
[2023-03-17 11:16:17,483] [INFO] [launch.py:161:main] global_rank_mapping=defaultdict(<class 'list'>, {'gpu108-23-r': [0, 1, 2, 3]})
[2023-03-17 11:16:17,483] [INFO] [launch.py:162:main] dist_world_size=4
[2023-03-17 11:16:17,483] [INFO] [launch.py:164:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ...............[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found. [93m[NO][0m
 ....... [93m[NO][0m
async_io cpu_adagrad...............  [93m[NO][0m............  .......[93m[NO][0m  [93m[NO][0m.......
 [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m .......cpu_adagrad  [92m[OKAY][0m............
 [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. cpu_adam[93m[NO][0m  ...................... [93m[NO][0m  [92m[OKAY][0m.......
 [92m[OKAY][0m
fused_lamb fused_adam.............  ............. [93m[NO][0m[93m[NO][0m  ..............  [92m[OKAY][0m[92m[OKAY][0m

quantizer ..............fused_lamb  [93m[NO][0m.............  [93m[NO][0m.......  .......[92m[OKAY][0m [92m[OKAY][0m

random_ltdquantizer  ...........................  [93m[NO][0m[93m[NO][0m  ..............  [92m[OKAY][0m[92m[OKAY][0m

random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
using world size: 4, data-parallel-size: 4, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. False
  bias_gelu_fusion ................................ False
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  data_parallel_size .............................. 4
  data_path ....................................... ['/ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 40
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 5000
  expert_interval ................................. 2
  ffn_hidden_size ................................. 4096
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 16
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 1024
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 32
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... 0
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 15.0
  loss_scale_window ............................... 1000
  lr .............................................. 6e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ None
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... False
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 32
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 1
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. ds_z_off-none_stage_3_nl1_hs1024_mb4_seq1024_gb16_nodes1
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 50
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 4
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 3
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 1
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
[2023-03-17 11:16:20,965] [INFO] [comm.py:652:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
> setting tensorboard ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
Helloworld from 3 3
> setting random seeds to 1234 ...
[2023-03-17 11:16:21,270] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> compiling dataset index builder ...
Helloworld from 1 1
Helloworld from 2 2
make: Entering directory `/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/megatron/data'
make: Nothing to be done for `default'.
make: Leaving directory `/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/megatron/data'
>>> done with dataset index builder. Compilation time: 0.034 seconds
WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (4) as the number of workers...
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
gpu108-23-r:45128:45128 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-23-r:45128:45128 [0] NCCL INFO Bootstrap : Using ib0:10.109.136.134<0>
gpu108-23-r:45128:45128 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu108-23-r:45128:45128 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu108-23-r:45128:45128 [0] NCCL INFO cudaDriverVersion 11080
NCCL version 2.17.1+cuda11.7
gpu108-23-r:45129:45129 [1] NCCL INFO cudaDriverVersion 11080
gpu108-23-r:45129:45129 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-23-r:45129:45129 [1] NCCL INFO Bootstrap : Using ib0:10.109.136.134<0>
gpu108-23-r:45129:45129 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu108-23-r:45129:45129 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu108-23-r:45130:45130 [2] NCCL INFO cudaDriverVersion 11080
gpu108-23-r:45130:45130 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-23-r:45131:45131 [3] NCCL INFO cudaDriverVersion 11080
gpu108-23-r:45130:45130 [2] NCCL INFO Bootstrap : Using ib0:10.109.136.134<0>
gpu108-23-r:45131:45131 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-23-r:45130:45130 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu108-23-r:45130:45130 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu108-23-r:45131:45131 [3] NCCL INFO Bootstrap : Using ib0:10.109.136.134<0>
gpu108-23-r:45131:45131 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu108-23-r:45131:45131 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu108-23-r:45128:45378 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-23-r:45130:45380 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-23-r:45128:45378 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.136.134<0>
gpu108-23-r:45128:45378 [0] NCCL INFO Using network IB
gpu108-23-r:45129:45381 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-23-r:45131:45383 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-23-r:45130:45380 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.136.134<0>
gpu108-23-r:45130:45380 [2] NCCL INFO Using network IB
gpu108-23-r:45129:45381 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.136.134<0>
gpu108-23-r:45129:45381 [1] NCCL INFO Using network IB
gpu108-23-r:45131:45383 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.136.134<0>
gpu108-23-r:45131:45383 [3] NCCL INFO Using network IB
gpu108-23-r:45131:45383 [3] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu108-23-r:45131:45383 [3] NCCL INFO Setting affinity for GPU 3 to ffff
gpu108-23-r:45130:45380 [2] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu108-23-r:45130:45380 [2] NCCL INFO Setting affinity for GPU 2 to ffff
gpu108-23-r:45129:45381 [1] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu108-23-r:45128:45378 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:45128:45378 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 00/24 :    0   1   2   3
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 01/24 :    0   1   3   2
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 02/24 :    0   2   3   1
gpu108-23-r:45131:45383 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 0/-1/-1->3->1 [3] 0/-1/-1->3->1 [4] 1/-1/-1->3->0 [5] 1/-1/-1->3->0 [6] 2/-1/-1->3->-1 [7] 2/-1/-1->3->-1 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] 0/-1/-1->3->1 [11] 0/-1/-1->3->1 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] 0/-1/-1->3->1 [15] 0/-1/-1->3->1 [16] 1/-1/-1->3->0 [17] 1/-1/-1->3->0 [18] 2/-1/-1->3->-1 [19] 2/-1/-1->3->-1 [20] -1/-1/-1->3->2 [21] -1/-1/-1->3->2 [22] 0/-1/-1->3->1 [23] 0/-1/-1->3->1
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 03/24 :    0   2   1   3
gpu108-23-r:45130:45380 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] -1/-1/-1->2->0 [3] -1/-1/-1->2->0 [4] 0/-1/-1->2->-1 [5] 0/-1/-1->2->-1 [6] 1/-1/-1->2->3 [7] 1/-1/-1->2->3 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] -1/-1/-1->2->0 [11] -1/-1/-1->2->0 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] -1/-1/-1->2->0 [15] -1/-1/-1->2->0 [16] 0/-1/-1->2->-1 [17] 0/-1/-1->2->-1 [18] 1/-1/-1->2->3 [19] 1/-1/-1->2->3 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] -1/-1/-1->2->0 [23] -1/-1/-1->2->0
gpu108-23-r:45131:45383 [3] NCCL INFO P2P Chunksize set to 524288
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 04/24 :    0   3   1   2
gpu108-23-r:45129:45381 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 3/-1/-1->1->-1 [3] 3/-1/-1->1->-1 [4] -1/-1/-1->1->3 [5] -1/-1/-1->1->3 [6] 0/-1/-1->1->2 [7] 0/-1/-1->1->2 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 3/-1/-1->1->-1 [11] 3/-1/-1->1->-1 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 3/-1/-1->1->-1 [15] 3/-1/-1->1->-1 [16] -1/-1/-1->1->3 [17] -1/-1/-1->1->3 [18] 0/-1/-1->1->2 [19] 0/-1/-1->1->2 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 3/-1/-1->1->-1 [23] 3/-1/-1->1->-1
gpu108-23-r:45130:45380 [2] NCCL INFO P2P Chunksize set to 524288
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 05/24 :    0   3   2   1
gpu108-23-r:45129:45381 [1] NCCL INFO P2P Chunksize set to 524288
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 06/24 :    0   1   2   3
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 07/24 :    0   1   3   2
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 08/24 :    0   2   3   1
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 09/24 :    0   2   1   3
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 10/24 :    0   3   1   2
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 11/24 :    0   3   2   1
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 12/24 :    0   1   2   3
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 13/24 :    0   1   3   2
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 14/24 :    0   2   3   1
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 15/24 :    0   2   1   3
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 16/24 :    0   3   1   2
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 17/24 :    0   3   2   1
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 18/24 :    0   1   2   3
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 19/24 :    0   1   3   2
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 20/24 :    0   2   3   1
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 21/24 :    0   2   1   3
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 22/24 :    0   3   1   2
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 23/24 :    0   3   2   1
gpu108-23-r:45128:45378 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 2/-1/-1->0->3 [3] 2/-1/-1->0->3 [4] 3/-1/-1->0->2 [5] 3/-1/-1->0->2 [6] -1/-1/-1->0->1 [7] -1/-1/-1->0->1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 2/-1/-1->0->3 [11] 2/-1/-1->0->3 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 2/-1/-1->0->3 [15] 2/-1/-1->0->3 [16] 3/-1/-1->0->2 [17] 3/-1/-1->0->2 [18] -1/-1/-1->0->1 [19] -1/-1/-1->0->1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 2/-1/-1->0->3 [23] 2/-1/-1->0->3
gpu108-23-r:45128:45378 [0] NCCL INFO P2P Chunksize set to 524288
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 00/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 00/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 00/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 00/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 01/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 02/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 04/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 06/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 06/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 06/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 06/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 07/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 08/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 10/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 09/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 12/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 12/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 12/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 12/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 13/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 14/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 16/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 15/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 18/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 18/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 18/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 18/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 19/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 20/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 22/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 21/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 02/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 01/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 01/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 02/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 03/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 03/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 04/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 04/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 08/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 07/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 07/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 08/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 09/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 09/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 10/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 10/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 14/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 13/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 14/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 13/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 15/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 15/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 16/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 16/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 20/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 19/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 20/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 19/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 21/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 21/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 22/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 22/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 04/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 01/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 02/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 03/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 05/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 05/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 05/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 05/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 08/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 09/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 07/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 10/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 11/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 11/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 11/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 11/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 14/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 15/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 13/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 16/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 17/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 17/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 17/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 17/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 20/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 22/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 19/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 21/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 23/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 23/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 23/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 23/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Connected all rings
gpu108-23-r:45128:45378 [0] NCCL INFO Connected all rings
gpu108-23-r:45129:45381 [1] NCCL INFO Connected all rings
gpu108-23-r:45130:45380 [2] NCCL INFO Connected all rings
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 01/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 07/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 02/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 08/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 04/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 09/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 05/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 13/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 01/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 19/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 10/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 07/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 20/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 11/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 09/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 08/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 14/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 21/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 13/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 09/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 16/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 19/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 20/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 17/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 21/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 21/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 22/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 23/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 02/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 04/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 05/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 10/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 04/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 02/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 11/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 05/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 03/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 14/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 10/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 05/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 16/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 11/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 11/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 17/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 05/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 16/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 14/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 22/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 11/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 17/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 15/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 23/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 15/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 22/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 17/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 17/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 23/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 23/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 23/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 00/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 06/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 08/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 09/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 00/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 12/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 01/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 18/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 02/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 06/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 00/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 20/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 03/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 07/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 01/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Channel 21/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 14/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 08/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 06/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Channel 15/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 12/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 07/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 13/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 09/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 18/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 12/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 19/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 13/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45380 [2] NCCL INFO Channel 20/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 18/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45131:45383 [3] NCCL INFO Connected all trees
gpu108-23-r:45131:45383 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
gpu108-23-r:45131:45383 [3] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 19/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45381 [1] NCCL INFO Channel 21/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45128:45378 [0] NCCL INFO Connected all trees
gpu108-23-r:45128:45378 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
gpu108-23-r:45128:45378 [0] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
gpu108-23-r:45130:45380 [2] NCCL INFO Connected all trees
gpu108-23-r:45130:45380 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
gpu108-23-r:45130:45380 [2] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
gpu108-23-r:45129:45381 [1] NCCL INFO Connected all trees
gpu108-23-r:45129:45381 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
gpu108-23-r:45129:45381 [1] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
gpu108-23-r:45129:45381 [1] NCCL INFO comm 0x39f6ebb0 rank 1 nranks 4 cudaDev 1 busId 46000 commId 0xcf8d0aee19e2b490 - Init COMPLETE
gpu108-23-r:45131:45383 [3] NCCL INFO comm 0x385b9e50 rank 3 nranks 4 cudaDev 3 busId c7000 commId 0xcf8d0aee19e2b490 - Init COMPLETE
gpu108-23-r:45128:45378 [0] NCCL INFO comm 0x3a855b90 rank 0 nranks 4 cudaDev 0 busId 7000 commId 0xcf8d0aee19e2b490 - Init COMPLETE
gpu108-23-r:45130:45380 [2] NCCL INFO comm 0x39ed5dd0 rank 2 nranks 4 cudaDev 2 busId 85000 commId 0xcf8d0aee19e2b490 - Init COMPLETE
>>> done with compiling and loading fused kernels. Compilation time: 6.424 seconds
time to initialize megatron (seconds): -36.268
[after megatron is initialized] datetime: 2023-03-17 11:16:27 
building GPT model ...
[2023-03-17 11:16:27,771] [INFO] [utils.py:829:see_memory_usage] Before Building Model
[2023-03-17 11:16:27,771] [INFO] [utils.py:830:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2023-03-17 11:16:27,772] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 33.06 GB, percent = 6.6%
gpu108-23-r:45128:45425 [0] NCCL INFO Using network IB
gpu108-23-r:45130:45427 [2] NCCL INFO Using network IB
gpu108-23-r:45131:45428 [3] NCCL INFO Using network IB
gpu108-23-r:45129:45426 [1] NCCL INFO Using network IB
gpu108-23-r:45130:45427 [2] NCCL INFO Setting affinity for GPU 2 to ffff
gpu108-23-r:45128:45425 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:45131:45428 [3] NCCL INFO Setting affinity for GPU 3 to ffff
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 00/24 :    0   1   2   3
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 01/24 :    0   1   3   2
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 02/24 :    0   2   3   1
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 03/24 :    0   2   1   3
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 04/24 :    0   3   1   2
gpu108-23-r:45131:45428 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 0/-1/-1->3->1 [3] 0/-1/-1->3->1 [4] 1/-1/-1->3->0 [5] 1/-1/-1->3->0 [6] 2/-1/-1->3->-1 [7] 2/-1/-1->3->-1 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] 0/-1/-1->3->1 [11] 0/-1/-1->3->1 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] 0/-1/-1->3->1 [15] 0/-1/-1->3->1 [16] 1/-1/-1->3->0 [17] 1/-1/-1->3->0 [18] 2/-1/-1->3->-1 [19] 2/-1/-1->3->-1 [20] -1/-1/-1->3->2 [21] -1/-1/-1->3->2 [22] 0/-1/-1->3->1 [23] 0/-1/-1->3->1
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 05/24 :    0   3   2   1
gpu108-23-r:45130:45427 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] -1/-1/-1->2->0 [3] -1/-1/-1->2->0 [4] 0/-1/-1->2->-1 [5] 0/-1/-1->2->-1 [6] 1/-1/-1->2->3 [7] 1/-1/-1->2->3 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] -1/-1/-1->2->0 [11] -1/-1/-1->2->0 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] -1/-1/-1->2->0 [15] -1/-1/-1->2->0 [16] 0/-1/-1->2->-1 [17] 0/-1/-1->2->-1 [18] 1/-1/-1->2->3 [19] 1/-1/-1->2->3 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] -1/-1/-1->2->0 [23] -1/-1/-1->2->0
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 06/24 :    0   1   2   3
gpu108-23-r:45131:45428 [3] NCCL INFO P2P Chunksize set to 524288
gpu108-23-r:45129:45426 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 3/-1/-1->1->-1 [3] 3/-1/-1->1->-1 [4] -1/-1/-1->1->3 [5] -1/-1/-1->1->3 [6] 0/-1/-1->1->2 [7] 0/-1/-1->1->2 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 3/-1/-1->1->-1 [11] 3/-1/-1->1->-1 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 3/-1/-1->1->-1 [15] 3/-1/-1->1->-1 [16] -1/-1/-1->1->3 [17] -1/-1/-1->1->3 [18] 0/-1/-1->1->2 [19] 0/-1/-1->1->2 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 3/-1/-1->1->-1 [23] 3/-1/-1->1->-1
gpu108-23-r:45130:45427 [2] NCCL INFO P2P Chunksize set to 524288
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 07/24 :    0   1   3   2
gpu108-23-r:45129:45426 [1] NCCL INFO P2P Chunksize set to 524288
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 08/24 :    0   2   3   1
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 09/24 :    0   2   1   3
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 10/24 :    0   3   1   2
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 11/24 :    0   3   2   1
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 12/24 :    0   1   2   3
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 13/24 :    0   1   3   2
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 14/24 :    0   2   3   1
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 15/24 :    0   2   1   3
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 16/24 :    0   3   1   2
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 17/24 :    0   3   2   1
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 18/24 :    0   1   2   3
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 19/24 :    0   1   3   2
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 20/24 :    0   2   3   1
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 21/24 :    0   2   1   3
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 22/24 :    0   3   1   2
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 23/24 :    0   3   2   1
gpu108-23-r:45128:45425 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 2/-1/-1->0->3 [3] 2/-1/-1->0->3 [4] 3/-1/-1->0->2 [5] 3/-1/-1->0->2 [6] -1/-1/-1->0->1 [7] -1/-1/-1->0->1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 2/-1/-1->0->3 [11] 2/-1/-1->0->3 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 2/-1/-1->0->3 [15] 2/-1/-1->0->3 [16] 3/-1/-1->0->2 [17] 3/-1/-1->0->2 [18] -1/-1/-1->0->1 [19] -1/-1/-1->0->1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 2/-1/-1->0->3 [23] 2/-1/-1->0->3
gpu108-23-r:45128:45425 [0] NCCL INFO P2P Chunksize set to 524288
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 00/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 00/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 00/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 00/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 01/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 02/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 04/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 06/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 06/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 06/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 06/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 09/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 08/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 07/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 10/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 12/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 12/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 12/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 12/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 14/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 15/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 13/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 16/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 18/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 18/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 18/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 18/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 21/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 20/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 19/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 22/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 02/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 01/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 01/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 02/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 04/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 04/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 03/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 03/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 08/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 07/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 07/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 08/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 10/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 10/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 09/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 09/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 14/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 14/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 13/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 13/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 16/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 15/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 16/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 15/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 20/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 19/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 19/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 20/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 22/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 22/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 21/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 21/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 04/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 01/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 02/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 03/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 05/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 05/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 05/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 05/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 07/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 10/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 08/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 09/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 11/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 11/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 11/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 11/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 16/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 13/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 14/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 15/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 17/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 17/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 17/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 17/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 22/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 19/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 20/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 21/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 23/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 23/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 23/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 23/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Connected all rings
gpu108-23-r:45128:45425 [0] NCCL INFO Connected all rings
gpu108-23-r:45130:45427 [2] NCCL INFO Connected all rings
gpu108-23-r:45129:45426 [1] NCCL INFO Connected all rings
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 01/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 07/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 08/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 02/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 09/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 04/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 13/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 01/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 05/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 19/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 07/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 10/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 20/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 09/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 08/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 11/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 21/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 09/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 13/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 14/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 20/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 19/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 16/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 21/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 21/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 17/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 22/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 23/0 : 3[c7000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 02/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 04/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 05/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 10/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 11/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 04/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 02/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 14/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 03/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 05/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 16/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 10/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 05/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 17/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 11/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 05/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 11/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 22/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 16/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 11/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 14/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 23/0 : 1[46000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 17/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 15/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 15/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 17/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 22/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 17/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 23/0 : 2[85000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 23/0 : 0[7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 23/0 : 3[c7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 00/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 06/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 08/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 09/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 12/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 00/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 01/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 18/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 06/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 20/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 00/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 02/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 07/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Channel 21/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 01/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 03/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 08/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 06/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 14/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 12/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 07/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Channel 15/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 13/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 09/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 18/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 12/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 19/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 13/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45130:45427 [2] NCCL INFO Channel 20/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 18/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 19/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45131:45428 [3] NCCL INFO Connected all trees
gpu108-23-r:45131:45428 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
gpu108-23-r:45131:45428 [3] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
gpu108-23-r:45129:45426 [1] NCCL INFO Channel 21/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:45128:45425 [0] NCCL INFO Connected all trees
gpu108-23-r:45128:45425 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
gpu108-23-r:45128:45425 [0] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
gpu108-23-r:45130:45427 [2] NCCL INFO Connected all trees
gpu108-23-r:45130:45427 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
gpu108-23-r:45130:45427 [2] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
gpu108-23-r:45129:45426 [1] NCCL INFO Connected all trees
gpu108-23-r:45129:45426 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
gpu108-23-r:45129:45426 [1] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 8 p2p channels per peer
gpu108-23-r:45130:45427 [2] NCCL INFO comm 0x351ed150 rank 2 nranks 4 cudaDev 2 busId 85000 commId 0xea63a05ee5e28edf - Init COMPLETE
gpu108-23-r:45128:45425 [0] NCCL INFO comm 0x35ed14f0 rank 0 nranks 4 cudaDev 0 busId 7000 commId 0xea63a05ee5e28edf - Init COMPLETE
gpu108-23-r:45129:45426 [1] NCCL INFO comm 0x35397330 rank 1 nranks 4 cudaDev 1 busId 46000 commId 0xea63a05ee5e28edf - Init COMPLETE
gpu108-23-r:45131:45428 [3] NCCL INFO comm 0x3471a770 rank 3 nranks 4 cudaDev 3 busId c7000 commId 0xea63a05ee5e28edf - Init COMPLETE
[2023-03-17 11:16:32,361] [INFO] [partition_parameters.py:415:__exit__] finished initializing model with 0.07B parameters
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[2023-03-17 11:16:32,399] [INFO] [utils.py:829:see_memory_usage] After Building Model
[2023-03-17 11:16:32,399] [INFO] [utils.py:830:see_memory_usage] MA 0.03 GB         Max_MA 0.12 GB         CA 0.12 GB         Max_CA 0 GB 
[2023-03-17 11:16:32,400] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 33.25 GB, percent = 6.6%
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 65158144
> learning rate decay style: cosine
DeepSpeed is enabled.
[2023-03-17 11:16:32,400] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed info: version=0.8.3+bbfd0a6, git-hash=bbfd0a6, git-branch=master
[2023-03-17 11:16:32,402] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-03-17 11:16:32,402] [INFO] [logging.py:93:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-03-17 11:16:32,403] [INFO] [logging.py:93:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-03-17 11:16:32,403] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2023-03-17 11:16:32,403] [INFO] [utils.py:55:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'apex.optimizers.fused_adam.FusedAdam'>
[2023-03-17 11:16:32,403] [INFO] [logging.py:93:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer
[2023-03-17 11:16:32,437] [INFO] [utils.py:829:see_memory_usage] Stage 3 initialize beginning
[2023-03-17 11:16:32,437] [INFO] [utils.py:830:see_memory_usage] MA 0.03 GB         Max_MA 0.03 GB         CA 0.12 GB         Max_CA 0 GB 
[2023-03-17 11:16:32,438] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 33.25 GB, percent = 6.6%
[2023-03-17 11:16:32,438] [INFO] [stage3.py:113:__init__] Reduce bucket size 90000000
[2023-03-17 11:16:32,438] [INFO] [stage3.py:114:__init__] Prefetch bucket size 50000000
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Emitting ninja build file /home/shaima0d/.cache/torch_extensions/py39_cu117/utils/build.ninja...
Building extension module utils...
Using envvar MAX_JOBS (4) as the number of workers...
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.1564478874206543 seconds
Loading extension module utils...
Time to load utils op: 0.10287332534790039 seconds
[2023-03-17 11:16:32,571] [INFO] [utils.py:829:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2023-03-17 11:16:32,572] [INFO] [utils.py:830:see_memory_usage] MA 0.03 GB         Max_MA 0.03 GB         CA 0.12 GB         Max_CA 0 GB 
[2023-03-17 11:16:32,572] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 33.25 GB, percent = 6.6%
Parameter Offload: Total persistent parameters: 15360 in 10 params
Loading extension module utils...
Loading extension module utils...
Time to load utils op: 0.2075204849243164 secondsTime to load utils op: 0.20751523971557617 seconds

[2023-03-17 11:16:32,598] [INFO] [utils.py:829:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2023-03-17 11:16:32,598] [INFO] [utils.py:830:see_memory_usage] MA 0.03 GB         Max_MA 0.03 GB         CA 0.12 GB         Max_CA 0 GB 
[2023-03-17 11:16:32,599] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 33.25 GB, percent = 6.6%
[2023-03-17 11:16:32,622] [INFO] [utils.py:829:see_memory_usage] Before creating fp16 partitions
[2023-03-17 11:16:32,623] [INFO] [utils.py:830:see_memory_usage] MA 0.03 GB         Max_MA 0.03 GB         CA 0.12 GB         Max_CA 0 GB 
[2023-03-17 11:16:32,623] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 33.25 GB, percent = 6.6%
[2023-03-17 11:16:32,732] [INFO] [utils.py:829:see_memory_usage] After creating fp16 partitions: 2
[2023-03-17 11:16:32,732] [INFO] [utils.py:830:see_memory_usage] MA 0.03 GB         Max_MA 0.03 GB         CA 0.03 GB         Max_CA 0 GB 
[2023-03-17 11:16:32,733] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 33.25 GB, percent = 6.6%
[2023-03-17 11:16:32,757] [INFO] [utils.py:829:see_memory_usage] Before creating fp32 partitions
[2023-03-17 11:16:32,757] [INFO] [utils.py:830:see_memory_usage] MA 0.03 GB         Max_MA 0.03 GB         CA 0.03 GB         Max_CA 0 GB 
[2023-03-17 11:16:32,757] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 33.25 GB, percent = 6.6%
[2023-03-17 11:16:32,782] [INFO] [utils.py:829:see_memory_usage] After creating fp32 partitions
[2023-03-17 11:16:32,782] [INFO] [utils.py:830:see_memory_usage] MA 0.09 GB         Max_MA 0.12 GB         CA 0.13 GB         Max_CA 0 GB 
[2023-03-17 11:16:32,783] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 33.25 GB, percent = 6.6%
[2023-03-17 11:16:32,807] [INFO] [utils.py:829:see_memory_usage] Before initializing optimizer states
[2023-03-17 11:16:32,807] [INFO] [utils.py:830:see_memory_usage] MA 0.09 GB         Max_MA 0.09 GB         CA 0.13 GB         Max_CA 0 GB 
[2023-03-17 11:16:32,808] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 33.25 GB, percent = 6.6%
[2023-03-17 11:16:32,809] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | init_optimizer_state: 0.88
[2023-03-17 11:16:32,832] [INFO] [utils.py:829:see_memory_usage] After initializing optimizer states
[2023-03-17 11:16:32,833] [INFO] [utils.py:830:see_memory_usage] MA 0.21 GB         Max_MA 0.27 GB         CA 0.31 GB         Max_CA 0 GB 
[2023-03-17 11:16:32,833] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 33.25 GB, percent = 6.6%
[2023-03-17 11:16:32,833] [INFO] [stage3.py:376:_setup_for_real_optimizer] optimizer state initialized
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...Time to load utils op: 0.001260519027709961 seconds

Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...Time to load utils op: 0.0017330646514892578 seconds

Loading extension module utils...
Time to load utils op: 0.0022101402282714844 seconds
[2023-03-17 11:16:32,868] [INFO] [utils.py:829:see_memory_usage] After initializing ZeRO optimizer
[2023-03-17 11:16:32,868] [INFO] [utils.py:830:see_memory_usage] MA 0.41 GB         Max_MA 0.6 GB         CA 0.78 GB         Max_CA 1 GB 
[2023-03-17 11:16:32,869] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 33.25 GB, percent = 6.6%
[2023-03-17 11:16:32,869] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[2023-03-17 11:16:32,869] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-03-17 11:16:32,869] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x2af7addee430>
[2023-03-17 11:16:32,869] [INFO] [logging.py:93:log_dist] [Rank 0] step=0, skipped=0, lr=[5.9999999999999995e-05, 5.9999999999999995e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:32,869] [INFO] [config.py:1018:print] DeepSpeedEngine configuration:
[2023-03-17 11:16:32,869] [INFO] [config.py:1022:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-03-17 11:16:32,869] [INFO] [config.py:1022:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 16, 'thread_count': 2, 'single_submit': False, 'overlap_events': True}
[2023-03-17 11:16:32,869] [INFO] [config.py:1022:print]   amp_enabled .................. False
[2023-03-17 11:16:32,869] [INFO] [config.py:1022:print]   amp_params ................... False
[2023-03-17 11:16:32,869] [INFO] [config.py:1022:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-03-17 11:16:32,869] [INFO] [config.py:1022:print]   bfloat16_enabled ............. False
[2023-03-17 11:16:32,869] [INFO] [config.py:1022:print]   checkpoint_parallel_write_pipeline  False
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   checkpoint_tag_validation_enabled  True
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   checkpoint_tag_validation_fail  False
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x2af7addee9a0>
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   communication_data_type ...... None
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   curriculum_enabled_legacy .... False
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   curriculum_params_legacy ..... False
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   data_efficiency_enabled ...... False
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   dataloader_drop_last ......... False
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   disable_allgather ............ False
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   dump_state ................... False
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   dynamic_loss_scale_args ...... {'init_scale': 32768, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   eigenvalue_enabled ........... False
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   eigenvalue_gas_boundary_resolution  1
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   eigenvalue_layer_num ......... 0
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   eigenvalue_max_iter .......... 100
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   eigenvalue_stability ......... 1e-06
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   eigenvalue_tol ............... 0.01
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   eigenvalue_verbose ........... False
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   elasticity_enabled ........... False
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   fp16_auto_cast ............... False
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   fp16_enabled ................. True
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   fp16_master_weights_and_gradients  False
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   global_rank .................. 0
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   grad_accum_dtype ............. None
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   gradient_accumulation_steps .. 1
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   gradient_clipping ............ 1
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   gradient_predivide_factor .... 1.0
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   initial_dynamic_scale ........ 32768
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   load_universal_checkpoint .... False
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   loss_scale ................... 0
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   memory_breakdown ............. False
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   optimizer_legacy_fusion ...... False
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   optimizer_name ............... None
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   optimizer_params ............. None
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   pld_enabled .................. False
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   pld_params ................... False
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   prescale_gradients ........... False
[2023-03-17 11:16:32,870] [INFO] [config.py:1022:print]   scheduler_name ............... None
[2023-03-17 11:16:32,871] [INFO] [config.py:1022:print]   scheduler_params ............. None
[2023-03-17 11:16:32,871] [INFO] [config.py:1022:print]   sparse_attention ............. None
[2023-03-17 11:16:32,871] [INFO] [config.py:1022:print]   sparse_gradients_enabled ..... False
[2023-03-17 11:16:32,871] [INFO] [config.py:1022:print]   steps_per_print .............. 1
[2023-03-17 11:16:32,871] [INFO] [config.py:1022:print]   train_batch_size ............. 16
[2023-03-17 11:16:32,871] [INFO] [config.py:1022:print]   train_micro_batch_size_per_gpu  4
[2023-03-17 11:16:32,871] [INFO] [config.py:1022:print]   use_node_local_storage ....... False
[2023-03-17 11:16:32,871] [INFO] [config.py:1022:print]   wall_clock_breakdown ......... True
[2023-03-17 11:16:32,871] [INFO] [config.py:1022:print]   world_size ................... 4
[2023-03-17 11:16:32,871] [INFO] [config.py:1022:print]   zero_allow_untested_optimizer  False
[2023-03-17 11:16:32,871] [INFO] [config.py:1022:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=90000000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=sys.maxsize max_live_parameters=3000000000 max_reuse_distance=3000000000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2023-03-17 11:16:32,871] [INFO] [config.py:1022:print]   zero_enabled ................. True
[2023-03-17 11:16:32,871] [INFO] [config.py:1022:print]   zero_force_ds_cpu_optimizer .. True
[2023-03-17 11:16:32,871] [INFO] [config.py:1022:print]   zero_optimization_stage ...... 3
[2023-03-17 11:16:32,871] [INFO] [config.py:1007:print_user_config]   json = {
    "train_batch_size": 16, 
    "train_micro_batch_size_per_gpu": 4, 
    "steps_per_print": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 3, 
        "stage3_max_live_parameters": 3.000000e+09, 
        "stage3_max_reuse_distance": 3.000000e+09, 
        "stage3_param_persistence_threshold": 1.000000e+05, 
        "stage3_prefetch_bucket_size": 5.000000e+07, 
        "contiguous_gradients": true, 
        "overlap_comm": true, 
        "reduce_bucket_size": 9.000000e+07, 
        "sub_group_size": 1.000000e+09, 
        "offload_optimizer": {
            "device": "none", 
            "buffer_count": 4, 
            "pipeline_read": false, 
            "pipeline_write": false, 
            "pin_memory": true
        }
    }, 
    "gradient_clipping": 1, 
    "fp16": {
        "enabled": true, 
        "initial_scale_power": 15, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "wall_clock_breakdown": true, 
    "zero_allow_untested_optimizer": false, 
    "aio": {
        "block_size": 1.048576e+06, 
        "queue_depth": 16, 
        "single_submit": false, 
        "overlap_events": true, 
        "thread_count": 2
    }
}
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0008325576782226562 seconds
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-03-17 11:16:32 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      800
    validation: 640
    test:       640
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.000502 seconds
    number of documents: 17868
 > dataset split:
    train:
     document indices in [0, 17511) total of 17511 documents
    validation:
     document indices in [17511, 17868) total of 357 documents
    test:
     document indices in [17868, 17868) total of 0 documents
NCCL version 2.17.1+cuda11.7NCCL version 2.17.1+cuda11.7NCCL version 2.17.1+cuda11.7


gpu108-23-r:45128:45449 [0] NCCL INFO Using network IB
gpu108-23-r:45130:45450 [2] NCCL INFO Using network IB
gpu108-23-r:45129:45451 [1] NCCL INFO Using network IB
gpu108-23-r:45131:45452 [3] NCCL INFO Using network IB
gpu108-23-r:45128:45449 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 00/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 01/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 02/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 03/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 04/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 05/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 06/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 07/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 08/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 09/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 10/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 11/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 12/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 13/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 14/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 15/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 16/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 17/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 18/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 19/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 20/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 21/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 22/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 23/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 24/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 25/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 26/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 27/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 28/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 29/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 30/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Channel 31/32 :    0
gpu108-23-r:45128:45449 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-r:45128:45449 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:45128:45449 [0] NCCL INFO Connected all rings
gpu108-23-r:45128:45449 [0] NCCL INFO Connected all trees
gpu108-23-r:45128:45449 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:45130:45450 [2] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:45131:45452 [3] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:45130:45450 [2] NCCL INFO Setting affinity for GPU 2 to ffff
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 00/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 01/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 02/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 03/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 04/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 05/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 06/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 07/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 08/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 09/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 10/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 11/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 12/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 13/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 14/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 15/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 16/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 17/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 18/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 19/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 20/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 21/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 22/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 23/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 24/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 25/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 26/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 27/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 28/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 29/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 30/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Channel 31/32 :    0
gpu108-23-r:45130:45450 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-r:45130:45450 [2] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:45129:45451 [1] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:45130:45450 [2] NCCL INFO Connected all rings
gpu108-23-r:45130:45450 [2] NCCL INFO Connected all trees
gpu108-23-r:45130:45450 [2] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:45131:45452 [3] NCCL INFO Setting affinity for GPU 3 to ffff
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 00/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 01/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 02/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 03/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 04/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 05/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 06/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 07/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 08/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 09/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 10/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 11/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 12/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 13/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 14/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 15/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 16/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 17/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 18/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 19/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 20/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 21/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 22/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 23/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 24/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 25/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 26/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 27/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 28/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 29/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 30/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Channel 31/32 :    0
gpu108-23-r:45131:45452 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-r:45131:45452 [3] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 00/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 01/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 02/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 03/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 04/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 05/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 06/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 07/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 08/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 09/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 10/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 11/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 12/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 13/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 14/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 15/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 16/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 17/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 18/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 19/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 20/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 21/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 22/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 23/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 24/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 25/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 26/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 27/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 28/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 29/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 30/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Channel 31/32 :    0
gpu108-23-r:45129:45451 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-r:45129:45451 [1] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:45131:45452 [3] NCCL INFO Connected all rings
gpu108-23-r:45131:45452 [3] NCCL INFO Connected all trees
gpu108-23-r:45131:45452 [3] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:45129:45451 [1] NCCL INFO Connected all rings
gpu108-23-r:45129:45451 [1] NCCL INFO Connected all trees
gpu108-23-r:45129:45451 [1] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:45128:45449 [0] NCCL INFO comm 0x35efa8b0 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0xc60ac7b76c082b99 - Init COMPLETE
 > loading doc-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_train_indexmap_800ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_train_indexmap_800ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_train_indexmap_800ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.002 seconds
    total number of samples: 1544006
    total number of epochs: 1
gpu108-23-r:45130:45450 [2] NCCL INFO comm 0x35215c60 rank 0 nranks 1 cudaDev 2 busId 85000 commId 0x54fde0ba885ffb21 - Init COMPLETE
gpu108-23-r:45129:45451 [1] NCCL INFO comm 0x353bb660 rank 0 nranks 1 cudaDev 1 busId 46000 commId 0xafcde1e6bd081a72 - Init COMPLETE
gpu108-23-r:45131:45452 [3] NCCL INFO comm 0x34747ad0 rank 0 nranks 1 cudaDev 3 busId c7000 commId 0x87469cea079b2edd - Init COMPLETE
 > loading doc-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_valid_indexmap_640ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_valid_indexmap_640ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_valid_indexmap_640ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.001 seconds
    total number of samples: 31426
    total number of epochs: 1
> finished creating GPT datasets ...
gpu108-23-r:45128:45469 [0] NCCL INFO Using network IB
gpu108-23-r:45131:45467 [3] NCCL INFO Using network IB
gpu108-23-r:45130:45466 [2] NCCL INFO Using network IB
gpu108-23-r:45129:45468 [1] NCCL INFO Using network IB
gpu108-23-r:45131:45467 [3] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:45131:45467 [3] NCCL INFO Setting affinity for GPU 3 to ffff
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 00/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 01/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 02/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 03/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 04/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 05/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 06/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 07/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 08/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 09/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 10/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 11/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 12/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 13/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 14/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 15/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 16/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 17/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 18/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 19/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 20/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 21/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 22/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 23/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 24/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 25/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 26/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 27/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 28/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 29/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 30/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Channel 31/32 :    0
gpu108-23-r:45131:45467 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-r:45131:45467 [3] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:45131:45467 [3] NCCL INFO Connected all rings
gpu108-23-r:45131:45467 [3] NCCL INFO Connected all trees
gpu108-23-r:45131:45467 [3] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:45128:45469 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:45130:45466 [2] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:45130:45466 [2] NCCL INFO Setting affinity for GPU 2 to ffff
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 00/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 01/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 02/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 03/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 04/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 05/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 06/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 07/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 08/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 09/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 10/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 11/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 12/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 13/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 14/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 15/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 16/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 17/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 18/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 19/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 20/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 21/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 22/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 23/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 24/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 25/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 26/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 27/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 28/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 29/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 30/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Channel 31/32 :    0
gpu108-23-r:45130:45466 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-r:45130:45466 [2] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:45129:45468 [1] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:45130:45466 [2] NCCL INFO Connected all rings
gpu108-23-r:45130:45466 [2] NCCL INFO Connected all trees
gpu108-23-r:45130:45466 [2] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 00/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 01/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 02/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 03/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 04/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 05/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 06/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 07/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 08/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 09/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 10/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 11/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 12/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 13/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 14/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 15/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 16/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 17/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 18/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 19/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 20/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 21/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 22/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 23/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 24/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 25/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 26/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 27/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 28/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 29/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 30/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Channel 31/32 :    0
gpu108-23-r:45128:45469 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-r:45128:45469 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:45128:45469 [0] NCCL INFO Connected all rings
gpu108-23-r:45128:45469 [0] NCCL INFO Connected all trees
gpu108-23-r:45128:45469 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 00/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 01/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 02/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 03/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 04/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 05/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 06/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 07/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 08/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 09/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 10/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 11/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 12/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 13/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 14/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 15/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 16/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 17/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 18/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 19/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 20/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 21/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 22/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 23/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 24/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 25/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 26/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 27/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 28/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 29/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 30/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Channel 31/32 :    0
gpu108-23-r:45129:45468 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-r:45129:45468 [1] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:45129:45468 [1] NCCL INFO Connected all rings
gpu108-23-r:45129:45468 [1] NCCL INFO Connected all trees
gpu108-23-r:45129:45468 [1] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:45131:45467 [3] NCCL INFO comm 0x3474ac40 rank 0 nranks 1 cudaDev 3 busId c7000 commId 0xce71d01c388c93f0 - Init COMPLETE
gpu108-23-r:45130:45466 [2] NCCL INFO comm 0x35218dd0 rank 0 nranks 1 cudaDev 2 busId 85000 commId 0x1497a1734f516f7d - Init COMPLETE
gpu108-23-r:45128:45469 [0] NCCL INFO comm 0x35ee9700 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0xfcce30073494d348 - Init COMPLETE
gpu108-23-r:45129:45468 [1] NCCL INFO comm 0x353be7d0 rank 0 nranks 1 cudaDev 1 busId 46000 commId 0x3331ae0cbecb3dba - Init COMPLETE
[after dataloaders are built] datetime: 2023-03-17 11:16:33 
done with setup ...
training ...
time (ms) | model-and-optimizer-setup: 5113.03 | train/valid/test-data-iterators-setup: 1022.67
[before the start of training step] datetime: 2023-03-17 11:16:33 
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
[2023-03-17 11:16:33,914] [INFO] [checkpointing.py:553:forward] Activation Checkpointing Information
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
[2023-03-17 11:16:33,914] [INFO] [checkpointing.py:554:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-03-17 11:16:33,914] [INFO] [checkpointing.py:557:forward] ----contiguous Memory Checkpointing False with 1 total layers
[2023-03-17 11:16:33,914] [INFO] [checkpointing.py:560:forward] ----Synchronization False
[2023-03-17 11:16:33,914] [INFO] [checkpointing.py:561:forward] ----Profiling time in checkpointing False
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
gpu108-23-r:45130:46333 [2] NCCL INFO Using network IB
gpu108-23-r:45131:46335 [3] NCCL INFO Using network IB
gpu108-23-r:45129:46338 [1] NCCL INFO Using network IB
gpu108-23-r:45128:46339 [0] NCCL INFO Using network IB
gpu108-23-r:45130:46333 [2] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:45130:46333 [2] NCCL INFO Setting affinity for GPU 2 to ffff
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 00/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 01/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 02/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 03/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 04/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 05/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 06/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 07/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 08/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 09/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 10/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 11/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 12/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 13/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 14/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 15/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 16/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 17/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 18/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 19/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 20/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 21/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 22/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 23/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 24/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 25/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 26/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 27/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 28/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 29/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 30/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Channel 31/32 :    0
gpu108-23-r:45130:46333 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-r:45130:46333 [2] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:45130:46333 [2] NCCL INFO Connected all rings
gpu108-23-r:45130:46333 [2] NCCL INFO Connected all trees
gpu108-23-r:45130:46333 [2] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:45129:46338 [1] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:45128:46339 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 00/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 01/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 02/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 03/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 04/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 05/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 06/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 07/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 08/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 09/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 10/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 11/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 12/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 13/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 14/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 15/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 16/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 17/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 18/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 19/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 20/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 21/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 22/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 23/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 24/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 25/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 26/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 27/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 28/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 29/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 30/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Channel 31/32 :    0
gpu108-23-r:45129:46338 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-r:45129:46338 [1] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:45129:46338 [1] NCCL INFO Connected all rings
gpu108-23-r:45129:46338 [1] NCCL INFO Connected all trees
gpu108-23-r:45129:46338 [1] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:45131:46335 [3] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 00/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 01/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 02/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 03/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 04/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 05/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 06/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 07/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 08/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 09/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 10/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 11/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 12/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 13/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 14/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 15/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 16/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 17/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 18/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 19/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 20/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 21/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 22/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 23/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 24/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 25/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 26/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 27/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 28/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 29/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 30/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Channel 31/32 :    0
gpu108-23-r:45128:46339 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-r:45128:46339 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:45128:46339 [0] NCCL INFO Connected all rings
gpu108-23-r:45128:46339 [0] NCCL INFO Connected all trees
gpu108-23-r:45128:46339 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:45131:46335 [3] NCCL INFO Setting affinity for GPU 3 to ffff
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 00/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 01/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 02/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 03/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 04/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 05/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 06/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 07/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 08/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 09/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 10/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 11/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 12/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 13/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 14/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 15/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 16/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 17/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 18/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 19/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 20/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 21/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 22/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 23/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 24/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 25/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 26/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 27/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 28/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 29/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 30/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Channel 31/32 :    0
gpu108-23-r:45131:46335 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-r:45131:46335 [3] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:45131:46335 [3] NCCL INFO Connected all rings
gpu108-23-r:45131:46335 [3] NCCL INFO Connected all trees
gpu108-23-r:45131:46335 [3] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:45130:46333 [2] NCCL INFO comm 0x6ef22ed0 rank 0 nranks 1 cudaDev 2 busId 85000 commId 0xe0692632be7ed247 - Init COMPLETE
gpu108-23-r:45129:46338 [1] NCCL INFO comm 0x6ee31a20 rank 0 nranks 1 cudaDev 1 busId 46000 commId 0xae6641582e016a90 - Init COMPLETE
gpu108-23-r:45131:46335 [3] NCCL INFO comm 0x6bfe3740 rank 0 nranks 1 cudaDev 3 busId c7000 commId 0x71fccac41755f6 - Init COMPLETE
gpu108-23-r:45128:46339 [0] NCCL INFO comm 0x6fbc9f80 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0x7c0a9db2d9cf5afd - Init COMPLETE
[2023-03-17 11:16:34,935] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.95
[2023-03-17 11:16:34,935] [INFO] [logging.py:93:log_dist] [Rank 0] step=1, skipped=0, lr=[5.9946721667563326e-05, 5.9946721667563326e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:34,935] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 823.92 | backward_microstep: 35.09 | backward_inner_microstep: 29.73 | backward_allreduce_microstep: 5.27 | step_microstep: 167.96
[2023-03-17 11:16:34,936] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 823.91 | backward: 35.09 | backward_inner: 29.74 | backward_allreduce: 5.27 | step: 167.96
[Rank 0] (after 1 iterations) memory (MB) | allocated: 422.45068359375 | max allocated: 2744.00537109375 | reserved: 3926.0 | max reserved: 3926.0
 iteration        1/      50 | consumed samples:           16 | consumed tokens:        16384 | elapsed time per iteration (ms): 1030.6 | learning rate: 5.995E-05 | global batch size:    16 | lm loss: 1.084395E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 15.526 | TFLOPs: 1.70 |
time (ms) | forward-compute: 825.69 | backward-compute: 34.99 | backward-embedding-all-reduce: 0.01 | optimizer: 168.34 | batch-generator: 3.67
[2023-03-17 11:16:34,991] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.75
[2023-03-17 11:16:34,991] [INFO] [logging.py:93:log_dist] [Rank 0] step=2, skipped=0, lr=[5.97870969354909e-05, 5.97870969354909e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:34,991] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 20.36 | backward_microstep: 27.96 | backward_inner_microstep: 25.03 | backward_allreduce_microstep: 2.86 | step_microstep: 3.79
[2023-03-17 11:16:34,992] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 20.35 | backward: 27.96 | backward_inner: 25.03 | backward_allreduce: 2.86 | step: 3.79
 iteration        2/      50 | consumed samples:           32 | consumed tokens:        32768 | elapsed time per iteration (ms): 55.6 | learning rate: 5.979E-05 | global batch size:    16 | lm loss: 1.084388E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 287.533 | TFLOPs: 31.39 |
time (ms) | forward-compute: 21.88 | backward-compute: 27.94 | backward-embedding-all-reduce: 0.01 | optimizer: 3.98 | batch-generator: 1.00
[2023-03-17 11:16:35,041] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.74
[2023-03-17 11:16:35,041] [INFO] [logging.py:93:log_dist] [Rank 0] step=3, skipped=0, lr=[5.95217557696746e-05, 5.95217557696746e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:35,042] [INFO] [timer.py:198:stop] epoch=0/micro_step=3/global_step=3, RunningAvgSamplesPerSec=328.0339819825104, CurrSamplesPerSec=328.0339819825104, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:35,042] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 17.31 | backward_microstep: 26.67 | backward_inner_microstep: 24.17 | backward_allreduce_microstep: 2.45 | step_microstep: 3.97
[2023-03-17 11:16:35,042] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 17.31 | backward: 26.67 | backward_inner: 24.16 | backward_allreduce: 2.45 | step: 3.98
 iteration        3/      50 | consumed samples:           48 | consumed tokens:        49152 | elapsed time per iteration (ms): 50.3 | learning rate: 5.952E-05 | global batch size:    16 | lm loss: 1.083992E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 318.346 | TFLOPs: 34.76 |
time (ms) | forward-compute: 18.58 | backward-compute: 26.63 | backward-embedding-all-reduce: 0.01 | optimizer: 3.94 | batch-generator: 0.71
[2023-03-17 11:16:35,091] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.74
[2023-03-17 11:16:35,091] [INFO] [logging.py:93:log_dist] [Rank 0] step=4, skipped=0, lr=[5.9151745350473036e-05, 5.9151745350473036e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:35,092] [INFO] [timer.py:198:stop] epoch=0/micro_step=4/global_step=4, RunningAvgSamplesPerSec=329.9208196294166, CurrSamplesPerSec=331.8294888720771, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:35,092] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 17.18 | backward_microstep: 26.51 | backward_inner_microstep: 24.10 | backward_allreduce_microstep: 2.35 | step_microstep: 4.04
[2023-03-17 11:16:35,092] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 17.17 | backward: 26.51 | backward_inner: 24.10 | backward_allreduce: 2.36 | step: 4.05
 iteration        4/      50 | consumed samples:           64 | consumed tokens:        65536 | elapsed time per iteration (ms): 49.8 | learning rate: 5.915E-05 | global batch size:    16 | lm loss: 1.083743E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 321.295 | TFLOPs: 35.08 |
time (ms) | forward-compute: 18.31 | backward-compute: 26.52 | backward-embedding-all-reduce: 0.01 | optimizer: 3.91 | batch-generator: 0.81
[2023-03-17 11:16:35,141] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.74
[2023-03-17 11:16:35,141] [INFO] [logging.py:93:log_dist] [Rank 0] step=5, skipped=0, lr=[5.8678525939969144e-05, 5.8678525939969144e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:35,141] [INFO] [timer.py:198:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=331.34289212781925, CurrSamplesPerSec=334.22413466806114, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:35,141] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 17.23 | backward_microstep: 26.43 | backward_inner_microstep: 24.06 | backward_allreduce_microstep: 2.30 | step_microstep: 4.08
[2023-03-17 11:16:35,142] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 17.22 | backward: 26.42 | backward_inner: 24.06 | backward_allreduce: 2.31 | step: 4.08
 iteration        5/      50 | consumed samples:           80 | consumed tokens:        81920 | elapsed time per iteration (ms): 49.6 | learning rate: 5.868E-05 | global batch size:    16 | lm loss: 1.083348E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 322.654 | TFLOPs: 35.23 |
time (ms) | forward-compute: 18.16 | backward-compute: 26.68 | backward-embedding-all-reduce: 0.01 | optimizer: 3.67 | batch-generator: 0.74
[2023-03-17 11:16:35,190] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.74
[2023-03-17 11:16:35,190] [INFO] [logging.py:93:log_dist] [Rank 0] step=6, skipped=0, lr=[5.810396511898279e-05, 5.810396511898279e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:35,191] [INFO] [timer.py:198:stop] epoch=0/micro_step=6/global_step=6, RunningAvgSamplesPerSec=332.28789859378094, CurrSamplesPerSec=335.15553957409406, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:35,191] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 17.16 | backward_microstep: 26.65 | backward_inner_microstep: 24.11 | backward_allreduce_microstep: 2.48 | step_microstep: 4.03
[2023-03-17 11:16:35,191] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 17.15 | backward: 26.65 | backward_inner: 24.10 | backward_allreduce: 2.49 | step: 4.03
 iteration        6/      50 | consumed samples:           96 | consumed tokens:        98304 | elapsed time per iteration (ms): 49.4 | learning rate: 5.810E-05 | global batch size:    16 | lm loss: 1.083093E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 323.853 | TFLOPs: 35.36 |
time (ms) | forward-compute: 18.00 | backward-compute: 26.27 | backward-embedding-all-reduce: 0.01 | optimizer: 4.12 | batch-generator: 0.73
[2023-03-17 11:16:35,239] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:35,239] [INFO] [logging.py:93:log_dist] [Rank 0] step=7, skipped=0, lr=[5.743033041658253e-05, 5.743033041658253e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:35,239] [INFO] [timer.py:198:stop] epoch=0/micro_step=7/global_step=7, RunningAvgSamplesPerSec=333.93143118736504, CurrSamplesPerSec=340.6714249454287, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:35,240] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 17.12 | backward_microstep: 26.11 | backward_inner_microstep: 23.74 | backward_allreduce_microstep: 2.30 | step_microstep: 3.82
[2023-03-17 11:16:35,240] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 17.11 | backward: 26.10 | backward_inner: 23.73 | backward_allreduce: 2.30 | step: 3.83
 iteration        7/      50 | consumed samples:          112 | consumed tokens:       114688 | elapsed time per iteration (ms): 48.8 | learning rate: 5.743E-05 | global batch size:    16 | lm loss: 1.082856E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 327.896 | TFLOPs: 35.80 |
time (ms) | forward-compute: 18.02 | backward-compute: 25.79 | backward-embedding-all-reduce: 0.01 | optimizer: 4.00 | batch-generator: 0.56
[2023-03-17 11:16:35,286] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:35,287] [INFO] [logging.py:93:log_dist] [Rank 0] step=8, skipped=0, lr=[5.666028036118432e-05, 5.666028036118432e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:35,287] [INFO] [timer.py:198:stop] epoch=0/micro_step=8/global_step=8, RunningAvgSamplesPerSec=336.50729169974744, CurrSamplesPerSec=350.0065923978804, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:35,287] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.20 | backward_microstep: 25.08 | backward_inner_microstep: 22.52 | backward_allreduce_microstep: 2.49 | step_microstep: 3.94
[2023-03-17 11:16:35,287] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.19 | backward: 25.08 | backward_inner: 22.51 | backward_allreduce: 2.49 | step: 3.94
 iteration        8/      50 | consumed samples:          128 | consumed tokens:       131072 | elapsed time per iteration (ms): 47.4 | learning rate: 5.666E-05 | global batch size:    16 | lm loss: 1.082281E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 337.641 | TFLOPs: 36.87 |
time (ms) | forward-compute: 17.43 | backward-compute: 25.12 | backward-embedding-all-reduce: 0.01 | optimizer: 3.80 | batch-generator: 0.71
[2023-03-17 11:16:35,333] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.71
[2023-03-17 11:16:35,334] [INFO] [logging.py:93:log_dist] [Rank 0] step=9, skipped=0, lr=[5.579685398855441e-05, 5.579685398855441e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:35,334] [INFO] [timer.py:198:stop] epoch=0/micro_step=9/global_step=9, RunningAvgSamplesPerSec=338.51551907023935, CurrSamplesPerSec=351.0869387797809, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:35,334] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.20 | backward_microstep: 24.83 | backward_inner_microstep: 22.45 | backward_allreduce_microstep: 2.31 | step_microstep: 4.20
[2023-03-17 11:16:35,334] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.19 | backward: 24.83 | backward_inner: 22.45 | backward_allreduce: 2.32 | step: 4.21
 iteration        9/      50 | consumed samples:          144 | consumed tokens:       147456 | elapsed time per iteration (ms): 47.2 | learning rate: 5.580E-05 | global batch size:    16 | lm loss: 1.082143E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 339.060 | TFLOPs: 37.02 |
time (ms) | forward-compute: 17.22 | backward-compute: 25.07 | backward-embedding-all-reduce: 0.01 | optimizer: 3.85 | batch-generator: 0.73
[2023-03-17 11:16:35,381] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:35,381] [INFO] [logging.py:93:log_dist] [Rank 0] step=10, skipped=0, lr=[5.4843458848123576e-05, 5.4843458848123576e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:35,381] [INFO] [timer.py:198:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=340.19542938613864, CurrSamplesPerSec=352.4384574582618, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:35,381] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.22 | backward_microstep: 24.84 | backward_inner_microstep: 22.47 | backward_allreduce_microstep: 2.31 | step_microstep: 4.13
[2023-03-17 11:16:35,381] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.21 | backward: 24.83 | backward_inner: 22.46 | backward_allreduce: 2.32 | step: 4.13
 iteration       10/      50 | consumed samples:          160 | consumed tokens:       163840 | elapsed time per iteration (ms): 47.0 | learning rate: 5.484E-05 | global batch size:    16 | lm loss: 1.081808E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 340.293 | TFLOPs: 37.16 |
time (ms) | forward-compute: 17.17 | backward-compute: 25.08 | backward-embedding-all-reduce: 0.01 | optimizer: 3.74 | batch-generator: 0.68
[2023-03-17 11:16:35,428] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.69
[2023-03-17 11:16:35,428] [INFO] [logging.py:93:log_dist] [Rank 0] step=11, skipped=0, lr=[5.380385755494631e-05, 5.380385755494631e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:35,428] [INFO] [timer.py:198:stop] epoch=0/micro_step=11/global_step=11, RunningAvgSamplesPerSec=341.32310987411876, CurrSamplesPerSec=350.62102403343783, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:35,429] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.15 | backward_microstep: 24.93 | backward_inner_microstep: 22.44 | backward_allreduce_microstep: 2.43 | step_microstep: 4.00
[2023-03-17 11:16:35,429] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.14 | backward: 24.93 | backward_inner: 22.43 | backward_allreduce: 2.43 | step: 4.01
 iteration       11/      50 | consumed samples:          176 | consumed tokens:       180224 | elapsed time per iteration (ms): 47.4 | learning rate: 5.380E-05 | global batch size:    16 | lm loss: 1.081438E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 337.894 | TFLOPs: 36.89 |
time (ms) | forward-compute: 17.51 | backward-compute: 24.98 | backward-embedding-all-reduce: 0.01 | optimizer: 3.82 | batch-generator: 0.71
[2023-03-17 11:16:35,475] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:35,475] [INFO] [logging.py:93:log_dist] [Rank 0] step=12, skipped=0, lr=[5.2682152940378117e-05, 5.2682152940378117e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:35,476] [INFO] [timer.py:198:stop] epoch=0/micro_step=12/global_step=12, RunningAvgSamplesPerSec=342.30396760018465, CurrSamplesPerSec=351.3921038852236, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:35,476] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.15 | backward_microstep: 24.75 | backward_inner_microstep: 22.39 | backward_allreduce_microstep: 2.31 | step_microstep: 4.15
[2023-03-17 11:16:35,476] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.15 | backward: 24.75 | backward_inner: 22.38 | backward_allreduce: 2.31 | step: 4.15
 iteration       12/      50 | consumed samples:          192 | consumed tokens:       196608 | elapsed time per iteration (ms): 47.2 | learning rate: 5.268E-05 | global batch size:    16 | lm loss: 1.081024E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 339.144 | TFLOPs: 37.03 |
time (ms) | forward-compute: 17.36 | backward-compute: 24.98 | backward-embedding-all-reduce: 0.01 | optimizer: 3.80 | batch-generator: 0.68
[2023-03-17 11:16:35,522] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:35,522] [INFO] [logging.py:93:log_dist] [Rank 0] step=13, skipped=0, lr=[5.14827718600746e-05, 5.14827718600746e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:35,522] [INFO] [timer.py:198:stop] epoch=0/micro_step=13/global_step=13, RunningAvgSamplesPerSec=343.33042991328807, CurrSamplesPerSec=353.9441042599537, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:35,523] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.20 | backward_microstep: 24.76 | backward_inner_microstep: 22.40 | backward_allreduce_microstep: 2.30 | step_microstep: 4.03
[2023-03-17 11:16:35,523] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.19 | backward: 24.75 | backward_inner: 22.39 | backward_allreduce: 2.31 | step: 4.04
 iteration       13/      50 | consumed samples:          208 | consumed tokens:       212992 | elapsed time per iteration (ms): 46.9 | learning rate: 5.148E-05 | global batch size:    16 | lm loss: 1.080414E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 341.141 | TFLOPs: 37.25 |
time (ms) | forward-compute: 17.19 | backward-compute: 24.98 | backward-embedding-all-reduce: 0.01 | optimizer: 3.71 | batch-generator: 0.68
[2023-03-17 11:16:35,569] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:35,569] [INFO] [logging.py:93:log_dist] [Rank 0] step=14, skipped=0, lr=[5.021044772321462e-05, 5.021044772321462e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:35,569] [INFO] [timer.py:198:stop] epoch=0/micro_step=14/global_step=14, RunningAvgSamplesPerSec=344.17802577660615, CurrSamplesPerSec=353.7855004006579, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:35,570] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.17 | backward_microstep: 24.76 | backward_inner_microstep: 22.40 | backward_allreduce_microstep: 2.31 | step_microstep: 4.13
[2023-03-17 11:16:35,570] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.16 | backward: 24.76 | backward_inner: 22.39 | backward_allreduce: 2.32 | step: 4.13
 iteration       14/      50 | consumed samples:          224 | consumed tokens:       229376 | elapsed time per iteration (ms): 46.8 | learning rate: 5.021E-05 | global batch size:    16 | lm loss: 1.079366E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 341.714 | TFLOPs: 37.31 |
time (ms) | forward-compute: 17.03 | backward-compute: 25.01 | backward-embedding-all-reduce: 0.01 | optimizer: 3.75 | batch-generator: 0.68
[2023-03-17 11:16:35,616] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.69
[2023-03-17 11:16:35,616] [INFO] [logging.py:93:log_dist] [Rank 0] step=15, skipped=0, lr=[4.887020181189677e-05, 4.887020181189677e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:35,617] [INFO] [timer.py:198:stop] epoch=0/micro_step=15/global_step=15, RunningAvgSamplesPerSec=344.62955024337754, CurrSamplesPerSec=350.14172866817626, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:35,617] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.19 | backward_microstep: 24.91 | backward_inner_microstep: 22.39 | backward_allreduce_microstep: 2.44 | step_microstep: 4.02
[2023-03-17 11:16:35,617] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.18 | backward: 24.90 | backward_inner: 22.39 | backward_allreduce: 2.44 | step: 4.03
 iteration       15/      50 | consumed samples:          240 | consumed tokens:       245760 | elapsed time per iteration (ms): 47.5 | learning rate: 4.887E-05 | global batch size:    16 | lm loss: 1.078437E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 336.946 | TFLOPs: 36.79 |
time (ms) | forward-compute: 17.56 | backward-compute: 25.04 | backward-embedding-all-reduce: 0.01 | optimizer: 3.83 | batch-generator: 0.78
[2023-03-17 11:16:35,664] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:35,664] [INFO] [logging.py:93:log_dist] [Rank 0] step=16, skipped=0, lr=[4.74673234644329e-05, 4.74673234644329e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:35,664] [INFO] [timer.py:198:stop] epoch=0/micro_step=16/global_step=16, RunningAvgSamplesPerSec=345.0067460951342, CurrSamplesPerSec=349.9865134786985, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:35,664] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.18 | backward_microstep: 24.91 | backward_inner_microstep: 22.40 | backward_allreduce_microstep: 2.45 | step_microstep: 3.87
[2023-03-17 11:16:35,664] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.17 | backward: 24.90 | backward_inner: 22.39 | backward_allreduce: 2.45 | step: 3.87
 iteration       16/      50 | consumed samples:          256 | consumed tokens:       262144 | elapsed time per iteration (ms): 47.3 | learning rate: 4.747E-05 | global batch size:    16 | lm loss: 1.078245E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 337.916 | TFLOPs: 36.90 |
time (ms) | forward-compute: 17.55 | backward-compute: 25.08 | backward-embedding-all-reduce: 0.01 | optimizer: 3.64 | batch-generator: 0.69
[2023-03-17 11:16:35,711] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.69
[2023-03-17 11:16:35,711] [INFO] [logging.py:93:log_dist] [Rank 0] step=17, skipped=0, lr=[4.6007349200746303e-05, 4.6007349200746303e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:35,711] [INFO] [timer.py:198:stop] epoch=0/micro_step=17/global_step=17, RunningAvgSamplesPerSec=345.54330666496406, CurrSamplesPerSec=353.2342934141823, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:35,711] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.13 | backward_microstep: 24.85 | backward_inner_microstep: 22.40 | backward_allreduce_microstep: 2.38 | step_microstep: 4.05
[2023-03-17 11:16:35,711] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.12 | backward: 24.84 | backward_inner: 22.39 | backward_allreduce: 2.39 | step: 4.06
 iteration       17/      50 | consumed samples:          272 | consumed tokens:       278528 | elapsed time per iteration (ms): 46.9 | learning rate: 4.601E-05 | global batch size:    16 | lm loss: 1.077498E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 340.981 | TFLOPs: 37.23 |
time (ms) | forward-compute: 17.12 | backward-compute: 24.97 | backward-embedding-all-reduce: 0.01 | optimizer: 3.81 | batch-generator: 0.65
[2023-03-17 11:16:35,758] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:35,758] [INFO] [logging.py:93:log_dist] [Rank 0] step=18, skipped=0, lr=[4.4496040872256956e-05, 4.4496040872256956e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:35,759] [INFO] [timer.py:198:stop] epoch=0/micro_step=18/global_step=18, RunningAvgSamplesPerSec=345.85055856699694, CurrSamplesPerSec=350.52579236571046, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:35,759] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.19 | backward_microstep: 24.90 | backward_inner_microstep: 22.32 | backward_allreduce_microstep: 2.50 | step_microstep: 4.21
[2023-03-17 11:16:35,759] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.18 | backward: 24.89 | backward_inner: 22.32 | backward_allreduce: 2.50 | step: 4.21
 iteration       18/      50 | consumed samples:          288 | consumed tokens:       294912 | elapsed time per iteration (ms): 47.4 | learning rate: 4.450E-05 | global batch size:    16 | lm loss: 1.076699E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 337.726 | TFLOPs: 36.87 |
time (ms) | forward-compute: 17.61 | backward-compute: 25.12 | backward-embedding-all-reduce: 0.01 | optimizer: 3.66 | batch-generator: 0.67
[2023-03-17 11:16:35,806] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.69
[2023-03-17 11:16:35,806] [INFO] [logging.py:93:log_dist] [Rank 0] step=19, skipped=0, lr=[4.293936292248631e-05, 4.293936292248631e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:35,807] [INFO] [timer.py:198:stop] epoch=0/micro_step=19/global_step=19, RunningAvgSamplesPerSec=345.8012949392131, CurrSamplesPerSec=345.01498123489796, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:35,807] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.11 | backward_microstep: 25.30 | backward_inner_microstep: 22.32 | backward_allreduce_microstep: 2.92 | step_microstep: 4.20
[2023-03-17 11:16:35,807] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.10 | backward: 25.30 | backward_inner: 22.31 | backward_allreduce: 2.92 | step: 4.21
 iteration       19/      50 | consumed samples:          304 | consumed tokens:       311296 | elapsed time per iteration (ms): 48.3 | learning rate: 4.294E-05 | global batch size:    16 | lm loss: 1.075270E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 331.027 | TFLOPs: 36.14 |
time (ms) | forward-compute: 17.77 | backward-compute: 25.41 | backward-embedding-all-reduce: 0.01 | optimizer: 4.04 | batch-generator: 1.25
[2023-03-17 11:16:35,854] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:35,854] [INFO] [logging.py:93:log_dist] [Rank 0] step=20, skipped=0, lr=[4.134345884812357e-05, 4.134345884812357e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:35,854] [INFO] [timer.py:198:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=345.98231822216417, CurrSamplesPerSec=349.0889721181856, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:35,854] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.18 | backward_microstep: 24.80 | backward_inner_microstep: 22.38 | backward_allreduce_microstep: 2.36 | step_microstep: 3.96
[2023-03-17 11:16:35,855] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.17 | backward: 24.80 | backward_inner: 22.37 | backward_allreduce: 2.36 | step: 3.96
 iteration       20/      50 | consumed samples:          320 | consumed tokens:       327680 | elapsed time per iteration (ms): 47.5 | learning rate: 4.134E-05 | global batch size:    16 | lm loss: 1.074038E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 336.976 | TFLOPs: 36.79 |
time (ms) | forward-compute: 17.70 | backward-compute: 24.99 | backward-embedding-all-reduce: 0.01 | optimizer: 3.74 | batch-generator: 0.88
[2023-03-17 11:16:35,901] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:35,902] [INFO] [logging.py:93:log_dist] [Rank 0] step=21, skipped=0, lr=[3.971462695345109e-05, 3.971462695345109e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:35,902] [INFO] [timer.py:198:stop] epoch=0/micro_step=21/global_step=21, RunningAvgSamplesPerSec=346.1123129838357, CurrSamplesPerSec=348.4690365662419, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:35,902] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.11 | backward_microstep: 24.85 | backward_inner_microstep: 22.32 | backward_allreduce_microstep: 2.46 | step_microstep: 3.91
[2023-03-17 11:16:35,902] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.10 | backward: 24.85 | backward_inner: 22.32 | backward_allreduce: 2.46 | step: 3.92
 iteration       21/      50 | consumed samples:          336 | consumed tokens:       344064 | elapsed time per iteration (ms): 47.6 | learning rate: 3.971E-05 | global batch size:    16 | lm loss: 1.072615E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 336.171 | TFLOPs: 36.71 |
time (ms) | forward-compute: 17.70 | backward-compute: 24.95 | backward-embedding-all-reduce: 0.01 | optimizer: 3.80 | batch-generator: 1.01
[2023-03-17 11:16:35,949] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:35,949] [INFO] [logging.py:93:log_dist] [Rank 0] step=22, skipped=0, lr=[3.805929549381457e-05, 3.805929549381457e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:35,949] [INFO] [timer.py:198:stop] epoch=0/micro_step=22/global_step=22, RunningAvgSamplesPerSec=346.2227827321108, CurrSamplesPerSec=348.33518810730004, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:35,950] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.15 | backward_microstep: 24.81 | backward_inner_microstep: 22.34 | backward_allreduce_microstep: 2.41 | step_microstep: 4.03
[2023-03-17 11:16:35,950] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.14 | backward: 24.81 | backward_inner: 22.33 | backward_allreduce: 2.41 | step: 4.04
 iteration       22/      50 | consumed samples:          352 | consumed tokens:       360448 | elapsed time per iteration (ms): 47.4 | learning rate: 3.806E-05 | global batch size:    16 | lm loss: 1.072828E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 337.509 | TFLOPs: 36.85 |
time (ms) | forward-compute: 17.61 | backward-compute: 24.93 | backward-embedding-all-reduce: 0.01 | optimizer: 3.84 | batch-generator: 0.79
[2023-03-17 11:16:35,996] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:35,997] [INFO] [logging.py:93:log_dist] [Rank 0] step=23, skipped=0, lr=[3.638399730623622e-05, 3.638399730623622e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:35,997] [INFO] [timer.py:198:stop] epoch=0/micro_step=23/global_step=23, RunningAvgSamplesPerSec=346.3349629971134, CurrSamplesPerSec=348.5939339161511, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:35,997] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.16 | backward_microstep: 24.83 | backward_inner_microstep: 22.37 | backward_allreduce_microstep: 2.40 | step_microstep: 4.22
[2023-03-17 11:16:35,997] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.15 | backward: 24.82 | backward_inner: 22.36 | backward_allreduce: 2.40 | step: 4.22
 iteration       23/      50 | consumed samples:          368 | consumed tokens:       376832 | elapsed time per iteration (ms): 47.5 | learning rate: 3.638E-05 | global batch size:    16 | lm loss: 1.070263E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 336.865 | TFLOPs: 36.78 |
time (ms) | forward-compute: 17.49 | backward-compute: 24.93 | backward-embedding-all-reduce: 0.01 | optimizer: 4.04 | batch-generator: 0.89
[2023-03-17 11:16:36,044] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:36,044] [INFO] [logging.py:93:log_dist] [Rank 0] step=24, skipped=0, lr=[3.469534402729146e-05, 3.469534402729146e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:36,044] [INFO] [timer.py:198:stop] epoch=0/micro_step=24/global_step=24, RunningAvgSamplesPerSec=346.51360183143055, CurrSamplesPerSec=350.30805288900723, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:36,044] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.18 | backward_microstep: 24.73 | backward_inner_microstep: 22.34 | backward_allreduce_microstep: 2.32 | step_microstep: 4.28
[2023-03-17 11:16:36,044] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.17 | backward: 24.73 | backward_inner: 22.33 | backward_allreduce: 2.32 | step: 4.28
 iteration       24/      50 | consumed samples:          384 | consumed tokens:       393216 | elapsed time per iteration (ms): 47.2 | learning rate: 3.470E-05 | global batch size:    16 | lm loss: 1.068232E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 338.701 | TFLOPs: 36.98 |
time (ms) | forward-compute: 17.30 | backward-compute: 25.11 | backward-embedding-all-reduce: 0.01 | optimizer: 3.81 | batch-generator: 0.66
[2023-03-17 11:16:36,091] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.69
[2023-03-17 11:16:36,091] [INFO] [logging.py:93:log_dist] [Rank 0] step=25, skipped=0, lr=[3.3e-05, 3.3e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:36,092] [INFO] [timer.py:198:stop] epoch=0/micro_step=25/global_step=25, RunningAvgSamplesPerSec=346.61902386848317, CurrSamplesPerSec=348.95464708757555, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:36,092] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.07 | backward_microstep: 24.80 | backward_inner_microstep: 22.33 | backward_allreduce_microstep: 2.41 | step_microstep: 4.11
[2023-03-17 11:16:36,092] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.06 | backward: 24.80 | backward_inner: 22.33 | backward_allreduce: 2.41 | step: 4.12
 iteration       25/      50 | consumed samples:          400 | consumed tokens:       409600 | elapsed time per iteration (ms): 47.5 | learning rate: 3.300E-05 | global batch size:    16 | lm loss: 1.067073E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 336.819 | TFLOPs: 36.78 |
time (ms) | forward-compute: 17.65 | backward-compute: 25.01 | backward-embedding-all-reduce: 0.01 | optimizer: 3.82 | batch-generator: 1.39
[2023-03-17 11:16:36,138] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.69
[2023-03-17 11:16:36,138] [INFO] [logging.py:93:log_dist] [Rank 0] step=26, skipped=0, lr=[3.1304655972708536e-05, 3.1304655972708536e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:36,139] [INFO] [timer.py:198:stop] epoch=0/micro_step=26/global_step=26, RunningAvgSamplesPerSec=346.81441570329497, CurrSamplesPerSec=351.37002596967415, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:36,139] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.10 | backward_microstep: 25.03 | backward_inner_microstep: 22.64 | backward_allreduce_microstep: 2.33 | step_microstep: 3.86
[2023-03-17 11:16:36,139] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.09 | backward: 25.03 | backward_inner: 22.63 | backward_allreduce: 2.33 | step: 3.86
 iteration       26/      50 | consumed samples:          416 | consumed tokens:       425984 | elapsed time per iteration (ms): 47.2 | learning rate: 3.130E-05 | global batch size:    16 | lm loss: 1.065839E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 339.014 | TFLOPs: 37.02 |
time (ms) | forward-compute: 17.30 | backward-compute: 24.97 | backward-embedding-all-reduce: 0.01 | optimizer: 3.88 | batch-generator: 0.66
[2023-03-17 11:16:36,186] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:36,186] [INFO] [logging.py:93:log_dist] [Rank 0] step=27, skipped=0, lr=[2.961600269376378e-05, 2.961600269376378e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:36,186] [INFO] [timer.py:198:stop] epoch=0/micro_step=27/global_step=27, RunningAvgSamplesPerSec=346.95275096673805, CurrSamplesPerSec=350.30622429165015, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:36,186] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.08 | backward_microstep: 24.78 | backward_inner_microstep: 22.38 | backward_allreduce_microstep: 2.34 | step_microstep: 4.05
[2023-03-17 11:16:36,186] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.07 | backward: 24.78 | backward_inner: 22.38 | backward_allreduce: 2.35 | step: 4.06
 iteration       27/      50 | consumed samples:          432 | consumed tokens:       442368 | elapsed time per iteration (ms): 47.3 | learning rate: 2.962E-05 | global batch size:    16 | lm loss: 1.064182E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 338.467 | TFLOPs: 36.96 |
time (ms) | forward-compute: 17.48 | backward-compute: 24.91 | backward-embedding-all-reduce: 0.01 | optimizer: 3.89 | batch-generator: 0.91
[2023-03-17 11:16:36,232] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:36,233] [INFO] [logging.py:93:log_dist] [Rank 0] step=28, skipped=0, lr=[2.7940704506185428e-05, 2.7940704506185428e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:36,233] [INFO] [timer.py:198:stop] epoch=0/micro_step=28/global_step=28, RunningAvgSamplesPerSec=347.1928400885318, CurrSamplesPerSec=353.30496035715413, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:36,233] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.00 | backward_microstep: 24.84 | backward_inner_microstep: 22.37 | backward_allreduce_microstep: 2.40 | step_microstep: 4.06
[2023-03-17 11:16:36,233] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 15.99 | backward: 24.84 | backward_inner: 22.37 | backward_allreduce: 2.40 | step: 4.06
 iteration       28/      50 | consumed samples:          448 | consumed tokens:       458752 | elapsed time per iteration (ms): 46.8 | learning rate: 2.794E-05 | global batch size:    16 | lm loss: 1.061518E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 341.521 | TFLOPs: 37.29 |
time (ms) | forward-compute: 17.04 | backward-compute: 24.97 | backward-embedding-all-reduce: 0.01 | optimizer: 3.83 | batch-generator: 0.56
[2023-03-17 11:16:36,279] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.69
[2023-03-17 11:16:36,280] [INFO] [logging.py:93:log_dist] [Rank 0] step=29, skipped=0, lr=[2.6285373046548923e-05, 2.6285373046548923e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:36,280] [INFO] [timer.py:198:stop] epoch=0/micro_step=29/global_step=29, RunningAvgSamplesPerSec=347.3989222220241, CurrSamplesPerSec=352.84427479310597, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:36,280] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.02 | backward_microstep: 24.84 | backward_inner_microstep: 22.39 | backward_allreduce_microstep: 2.37 | step_microstep: 4.05
[2023-03-17 11:16:36,280] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.01 | backward: 24.84 | backward_inner: 22.39 | backward_allreduce: 2.38 | step: 4.05
 iteration       29/      50 | consumed samples:          464 | consumed tokens:       475136 | elapsed time per iteration (ms): 46.9 | learning rate: 2.629E-05 | global batch size:    16 | lm loss: 1.062030E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 340.909 | TFLOPs: 37.22 |
time (ms) | forward-compute: 17.11 | backward-compute: 24.97 | backward-embedding-all-reduce: 0.01 | optimizer: 3.83 | batch-generator: 0.57
[2023-03-17 11:16:36,326] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:36,327] [INFO] [logging.py:93:log_dist] [Rank 0] step=30, skipped=0, lr=[2.465654115187642e-05, 2.465654115187642e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:36,327] [INFO] [timer.py:198:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=347.5533430759217, CurrSamplesPerSec=351.77522906925543, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:36,327] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.04 | backward_microstep: 24.83 | backward_inner_microstep: 22.37 | backward_allreduce_microstep: 2.39 | step_microstep: 4.04
[2023-03-17 11:16:36,327] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.03 | backward: 24.83 | backward_inner: 22.37 | backward_allreduce: 2.39 | step: 4.05
 iteration       30/      50 | consumed samples:          480 | consumed tokens:       491520 | elapsed time per iteration (ms): 47.1 | learning rate: 2.466E-05 | global batch size:    16 | lm loss: 1.057788E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 339.579 | TFLOPs: 37.08 |
time (ms) | forward-compute: 17.24 | backward-compute: 24.96 | backward-embedding-all-reduce: 0.01 | optimizer: 3.88 | batch-generator: 0.65
[2023-03-17 11:16:36,374] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:36,374] [INFO] [logging.py:93:log_dist] [Rank 0] step=31, skipped=0, lr=[2.3060637077513695e-05, 2.3060637077513695e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:36,374] [INFO] [timer.py:198:stop] epoch=0/micro_step=31/global_step=31, RunningAvgSamplesPerSec=347.6356265251372, CurrSamplesPerSec=349.9554869527127, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:36,375] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 15.96 | backward_microstep: 24.76 | backward_inner_microstep: 22.36 | backward_allreduce_microstep: 2.34 | step_microstep: 4.12
[2023-03-17 11:16:36,375] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 15.95 | backward: 24.76 | backward_inner: 22.36 | backward_allreduce: 2.34 | step: 4.12
 iteration       31/      50 | consumed samples:          496 | consumed tokens:       507904 | elapsed time per iteration (ms): 47.3 | learning rate: 2.306E-05 | global batch size:    16 | lm loss: 1.056400E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 338.266 | TFLOPs: 36.93 |
time (ms) | forward-compute: 17.43 | backward-compute: 24.92 | backward-embedding-all-reduce: 0.01 | optimizer: 3.90 | batch-generator: 0.71
[2023-03-17 11:16:36,421] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:36,421] [INFO] [logging.py:93:log_dist] [Rank 0] step=32, skipped=0, lr=[2.150395912774304e-05, 2.150395912774304e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:36,421] [INFO] [timer.py:198:stop] epoch=0/micro_step=32/global_step=32, RunningAvgSamplesPerSec=347.7995591665729, CurrSamplesPerSec=352.6217934571287, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:36,422] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.00 | backward_microstep: 24.76 | backward_inner_microstep: 22.34 | backward_allreduce_microstep: 2.36 | step_microstep: 4.10
[2023-03-17 11:16:36,422] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 15.99 | backward: 24.76 | backward_inner: 22.33 | backward_allreduce: 2.36 | step: 4.10
 iteration       32/      50 | consumed samples:          512 | consumed tokens:       524288 | elapsed time per iteration (ms): 47.0 | learning rate: 2.150E-05 | global batch size:    16 | lm loss: 1.054190E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 340.452 | TFLOPs: 37.17 |
time (ms) | forward-compute: 17.17 | backward-compute: 24.92 | backward-embedding-all-reduce: 0.01 | optimizer: 3.86 | batch-generator: 0.76
[2023-03-17 11:16:36,468] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:36,469] [INFO] [logging.py:93:log_dist] [Rank 0] step=33, skipped=0, lr=[1.999265079925368e-05, 1.999265079925368e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:36,469] [INFO] [timer.py:198:stop] epoch=0/micro_step=33/global_step=33, RunningAvgSamplesPerSec=347.8391433462752, CurrSamplesPerSec=349.0308729300158, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:36,469] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 15.94 | backward_microstep: 24.75 | backward_inner_microstep: 22.37 | backward_allreduce_microstep: 2.31 | step_microstep: 4.47
[2023-03-17 11:16:36,469] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 15.93 | backward: 24.74 | backward_inner: 22.37 | backward_allreduce: 2.31 | step: 4.47
 iteration       33/      50 | consumed samples:          528 | consumed tokens:       540672 | elapsed time per iteration (ms): 47.5 | learning rate: 1.999E-05 | global batch size:    16 | lm loss: 1.050462E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 337.117 | TFLOPs: 36.81 |
time (ms) | forward-compute: 17.55 | backward-compute: 24.86 | backward-embedding-all-reduce: 0.01 | optimizer: 4.00 | batch-generator: 0.94
[2023-03-17 11:16:36,516] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:36,516] [INFO] [logging.py:93:log_dist] [Rank 0] step=34, skipped=0, lr=[1.853267653556708e-05, 1.853267653556708e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:36,516] [INFO] [timer.py:198:stop] epoch=0/micro_step=34/global_step=34, RunningAvgSamplesPerSec=347.9918808298199, CurrSamplesPerSec=352.79419201875714, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:36,516] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.02 | backward_microstep: 24.84 | backward_inner_microstep: 22.34 | backward_allreduce_microstep: 2.43 | step_microstep: 4.09
[2023-03-17 11:16:36,516] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.01 | backward: 24.83 | backward_inner: 22.34 | backward_allreduce: 2.43 | step: 4.09
 iteration       34/      50 | consumed samples:          544 | consumed tokens:       557056 | elapsed time per iteration (ms): 47.2 | learning rate: 1.853E-05 | global batch size:    16 | lm loss: 1.049708E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 339.134 | TFLOPs: 37.03 |
time (ms) | forward-compute: 17.31 | backward-compute: 24.88 | backward-embedding-all-reduce: 0.01 | optimizer: 3.95 | batch-generator: 0.67
[2023-03-17 11:16:36,563] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:36,563] [INFO] [logging.py:93:log_dist] [Rank 0] step=35, skipped=0, lr=[1.712979818810323e-05, 1.712979818810323e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:36,563] [INFO] [timer.py:198:stop] epoch=0/micro_step=35/global_step=35, RunningAvgSamplesPerSec=348.08503262306203, CurrSamplesPerSec=351.09244910878243, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:36,563] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.01 | backward_microstep: 24.81 | backward_inner_microstep: 22.32 | backward_allreduce_microstep: 2.43 | step_microstep: 4.10
[2023-03-17 11:16:36,564] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.00 | backward: 24.80 | backward_inner: 22.31 | backward_allreduce: 2.43 | step: 4.11
 iteration       35/      50 | consumed samples:          560 | consumed tokens:       573440 | elapsed time per iteration (ms): 47.2 | learning rate: 1.713E-05 | global batch size:    16 | lm loss: 1.047241E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 339.125 | TFLOPs: 37.03 |
time (ms) | forward-compute: 17.35 | backward-compute: 24.95 | backward-embedding-all-reduce: 0.01 | optimizer: 3.87 | batch-generator: 0.71
[2023-03-17 11:16:36,610] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.69
[2023-03-17 11:16:36,610] [INFO] [logging.py:93:log_dist] [Rank 0] step=36, skipped=0, lr=[1.5789552276785377e-05, 1.5789552276785377e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:36,610] [INFO] [timer.py:198:stop] epoch=0/micro_step=36/global_step=36, RunningAvgSamplesPerSec=348.21196400463816, CurrSamplesPerSec=352.4532654079462, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:36,610] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.00 | backward_microstep: 24.82 | backward_inner_microstep: 22.33 | backward_allreduce_microstep: 2.44 | step_microstep: 3.97
[2023-03-17 11:16:36,611] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 15.99 | backward: 24.82 | backward_inner: 22.32 | backward_allreduce: 2.44 | step: 3.98
 iteration       36/      50 | consumed samples:          576 | consumed tokens:       589824 | elapsed time per iteration (ms): 47.0 | learning rate: 1.579E-05 | global batch size:    16 | lm loss: 1.044767E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 340.210 | TFLOPs: 37.15 |
time (ms) | forward-compute: 17.30 | backward-compute: 24.95 | backward-embedding-all-reduce: 0.01 | optimizer: 3.76 | batch-generator: 0.65
[2023-03-17 11:16:36,657] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:36,657] [INFO] [logging.py:93:log_dist] [Rank 0] step=37, skipped=0, lr=[1.4517228139925405e-05, 1.4517228139925405e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:36,658] [INFO] [timer.py:198:stop] epoch=0/micro_step=37/global_step=37, RunningAvgSamplesPerSec=348.2488355183796, CurrSamplesPerSec=349.50712983698764, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:36,658] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 15.98 | backward_microstep: 24.85 | backward_inner_microstep: 22.37 | backward_allreduce_microstep: 2.42 | step_microstep: 4.15
[2023-03-17 11:16:36,658] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 15.97 | backward: 24.85 | backward_inner: 22.36 | backward_allreduce: 2.42 | step: 4.15
 iteration       37/      50 | consumed samples:          592 | consumed tokens:       606208 | elapsed time per iteration (ms): 47.3 | learning rate: 1.452E-05 | global batch size:    16 | lm loss: 1.043402E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 338.061 | TFLOPs: 36.91 |
time (ms) | forward-compute: 17.42 | backward-compute: 25.14 | backward-embedding-all-reduce: 0.01 | optimizer: 3.74 | batch-generator: 0.74
[2023-03-17 11:16:36,704] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.69
[2023-03-17 11:16:36,704] [INFO] [logging.py:93:log_dist] [Rank 0] step=38, skipped=0, lr=[1.3317847059621894e-05, 1.3317847059621894e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:36,705] [INFO] [timer.py:198:stop] epoch=0/micro_step=38/global_step=38, RunningAvgSamplesPerSec=348.3764772248462, CurrSamplesPerSec=352.9036505716179, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:36,705] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.01 | backward_microstep: 24.80 | backward_inner_microstep: 22.37 | backward_allreduce_microstep: 2.38 | step_microstep: 3.92
[2023-03-17 11:16:36,705] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.00 | backward: 24.80 | backward_inner: 22.37 | backward_allreduce: 2.38 | step: 3.92
 iteration       38/      50 | consumed samples:          608 | consumed tokens:       622592 | elapsed time per iteration (ms): 47.0 | learning rate: 1.332E-05 | global batch size:    16 | lm loss: 1.042062E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 340.514 | TFLOPs: 37.18 |
time (ms) | forward-compute: 17.32 | backward-compute: 24.92 | backward-embedding-all-reduce: 0.01 | optimizer: 3.74 | batch-generator: 0.79
[2023-03-17 11:16:36,751] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:36,752] [INFO] [logging.py:93:log_dist] [Rank 0] step=39, skipped=0, lr=[1.2196142445053694e-05, 1.2196142445053694e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:36,752] [INFO] [timer.py:198:stop] epoch=0/micro_step=39/global_step=39, RunningAvgSamplesPerSec=348.4092857956691, CurrSamplesPerSec=349.59452391619175, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:36,752] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 15.94 | backward_microstep: 24.74 | backward_inner_microstep: 22.35 | backward_allreduce_microstep: 2.33 | step_microstep: 3.92
[2023-03-17 11:16:36,752] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 15.93 | backward: 24.74 | backward_inner: 22.34 | backward_allreduce: 2.33 | step: 3.93
 iteration       39/      50 | consumed samples:          624 | consumed tokens:       638976 | elapsed time per iteration (ms): 47.4 | learning rate: 1.220E-05 | global batch size:    16 | lm loss: 1.039051E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 337.327 | TFLOPs: 36.83 |
time (ms) | forward-compute: 17.74 | backward-compute: 24.84 | backward-embedding-all-reduce: 0.01 | optimizer: 3.80 | batch-generator: 1.15
[2023-03-17 11:16:36,799] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.69
[2023-03-17 11:16:36,799] [INFO] [logging.py:93:log_dist] [Rank 0] step=40, skipped=0, lr=[1.1156541151876422e-05, 1.1156541151876422e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:36,799] [INFO] [timer.py:198:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=348.46579861889234, CurrSamplesPerSec=350.5697390141463, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:36,799] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.06 | backward_microstep: 25.00 | backward_inner_microstep: 22.48 | backward_allreduce_microstep: 2.47 | step_microstep: 3.78
[2023-03-17 11:16:36,800] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.05 | backward: 25.00 | backward_inner: 22.47 | backward_allreduce: 2.47 | step: 3.78
 iteration       40/      50 | consumed samples:          640 | consumed tokens:       655360 | elapsed time per iteration (ms): 47.2 | learning rate: 1.116E-05 | global batch size:    16 | lm loss: 1.037596E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 339.127 | TFLOPs: 37.03 |
time (ms) | forward-compute: 17.41 | backward-compute: 24.88 | backward-embedding-all-reduce: 0.01 | optimizer: 3.81 | batch-generator: 0.83
[2023-03-17 11:16:36,846] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.69
[2023-03-17 11:16:36,846] [INFO] [logging.py:93:log_dist] [Rank 0] step=41, skipped=0, lr=[1.0203146011445599e-05, 1.0203146011445599e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:36,847] [INFO] [timer.py:198:stop] epoch=0/micro_step=41/global_step=41, RunningAvgSamplesPerSec=348.5540090136102, CurrSamplesPerSec=351.93941777714844, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:36,847] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 15.97 | backward_microstep: 24.77 | backward_inner_microstep: 22.39 | backward_allreduce_microstep: 2.33 | step_microstep: 4.33
[2023-03-17 11:16:36,847] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 15.96 | backward: 24.77 | backward_inner: 22.38 | backward_allreduce: 2.33 | step: 4.33
 iteration       41/      50 | consumed samples:          656 | consumed tokens:       671744 | elapsed time per iteration (ms): 47.1 | learning rate: 1.020E-05 | global batch size:    16 | lm loss: 1.038515E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 339.972 | TFLOPs: 37.12 |
time (ms) | forward-compute: 17.29 | backward-compute: 24.88 | backward-embedding-all-reduce: 0.01 | optimizer: 3.84 | batch-generator: 0.78
[2023-03-17 11:16:36,893] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:36,893] [INFO] [logging.py:93:log_dist] [Rank 0] step=42, skipped=0, lr=[9.33971963881569e-06, 9.33971963881569e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:36,893] [INFO] [timer.py:198:stop] epoch=0/micro_step=42/global_step=42, RunningAvgSamplesPerSec=348.6871674741224, CurrSamplesPerSec=353.9609059358419, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:36,894] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.00 | backward_microstep: 24.95 | backward_inner_microstep: 22.42 | backward_allreduce_microstep: 2.48 | step_microstep: 3.84
[2023-03-17 11:16:36,894] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 15.99 | backward: 24.95 | backward_inner: 22.42 | backward_allreduce: 2.48 | step: 3.84
 iteration       42/      50 | consumed samples:          672 | consumed tokens:       688128 | elapsed time per iteration (ms): 47.2 | learning rate: 9.340E-06 | global batch size:    16 | lm loss: 1.034731E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 339.259 | TFLOPs: 37.04 |
time (ms) | forward-compute: 17.42 | backward-compute: 25.02 | backward-embedding-all-reduce: 0.01 | optimizer: 3.69 | batch-generator: 0.68
[2023-03-17 11:16:36,940] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:36,940] [INFO] [logging.py:93:log_dist] [Rank 0] step=43, skipped=0, lr=[8.569669583417477e-06, 8.569669583417477e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:36,940] [INFO] [timer.py:198:stop] epoch=0/micro_step=43/global_step=43, RunningAvgSamplesPerSec=348.84934908904927, CurrSamplesPerSec=355.46267075580135, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:36,940] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.02 | backward_microstep: 24.78 | backward_inner_microstep: 22.38 | backward_allreduce_microstep: 2.35 | step_microstep: 3.82
[2023-03-17 11:16:36,940] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.01 | backward: 24.78 | backward_inner: 22.37 | backward_allreduce: 2.35 | step: 3.82
 iteration       43/      50 | consumed samples:          688 | consumed tokens:       704512 | elapsed time per iteration (ms): 46.6 | learning rate: 8.570E-06 | global batch size:    16 | lm loss: 1.036892E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 343.229 | TFLOPs: 37.48 |
time (ms) | forward-compute: 17.02 | backward-compute: 24.89 | backward-embedding-all-reduce: 0.01 | optimizer: 3.65 | batch-generator: 0.61
[2023-03-17 11:16:36,986] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.69
[2023-03-17 11:16:36,987] [INFO] [logging.py:93:log_dist] [Rank 0] step=44, skipped=0, lr=[7.896034881017213e-06, 7.896034881017213e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:36,987] [INFO] [timer.py:198:stop] epoch=0/micro_step=44/global_step=44, RunningAvgSamplesPerSec=348.9741757066221, CurrSamplesPerSec=354.17012697776045, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:36,987] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.01 | backward_microstep: 24.87 | backward_inner_microstep: 22.37 | backward_allreduce_microstep: 2.42 | step_microstep: 3.85
[2023-03-17 11:16:36,987] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.01 | backward: 24.86 | backward_inner: 22.37 | backward_allreduce: 2.43 | step: 3.85
 iteration       44/      50 | consumed samples:          704 | consumed tokens:       720896 | elapsed time per iteration (ms): 46.7 | learning rate: 7.896E-06 | global batch size:    16 | lm loss: 1.027463E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 342.249 | TFLOPs: 37.37 |
time (ms) | forward-compute: 17.08 | backward-compute: 24.95 | backward-embedding-all-reduce: 0.01 | optimizer: 3.66 | batch-generator: 0.66
[2023-03-17 11:16:37,034] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.69
[2023-03-17 11:16:37,034] [INFO] [logging.py:93:log_dist] [Rank 0] step=45, skipped=0, lr=[7.3214740600308545e-06, 7.3214740600308545e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:37,034] [INFO] [timer.py:198:stop] epoch=0/micro_step=45/global_step=45, RunningAvgSamplesPerSec=348.99511945454, CurrSamplesPerSec=349.8770326421872, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:37,034] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.03 | backward_microstep: 24.76 | backward_inner_microstep: 22.30 | backward_allreduce_microstep: 2.39 | step_microstep: 3.89
[2023-03-17 11:16:37,035] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.02 | backward: 24.75 | backward_inner: 22.29 | backward_allreduce: 2.40 | step: 3.89
 iteration       45/      50 | consumed samples:          720 | consumed tokens:       737280 | elapsed time per iteration (ms): 47.4 | learning rate: 7.321E-06 | global batch size:    16 | lm loss: 1.032204E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 337.633 | TFLOPs: 36.86 |
time (ms) | forward-compute: 17.78 | backward-compute: 24.91 | backward-embedding-all-reduce: 0.01 | optimizer: 3.67 | batch-generator: 1.08
[2023-03-17 11:16:37,081] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:37,081] [INFO] [logging.py:93:log_dist] [Rank 0] step=46, skipped=0, lr=[6.848254649526961e-06, 6.848254649526961e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:37,081] [INFO] [timer.py:198:stop] epoch=0/micro_step=46/global_step=46, RunningAvgSamplesPerSec=349.07708661983287, CurrSamplesPerSec=352.638469824755, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:37,081] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.05 | backward_microstep: 24.82 | backward_inner_microstep: 22.34 | backward_allreduce_microstep: 2.42 | step_microstep: 3.88
[2023-03-17 11:16:37,082] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.04 | backward: 24.82 | backward_inner: 22.34 | backward_allreduce: 2.42 | step: 3.88
 iteration       46/      50 | consumed samples:          736 | consumed tokens:       753664 | elapsed time per iteration (ms): 46.9 | learning rate: 6.848E-06 | global batch size:    16 | lm loss: 1.030843E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 340.945 | TFLOPs: 37.23 |
time (ms) | forward-compute: 17.25 | backward-compute: 24.97 | backward-embedding-all-reduce: 0.01 | optimizer: 3.66 | batch-generator: 0.71
[2023-03-17 11:16:37,128] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:37,128] [INFO] [logging.py:93:log_dist] [Rank 0] step=47, skipped=0, lr=[6.478244230325408e-06, 6.478244230325408e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:37,128] [INFO] [timer.py:198:stop] epoch=0/micro_step=47/global_step=47, RunningAvgSamplesPerSec=349.1265451534584, CurrSamplesPerSec=351.31668245899664, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:37,129] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 15.99 | backward_microstep: 24.80 | backward_inner_microstep: 22.32 | backward_allreduce_microstep: 2.42 | step_microstep: 3.89
[2023-03-17 11:16:37,129] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 15.98 | backward: 24.80 | backward_inner: 22.31 | backward_allreduce: 2.43 | step: 3.89
 iteration       47/      50 | consumed samples:          752 | consumed tokens:       770048 | elapsed time per iteration (ms): 47.2 | learning rate: 6.478E-06 | global batch size:    16 | lm loss: 1.030047E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 339.083 | TFLOPs: 37.02 |
time (ms) | forward-compute: 17.51 | backward-compute: 24.97 | backward-embedding-all-reduce: 0.01 | optimizer: 3.67 | batch-generator: 0.90
[2023-03-17 11:16:37,175] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.70
[2023-03-17 11:16:37,175] [INFO] [logging.py:93:log_dist] [Rank 0] step=48, skipped=0, lr=[6.2129030645091e-06, 6.2129030645091e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:37,176] [INFO] [timer.py:198:stop] epoch=0/micro_step=48/global_step=48, RunningAvgSamplesPerSec=349.17224713098653, CurrSamplesPerSec=351.2412934021417, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:37,176] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.02 | backward_microstep: 24.81 | backward_inner_microstep: 22.36 | backward_allreduce_microstep: 2.39 | step_microstep: 3.89
[2023-03-17 11:16:37,176] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.01 | backward: 24.81 | backward_inner: 22.35 | backward_allreduce: 2.39 | step: 3.89
 iteration       48/      50 | consumed samples:          768 | consumed tokens:       786432 | elapsed time per iteration (ms): 47.1 | learning rate: 6.213E-06 | global batch size:    16 | lm loss: 1.033036E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 339.524 | TFLOPs: 37.07 |
time (ms) | forward-compute: 17.44 | backward-compute: 25.01 | backward-embedding-all-reduce: 0.01 | optimizer: 3.65 | batch-generator: 0.70
[2023-03-17 11:16:37,222] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.69
[2023-03-17 11:16:37,222] [INFO] [logging.py:93:log_dist] [Rank 0] step=49, skipped=0, lr=[6.053278332436668e-06, 6.053278332436668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:37,223] [INFO] [timer.py:198:stop] epoch=0/micro_step=49/global_step=49, RunningAvgSamplesPerSec=349.23554213879265, CurrSamplesPerSec=352.1721269751308, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:37,223] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.02 | backward_microstep: 24.76 | backward_inner_microstep: 22.31 | backward_allreduce_microstep: 2.39 | step_microstep: 3.83
[2023-03-17 11:16:37,223] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.01 | backward: 24.76 | backward_inner: 22.30 | backward_allreduce: 2.39 | step: 3.84
 iteration       49/      50 | consumed samples:          784 | consumed tokens:       802816 | elapsed time per iteration (ms): 47.1 | learning rate: 6.053E-06 | global batch size:    16 | lm loss: 1.025728E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 339.926 | TFLOPs: 37.12 |
time (ms) | forward-compute: 17.45 | backward-compute: 24.86 | backward-embedding-all-reduce: 0.01 | optimizer: 3.71 | batch-generator: 0.69
[2023-03-17 11:16:37,269] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.69
[2023-03-17 11:16:37,269] [INFO] [logging.py:93:log_dist] [Rank 0] step=50, skipped=0, lr=[6e-06, 6e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 11:16:37,269] [INFO] [timer.py:198:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=349.3497865818511, CurrSamplesPerSec=354.80490422590316, MemAllocated=0.41GB, MaxMemAllocated=2.69GB
[2023-03-17 11:16:37,269] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 16.05 | backward_microstep: 24.86 | backward_inner_microstep: 22.29 | backward_allreduce_microstep: 2.51 | step_microstep: 3.90
[2023-03-17 11:16:37,270] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 16.04 | backward: 24.85 | backward_inner: 22.28 | backward_allreduce: 2.51 | step: 3.90
 iteration       50/      50 | consumed samples:          800 | consumed tokens:       819200 | elapsed time per iteration (ms): 46.6 | learning rate: 6.000E-06 | global batch size:    16 | lm loss: 1.026825E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 343.389 | TFLOPs: 37.49 |
time (ms) | forward-compute: 16.96 | backward-compute: 24.96 | backward-embedding-all-reduce: 0.01 | optimizer: 3.70 | batch-generator: 0.56
[after training is done] datetime: 2023-03-17 11:16:37 
------------------------------------------------------------------------------------------------------------------
 validation loss at the end of training for val data | lm loss value: 1.021308E+01 | lm loss PPL: 2.725730E+04 | 
------------------------------------------------------------------------------------------------------------------
gpu108-23-r:45129:45403 [1] NCCL INFO [Service thread] Connection closed by localRank 1
gpu108-23-r:45128:45404 [0] NCCL INFO [Service thread] Connection closed by localRank 0
gpu108-23-r:45130:45402 [2] NCCL INFO [Service thread] Connection closed by localRank 2
gpu108-23-r:45131:45401 [3] NCCL INFO [Service thread] Connection closed by localRank 3
gpu108-23-r:45129:45129 [1] NCCL INFO comm 0x39f6ebb0 rank 1 nranks 4 cudaDev 1 busId 46000 - Abort COMPLETE
gpu108-23-r:45128:45128 [0] NCCL INFO comm 0x3a855b90 rank 0 nranks 4 cudaDev 0 busId 7000 - Abort COMPLETE
gpu108-23-r:45131:45131 [3] NCCL INFO comm 0x385b9e50 rank 3 nranks 4 cudaDev 3 busId c7000 - Abort COMPLETE
gpu108-23-r:45130:45130 [2] NCCL INFO comm 0x39ed5dd0 rank 2 nranks 4 cudaDev 2 busId 85000 - Abort COMPLETE
[2023-03-17 11:16:39,513] [INFO] [launch.py:350:main] Process 45131 exits successfully.
[2023-03-17 11:16:39,514] [INFO] [launch.py:350:main] Process 45128 exits successfully.
[2023-03-17 11:16:39,514] [INFO] [launch.py:350:main] Process 45129 exits successfully.
[2023-03-17 11:16:40,515] [INFO] [launch.py:350:main] Process 45130 exits successfully.
