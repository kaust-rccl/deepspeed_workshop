------- JOB Configuration ---------
scontrol show job 24297798
JobId=24297798 JobName=g4
   UserId=shaima0d(174988) GroupId=g-shaima0d(1174988) MCS_label=N/A
   Priority=9697 Nice=0 Account=a100_training_acc QOS=a100_training_qos
   JobState=RUNNING Reason=None Dependency=(null)
   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=00:00:00 TimeLimit=00:15:00 TimeMin=N/A
   SubmitTime=2023-03-17T14:29:18 EligibleTime=2023-03-17T14:29:18
   AccrueTime=2023-03-17T14:29:18
   StartTime=2023-03-17T14:29:21 EndTime=2023-03-17T14:44:21 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2023-03-17T14:29:21 Scheduler=Main
   Partition=a100_training AllocNode:Sid=login510-27:152451
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=gpu108-09-l,gpu108-16-r
   BatchHost=gpu108-09-l
   NumNodes=2 NumCPUs=120 NumTasks=8 CPUs/Task=15 ReqB:S:C:T=0:0:*:*
   TRES=cpu=120,mem=240G,node=2,billing=120,gres/gpu=8
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
   MinCPUsNode=15 MinMemoryCPU=2G MinTmpDiskNode=0
   Features=(a100)&el7 DelayBoot=00:00:00
   Reservation=A100
   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
   Command=/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/G8N2.slurm
   WorkDir=/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed
   StdErr=/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/slurm-24297798.out
   StdIn=/dev/null
   StdOut=/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/slurm-24297798.out
   Power=
   TresPerJob=gres:gpu:8
   TresPerNode=gres:gpu:4
   

------- GPU Configuration ---------
nvidia-smi -L
GPU 0: NVIDIA A100-SXM4-80GB (UUID: GPU-92a4a935-8de7-2474-9cab-8895a6be9bda)
GPU 1: NVIDIA A100-SXM4-80GB (UUID: GPU-d84eac71-0b05-f77c-ca54-360f4f44ecd2)
GPU 2: NVIDIA A100-SXM4-80GB (UUID: GPU-ad9c6cbc-8f0a-e612-c325-693e23319777)
GPU 3: NVIDIA A100-SXM4-80GB (UUID: GPU-c9122ab1-c1e8-a2ec-027b-838e580a5876)
------- NVLink Configuration ------
nvidia-smi topo -m
	[4mGPU0	GPU1	GPU2	GPU3	mlx5_0	mlx5_1	CPU Affinity	NUMA Affinity[0m
GPU0	 X 	NV4	NV4	NV4	NODE	SYS	32-59	1
GPU1	NV4	 X 	NV4	NV4	PHB	SYS	32-59	1
GPU2	NV4	NV4	 X 	NV4	SYS	NODE	0-31	0
GPU3	NV4	NV4	NV4	 X 	SYS	PHB	0-31	0
mlx5_0	NODE	PHB	SYS	SYS	 X 	SYS		
mlx5_1	SYS	SYS	NODE	PHB	SYS	 X 		

Legend:

  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing at most a single PCIe bridge
  NV#  = Connection traversing a bonded set of # NVLinks
------- Infiniband Configuration --
ibv_devinfo
hca_id:	mlx5_0
	transport:			InfiniBand (0)
	fw_ver:				20.34.1002
	node_guid:			88e9:a4ff:ff1c:f62c
	sys_image_guid:			88e9:a4ff:ff1c:f62c
	vendor_id:			0x02c9
	vendor_part_id:			4123
	hw_ver:				0x0
	board_id:			MT_0000000451
	phys_port_cnt:			1
		port:	1
			state:			PORT_ACTIVE (4)
			max_mtu:		4096 (5)
			active_mtu:		4096 (5)
			sm_lid:			1
			port_lid:		607
			port_lmc:		0x00
			link_layer:		InfiniBand

hca_id:	mlx5_1
	transport:			InfiniBand (0)
	fw_ver:				20.34.1002
	node_guid:			88e9:a4ff:ff1c:f628
	sys_image_guid:			88e9:a4ff:ff1c:f628
	vendor_id:			0x02c9
	vendor_part_id:			4123
	hw_ver:				0x0
	board_id:			MT_0000000451
	phys_port_cnt:			1
		port:	1
			state:			PORT_ACTIVE (4)
			max_mtu:		4096 (5)
			active_mtu:		4096 (5)
			sm_lid:			1
			port_lid:		606
			port_lmc:		0x00
			link_layer:		InfiniBand

Loading module for CUDA 11.7.0
CUDA 11.7.0 is now loaded
GNU 11.1.0 is now loaded
Loading module for nccl-2.17.1.1
nccl-2.17.1.1 modules now loaded
Loading module for gdrcopy 2.0_cuda11.7.0
gdrcopy 2.0_cuda11.7.0 modules now loaded
Loading module for ucx-gpu 1.14.0
ucx-gpu 1.14.0 modules now loaded
Loading module for OPENMPI 4.1.4
OPENMPI 4.1.4 modules now loaded
Loading module for pytorch-1.13.1_cuda11.7.0
pytorch-1.13.1_cuda11.7.0 modules now loaded
Loading module for deepspeed-0.8.3
deepspeed-0.8.3 modules now loaded
Loading module for apex-22.03
apex-22.03 modules now loaded
Currently Loaded Modulefiles:
  1) dl/2023             5) nccl/2.17.1.1       9) pytorch/1.13.1
  2) python/3.9.16       6) gdrcopy/2.0        10) deepspeed/0.8.3
  3) cuda/11.7.0         7) ucx/1.14.0         11) apex/22.03
  4) gcc/11.1.0          8) openmpi-gpu/4.1.4
[2023-03-17 14:29:25,336] [INFO] [runner.py:550:main] cmd = srun -n 8 --nodes 2 --gpus 8 /sw/csgv/dl/apps/python/3.9.16/bin/python3.9 -u pretrain_gpt.py --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 1 --hidden-size 12288 --num-attention-heads 96 --seq-length 1024 --loss-scale 15 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 32 --train-iters 50 --lr 6.0e-5 --min-lr 6.0e-6 --lr-decay-style cosine --log-interval 1 --eval-iters 40 --eval-interval 1000 --data-path /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document --vocab-file /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json --merge-file /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt --save-interval 1000 --split 98,2,0 --clip-grad 1.0 --weight-decay 0.1 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.006 --fp16 --checkpoint-activations --tensorboard-dir ds_z_off-none_stage_3_nl1_hs12288_mb4_seq1024_gb32_nodes2 --deepspeed-activation-checkpointing --zero-stage=3 --deepspeed_config=ds_config.json --no-pipeline-parallel --deepspeed --exit-interval 5000
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [93m[NO][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  data_parallel_size .............................. 1
  data_path ....................................... ['/ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 40
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 5000
  expert_interval ................................. 2
  ffn_hidden_size ................................. 49152
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 12288
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 128
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 15.0
  loss_scale_window ............................... 1000
  lr .............................................. 6e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ None
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 96
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 1
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. ds_z_off-none_stage_3_nl1_hs12288_mb4_seq1024_gb32_nodes2
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 50
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 1
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 3
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  data_parallel_size .............................. 1
  data_path ....................................... ['/ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 40
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  exit_interval ................................... 5000
  expert_interval ................................. 2
  ffn_hidden_size ................................. 49152
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 12288
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 128
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 15.0
  loss_scale_window ............................... 1000
  lr .............................................. 6e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  data_parallel_size .............................. 1
  data_path ....................................... ['/ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ None
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 40
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 5000
  expert_interval ................................. 2
  ffn_hidden_size ................................. 49152
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 12288
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 96
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 1
  num_layers_per_virtual_pipeline_stage ........... None
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 128
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  num_layers_teacher .............................. None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 15.0
  loss_scale_window ............................... 1000
  lr .............................................. 6e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ None
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. ds_z_off-none_stage_3_nl1_hs12288_mb4_seq1024_gb32_nodes2
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 50
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 96
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 1
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  num_experts_teacher ............................. [1]
  num_layers ...................................... 1
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 3
-------------------- end of arguments ---------------------
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. ds_z_off-none_stage_3_nl1_hs12288_mb4_seq1024_gb32_nodes2
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 50
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 1
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 3
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  data_parallel_size .............................. 1
  data_path ....................................... ['/ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 40
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 5000
  expert_interval ................................. 2
  ffn_hidden_size ................................. 49152
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 12288
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 128
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 15.0
  loss_scale_window ............................... 1000
  lr .............................................. 6e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ None
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 96
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 1
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. ds_z_off-none_stage_3_nl1_hs12288_mb4_seq1024_gb32_nodes2
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 50
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 1
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 3
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> setting tensorboard ...
> setting tensorboard ...
> setting tensorboard ...
> setting tensorboard ...
[2023-03-17 14:29:29,570] [INFO] [comm.py:634:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-03-17 14:29:29,570] [INFO] [comm.py:634:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-03-17 14:29:29,571] [INFO] [comm.py:634:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-03-17 14:29:29,571] [INFO] [comm.py:634:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  data_parallel_size .............................. 1
  data_path ....................................... ['/ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 40
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 5000
  expert_interval ................................. 2
  ffn_hidden_size ................................. 49152
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 12288
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 128
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  data_parallel_size .............................. 1
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  data_path ....................................... ['/ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 15.0
  loss_scale_window ............................... 1000
  lr .............................................. 6e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 40
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  lr_warmup_tokens ................................ None
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  exit_interval ................................... 5000
  expert_interval ................................. 2
  ffn_hidden_size ................................. 49152
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 12288
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  no_save_rng ..................................... None
  num_attention_heads ............................. 96
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 1
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 128
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 15.0
  loss_scale_window ............................... 1000
  lr .............................................. 6e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ None
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mlp_type ........................................ standard
  tensorboard_dir ................................. ds_z_off-none_stage_3_nl1_hs12288_mb4_seq1024_gb32_nodes2
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  train_iters ..................................... 50
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 96
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 1
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 1
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 3
-------------------- end of arguments ---------------------
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. ds_z_off-none_stage_3_nl1_hs12288_mb4_seq1024_gb32_nodes2
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 50
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 1
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 3
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  data_parallel_size .............................. 1
  data_path ....................................... ['/ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 40
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 5000
  expert_interval ................................. 2
  ffn_hidden_size ................................. 49152
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 12288
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 128
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 15.0
  loss_scale_window ............................... 1000
  lr .............................................. 6e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ None
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 96
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 1
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. ds_z_off-none_stage_3_nl1_hs12288_mb4_seq1024_gb32_nodes2
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 50
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 1
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 3
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  data_parallel_size .............................. 1
  data_path ....................................... ['/ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 40
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 5000
  expert_interval ................................. 2
  ffn_hidden_size ................................. 49152
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 12288
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 128
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 15.0
  loss_scale_window ............................... 1000
  lr .............................................. 6e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ None
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 96
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 1
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. ds_z_off-none_stage_3_nl1_hs12288_mb4_seq1024_gb32_nodes2
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 50
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 1
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 3
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> setting tensorboard ...
> setting tensorboard ...
> setting tensorboard ...
> setting tensorboard ...
[2023-03-17 14:29:31,245] [INFO] [comm.py:634:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-03-17 14:29:31,245] [INFO] [comm.py:634:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-03-17 14:29:31,246] [INFO] [comm.py:634:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-03-17 14:29:31,246] [INFO] [comm.py:634:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-03-17 14:29:31,928] [INFO] [comm.py:688:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=8, master_addr=10.109.8.49, master_port=29500
[2023-03-17 14:29:31,928] [INFO] [comm.py:688:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=8, master_addr=10.109.8.49, master_port=29500
[2023-03-17 14:29:31,928] [INFO] [comm.py:688:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=8, master_addr=10.109.8.49, master_port=29500
[2023-03-17 14:29:31,928] [INFO] [comm.py:688:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=0, world_size=8, master_addr=10.109.8.49, master_port=29500
[2023-03-17 14:29:31,928] [INFO] [comm.py:688:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=8, master_addr=10.109.8.49, master_port=29500
[2023-03-17 14:29:31,928] [INFO] [comm.py:652:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-03-17 14:29:31,929] [INFO] [comm.py:688:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=3, world_size=8, master_addr=10.109.8.49, master_port=29500
[2023-03-17 14:29:31,930] [INFO] [comm.py:688:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=1, world_size=8, master_addr=10.109.8.49, master_port=29500
[2023-03-17 14:29:31,931] [INFO] [comm.py:688:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=2, world_size=8, master_addr=10.109.8.49, master_port=29500
Hi, I am 0 and pinning GPU 0
Hi, I am 4 and pinning GPU 0
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
Hi, I am 7 and pinning GPU 3
Hi, I am 5 and pinning GPU 1
Hi, I am 6 and pinning GPU 2
Hi, I am 2 and pinning GPU 2
Hi, I am 3 and pinning GPU 3
Hi, I am 1 and pinning GPU 1
> setting random seeds to 1234 ...
[2023-03-17 14:29:33,226] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> compiling dataset index builder ...
make: Entering directory `/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/megatron/data'
make: Nothing to be done for `default'.
make: Leaving directory `/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/megatron/data'
>>> done with dataset index builder. Compilation time: 0.049 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (15) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (15) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (15) as the number of workers...
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
gpu108-09-l:50445:50445 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-09-l:50445:50445 [0] NCCL INFO Bootstrap : Using ib0:10.109.136.49<0>
gpu108-09-l:50445:50445 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu108-09-l:50445:50445 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu108-09-l:50445:50445 [0] NCCL INFO cudaDriverVersion 11080
NCCL version 2.17.1+cuda11.7
gpu108-09-l:50447:50447 [2] NCCL INFO cudaDriverVersion 11080
gpu108-09-l:50447:50447 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-09-l:50447:50447 [2] NCCL INFO Bootstrap : Using ib0:10.109.136.49<0>
gpu108-09-l:50447:50447 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu108-09-l:50447:50447 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu108-09-l:50448:50448 [3] NCCL INFO cudaDriverVersion 11080
gpu108-09-l:50446:50446 [1] NCCL INFO cudaDriverVersion 11080
gpu108-09-l:50448:50448 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-09-l:50448:50448 [3] NCCL INFO Bootstrap : Using ib0:10.109.136.49<0>
gpu108-09-l:50446:50446 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-09-l:50448:50448 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu108-09-l:50448:50448 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu108-09-l:50446:50446 [1] NCCL INFO Bootstrap : Using ib0:10.109.136.49<0>
gpu108-09-l:50446:50446 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu108-09-l:50446:50446 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu108-16-r:61912:61912 [1] NCCL INFO cudaDriverVersion 11080
gpu108-16-r:61912:61912 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-16-r:61913:61913 [2] NCCL INFO cudaDriverVersion 11080
gpu108-16-r:61912:61912 [1] NCCL INFO Bootstrap : Using ib0:10.109.136.92<0>
gpu108-16-r:61912:61912 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu108-16-r:61912:61912 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu108-16-r:61911:61911 [0] NCCL INFO cudaDriverVersion 11080
gpu108-16-r:61913:61913 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-16-r:61911:61911 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-16-r:61913:61913 [2] NCCL INFO Bootstrap : Using ib0:10.109.136.92<0>
gpu108-16-r:61911:61911 [0] NCCL INFO Bootstrap : Using ib0:10.109.136.92<0>
gpu108-16-r:61911:61911 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu108-16-r:61911:61911 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu108-16-r:61913:61913 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu108-16-r:61913:61913 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu108-16-r:61914:61914 [3] NCCL INFO cudaDriverVersion 11080
gpu108-16-r:61914:61914 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-16-r:61914:61914 [3] NCCL INFO Bootstrap : Using ib0:10.109.136.92<0>
gpu108-16-r:61914:61914 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu108-16-r:61914:61914 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu108-09-l:50445:50910 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-09-l:50446:50911 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-09-l:50448:50912 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-09-l:50447:50914 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-16-r:61912:62316 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-09-l:50445:50910 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.136.49<0>
gpu108-09-l:50445:50910 [0] NCCL INFO Using network IB
gpu108-09-l:50446:50911 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.136.49<0>
gpu108-09-l:50446:50911 [1] NCCL INFO Using network IB
gpu108-09-l:50448:50912 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.136.49<0>
gpu108-09-l:50448:50912 [3] NCCL INFO Using network IB
gpu108-09-l:50447:50914 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.136.49<0>
gpu108-09-l:50447:50914 [2] NCCL INFO Using network IB
gpu108-16-r:61912:62316 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.136.92<0>
gpu108-16-r:61912:62316 [1] NCCL INFO Using network IB
gpu108-16-r:61911:62319 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-16-r:61913:62320 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-16-r:61914:62321 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-16-r:61911:62319 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.136.92<0>
gpu108-16-r:61911:62319 [0] NCCL INFO Using network IB
gpu108-16-r:61913:62320 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.136.92<0>
gpu108-16-r:61913:62320 [2] NCCL INFO Using network IB
gpu108-16-r:61914:62321 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.136.92<0>
gpu108-16-r:61914:62321 [3] NCCL INFO Using network IB
gpu108-09-l:50445:50910 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-09-l:50445:50910 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu108-09-l:50445:50910 [0] NCCL INFO Setting affinity for GPU 0 to 0fffffff,00000000
gpu108-09-l:50446:50911 [1] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu108-09-l:50446:50911 [1] NCCL INFO Setting affinity for GPU 1 to 0fffffff,00000000
gpu108-16-r:61911:62319 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu108-16-r:61911:62319 [0] NCCL INFO Setting affinity for GPU 0 to 0fffffff,00000000
gpu108-09-l:50448:50912 [3] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu108-09-l:50448:50912 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff
gpu108-09-l:50447:50914 [2] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu108-09-l:50447:50914 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff
gpu108-16-r:61914:62321 [3] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu108-16-r:61913:62320 [2] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu108-16-r:61914:62321 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff
gpu108-16-r:61913:62320 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff
gpu108-16-r:61912:62316 [1] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu108-16-r:61912:62316 [1] NCCL INFO Setting affinity for GPU 1 to 0fffffff,00000000
gpu108-16-r:61912:62316 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 4/-1/-1->5->6 [2] 6/-1/-1->5->4 [3] 4/-1/-1->5->6
gpu108-16-r:61912:62316 [1] NCCL INFO P2P Chunksize set to 131072
gpu108-09-l:50445:50910 [0] NCCL INFO Channel 00/04 :    0   3   2   1   4   7   6   5
gpu108-09-l:50445:50910 [0] NCCL INFO Channel 01/04 :    0   3   6   5   4   7   2   1
gpu108-16-r:61913:62320 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 5/-1/-1->6->7 [2] 7/-1/-1->6->5 [3] 5/-1/-1->6->7
gpu108-16-r:61913:62320 [2] NCCL INFO P2P Chunksize set to 131072
gpu108-09-l:50445:50910 [0] NCCL INFO Channel 02/04 :    0   3   2   1   4   7   6   5
gpu108-09-l:50445:50910 [0] NCCL INFO Channel 03/04 :    0   3   6   5   4   7   2   1
gpu108-16-r:61914:62321 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 6/-1/-1->7->3 [2] -1/-1/-1->7->6 [3] 6/3/-1->7->-1
gpu108-16-r:61914:62321 [3] NCCL INFO P2P Chunksize set to 131072
gpu108-09-l:50446:50911 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 0/-1/-1->1->2 [2] 2/-1/-1->1->0 [3] 0/-1/-1->1->2
gpu108-09-l:50446:50911 [1] NCCL INFO P2P Chunksize set to 131072
gpu108-09-l:50447:50914 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 1/-1/-1->2->3 [2] 3/-1/-1->2->1 [3] 1/-1/-1->2->3
gpu108-09-l:50447:50914 [2] NCCL INFO P2P Chunksize set to 131072
gpu108-09-l:50448:50912 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 2/7/-1->3->-1 [2] -1/-1/-1->3->2 [3] 2/-1/-1->3->7
gpu108-09-l:50448:50912 [3] NCCL INFO P2P Chunksize set to 131072
gpu108-09-l:50445:50910 [0] NCCL INFO Trees [0] 1/4/-1->0->-1 [1] -1/-1/-1->0->1 [2] 1/-1/-1->0->4 [3] -1/-1/-1->0->1
gpu108-09-l:50445:50910 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-16-r:61911:62319 [0] NCCL INFO Trees [0] 5/-1/-1->4->0 [1] -1/-1/-1->4->5 [2] 5/0/-1->4->-1 [3] -1/-1/-1->4->5
gpu108-16-r:61911:62319 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-09-l:50445:50910 [0] NCCL INFO Channel 00/0 : 5[46000] -> 0[7000] [receive] via NET/IB/0/GDRDMA
gpu108-09-l:50446:50911 [1] NCCL INFO Channel 00/0 : 1[46000] -> 4[7000] [send] via NET/IB/0/GDRDMA
gpu108-09-l:50447:50914 [2] NCCL INFO Channel 01/0 : 7[c7000] -> 2[85000] [receive] via NET/IB/1/GDRDMA
gpu108-16-r:61911:62319 [0] NCCL INFO Channel 00/0 : 1[46000] -> 4[7000] [receive] via NET/IB/0/GDRDMA
gpu108-16-r:61912:62316 [1] NCCL INFO Channel 00/0 : 5[46000] -> 0[7000] [send] via NET/IB/0/GDRDMA
gpu108-16-r:61913:62320 [2] NCCL INFO Channel 01/0 : 3[c7000] -> 6[85000] [receive] via NET/IB/1/GDRDMA
gpu108-09-l:50445:50910 [0] NCCL INFO Channel 02/0 : 5[46000] -> 0[7000] [receive] via NET/IB/0/GDRDMA
gpu108-09-l:50446:50911 [1] NCCL INFO Channel 02/0 : 1[46000] -> 4[7000] [send] via NET/IB/0/GDRDMA
gpu108-09-l:50447:50914 [2] NCCL INFO Channel 03/0 : 7[c7000] -> 2[85000] [receive] via NET/IB/1/GDRDMA
gpu108-09-l:50445:50910 [0] NCCL INFO Channel 00/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:61911:62319 [0] NCCL INFO Channel 02/0 : 1[46000] -> 4[7000] [receive] via NET/IB/0/GDRDMA
gpu108-16-r:61912:62316 [1] NCCL INFO Channel 02/0 : 5[46000] -> 0[7000] [send] via NET/IB/0/GDRDMA
gpu108-16-r:61913:62320 [2] NCCL INFO Channel 03/0 : 3[c7000] -> 6[85000] [receive] via NET/IB/1/GDRDMA
gpu108-16-r:61911:62319 [0] NCCL INFO Channel 00/0 : 4[7000] -> 7[c7000] via P2P/IPC/read
gpu108-09-l:50445:50910 [0] NCCL INFO Channel 01/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:61911:62319 [0] NCCL INFO Channel 01/0 : 4[7000] -> 7[c7000] via P2P/IPC/read
gpu108-09-l:50445:50910 [0] NCCL INFO Channel 02/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:61911:62319 [0] NCCL INFO Channel 02/0 : 4[7000] -> 7[c7000] via P2P/IPC/read
gpu108-16-r:61914:62321 [3] NCCL INFO Channel 01/0 : 7[c7000] -> 2[85000] [send] via NET/IB/1/GDRDMA
gpu108-09-l:50448:50912 [3] NCCL INFO Channel 01/0 : 3[c7000] -> 6[85000] [send] via NET/IB/1/GDRDMA
gpu108-09-l:50445:50910 [0] NCCL INFO Channel 03/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:61911:62319 [0] NCCL INFO Channel 03/0 : 4[7000] -> 7[c7000] via P2P/IPC/read
gpu108-16-r:61914:62321 [3] NCCL INFO Channel 03/0 : 7[c7000] -> 2[85000] [send] via NET/IB/1/GDRDMA
gpu108-16-r:61914:62321 [3] NCCL INFO Channel 00/0 : 7[c7000] -> 6[85000] via P2P/IPC/read
gpu108-09-l:50448:50912 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 6[85000] [send] via NET/IB/1/GDRDMA
gpu108-09-l:50448:50912 [3] NCCL INFO Channel 00/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:61914:62321 [3] NCCL INFO Channel 02/0 : 7[c7000] -> 6[85000] via P2P/IPC/read
gpu108-09-l:50448:50912 [3] NCCL INFO Channel 02/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-09-l:50447:50914 [2] NCCL INFO Channel 00/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:61913:62320 [2] NCCL INFO Channel 00/0 : 6[85000] -> 5[46000] via P2P/IPC/read
gpu108-09-l:50447:50914 [2] NCCL INFO Channel 01/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:61913:62320 [2] NCCL INFO Channel 01/0 : 6[85000] -> 5[46000] via P2P/IPC/read
gpu108-09-l:50446:50911 [1] NCCL INFO Channel 01/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:61912:62316 [1] NCCL INFO Channel 01/0 : 5[46000] -> 4[7000] via P2P/IPC/read
gpu108-09-l:50447:50914 [2] NCCL INFO Channel 02/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:61913:62320 [2] NCCL INFO Channel 02/0 : 6[85000] -> 5[46000] via P2P/IPC/read
gpu108-09-l:50446:50911 [1] NCCL INFO Channel 03/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:61912:62316 [1] NCCL INFO Channel 03/0 : 5[46000] -> 4[7000] via P2P/IPC/read
gpu108-09-l:50447:50914 [2] NCCL INFO Channel 03/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:61913:62320 [2] NCCL INFO Channel 03/0 : 6[85000] -> 5[46000] via P2P/IPC/read
gpu108-09-l:50448:50912 [3] NCCL INFO Connected all rings
gpu108-09-l:50446:50911 [1] NCCL INFO Connected all rings
gpu108-16-r:61914:62321 [3] NCCL INFO Connected all rings
gpu108-09-l:50447:50914 [2] NCCL INFO Connected all rings
gpu108-09-l:50445:50910 [0] NCCL INFO Connected all rings
gpu108-09-l:50445:50910 [0] NCCL INFO Channel 00/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:61912:62316 [1] NCCL INFO Connected all rings
gpu108-16-r:61911:62319 [0] NCCL INFO Connected all rings
gpu108-16-r:61911:62319 [0] NCCL INFO Channel 00/0 : 4[7000] -> 5[46000] via P2P/IPC/read
gpu108-16-r:61913:62320 [2] NCCL INFO Connected all rings
gpu108-09-l:50445:50910 [0] NCCL INFO Channel 01/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:61911:62319 [0] NCCL INFO Channel 01/0 : 4[7000] -> 5[46000] via P2P/IPC/read
gpu108-09-l:50445:50910 [0] NCCL INFO Channel 02/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:61911:62319 [0] NCCL INFO Channel 02/0 : 4[7000] -> 5[46000] via P2P/IPC/read
gpu108-09-l:50445:50910 [0] NCCL INFO Channel 03/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:61911:62319 [0] NCCL INFO Channel 03/0 : 4[7000] -> 5[46000] via P2P/IPC/read
gpu108-09-l:50446:50911 [1] NCCL INFO Channel 00/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-09-l:50447:50914 [2] NCCL INFO Channel 00/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:61912:62316 [1] NCCL INFO Channel 00/0 : 5[46000] -> 6[85000] via P2P/IPC/read
gpu108-16-r:61913:62320 [2] NCCL INFO Channel 00/0 : 6[85000] -> 7[c7000] via P2P/IPC/read
gpu108-09-l:50446:50911 [1] NCCL INFO Channel 01/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-09-l:50447:50914 [2] NCCL INFO Channel 01/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:61912:62316 [1] NCCL INFO Channel 01/0 : 5[46000] -> 6[85000] via P2P/IPC/read
gpu108-16-r:61913:62320 [2] NCCL INFO Channel 01/0 : 6[85000] -> 7[c7000] via P2P/IPC/read
gpu108-09-l:50446:50911 [1] NCCL INFO Channel 02/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-09-l:50447:50914 [2] NCCL INFO Channel 02/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:61912:62316 [1] NCCL INFO Channel 02/0 : 5[46000] -> 6[85000] via P2P/IPC/read
gpu108-16-r:61913:62320 [2] NCCL INFO Channel 02/0 : 6[85000] -> 7[c7000] via P2P/IPC/read
gpu108-09-l:50446:50911 [1] NCCL INFO Channel 03/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-09-l:50447:50914 [2] NCCL INFO Channel 03/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:61912:62316 [1] NCCL INFO Channel 03/0 : 5[46000] -> 6[85000] via P2P/IPC/read
gpu108-16-r:61913:62320 [2] NCCL INFO Channel 03/0 : 6[85000] -> 7[c7000] via P2P/IPC/read
gpu108-09-l:50446:50911 [1] NCCL INFO Channel 00/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:61912:62316 [1] NCCL INFO Channel 00/0 : 5[46000] -> 4[7000] via P2P/IPC/read
gpu108-09-l:50445:50910 [0] NCCL INFO Channel 00/0 : 4[7000] -> 0[7000] [receive] via NET/IB/0/GDRDMA
gpu108-09-l:50446:50911 [1] NCCL INFO Channel 02/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-09-l:50448:50912 [3] NCCL INFO Channel 01/0 : 7[c7000] -> 3[c7000] [receive] via NET/IB/1/GDRDMA
gpu108-16-r:61911:62319 [0] NCCL INFO Channel 00/0 : 0[7000] -> 4[7000] [receive] via NET/IB/0/GDRDMA
gpu108-16-r:61912:62316 [1] NCCL INFO Channel 02/0 : 5[46000] -> 4[7000] via P2P/IPC/read
gpu108-16-r:61914:62321 [3] NCCL INFO Channel 01/0 : 3[c7000] -> 7[c7000] [receive] via NET/IB/1/GDRDMA
gpu108-09-l:50445:50910 [0] NCCL INFO Channel 02/0 : 4[7000] -> 0[7000] [receive] via NET/IB/0/GDRDMA
gpu108-09-l:50448:50912 [3] NCCL INFO Channel 03/0 : 7[c7000] -> 3[c7000] [receive] via NET/IB/1/GDRDMA
gpu108-16-r:61911:62319 [0] NCCL INFO Channel 02/0 : 0[7000] -> 4[7000] [receive] via NET/IB/0/GDRDMA
gpu108-16-r:61914:62321 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 7[c7000] [receive] via NET/IB/1/GDRDMA
gpu108-09-l:50445:50910 [0] NCCL INFO Channel 00/0 : 0[7000] -> 4[7000] [send] via NET/IB/0/GDRDMA
gpu108-09-l:50448:50912 [3] NCCL INFO Channel 01/0 : 3[c7000] -> 7[c7000] [send] via NET/IB/1/GDRDMA
gpu108-16-r:61911:62319 [0] NCCL INFO Channel 00/0 : 4[7000] -> 0[7000] [send] via NET/IB/0/GDRDMA
gpu108-16-r:61914:62321 [3] NCCL INFO Channel 01/0 : 7[c7000] -> 3[c7000] [send] via NET/IB/1/GDRDMA
gpu108-09-l:50445:50910 [0] NCCL INFO Channel 02/0 : 0[7000] -> 4[7000] [send] via NET/IB/0/GDRDMA
gpu108-09-l:50448:50912 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 7[c7000] [send] via NET/IB/1/GDRDMA
gpu108-16-r:61911:62319 [0] NCCL INFO Channel 02/0 : 4[7000] -> 0[7000] [send] via NET/IB/0/GDRDMA
gpu108-16-r:61914:62321 [3] NCCL INFO Channel 03/0 : 7[c7000] -> 3[c7000] [send] via NET/IB/1/GDRDMA
gpu108-16-r:61914:62321 [3] NCCL INFO Channel 01/0 : 7[c7000] -> 6[85000] via P2P/IPC/read
gpu108-09-l:50448:50912 [3] NCCL INFO Channel 01/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-09-l:50448:50912 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:61914:62321 [3] NCCL INFO Channel 03/0 : 7[c7000] -> 6[85000] via P2P/IPC/read
gpu108-16-r:61912:62316 [1] NCCL INFO Connected all trees
gpu108-16-r:61912:62316 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-16-r:61912:62316 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-09-l:50446:50911 [1] NCCL INFO Connected all trees
gpu108-09-l:50446:50911 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-09-l:50446:50911 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-09-l:50447:50914 [2] NCCL INFO Connected all trees
gpu108-09-l:50447:50914 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-09-l:50447:50914 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-16-r:61913:62320 [2] NCCL INFO Connected all trees
gpu108-16-r:61913:62320 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-16-r:61913:62320 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-16-r:61911:62319 [0] NCCL INFO Connected all trees
gpu108-16-r:61911:62319 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-16-r:61911:62319 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-09-l:50445:50910 [0] NCCL INFO Connected all trees
gpu108-09-l:50445:50910 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-09-l:50445:50910 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-16-r:61914:62321 [3] NCCL INFO Connected all trees
gpu108-09-l:50448:50912 [3] NCCL INFO Connected all trees
gpu108-09-l:50448:50912 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-09-l:50448:50912 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-16-r:61914:62321 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-16-r:61914:62321 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-09-l:50445:50910 [0] NCCL INFO comm 0x3a07ea30 rank 0 nranks 8 cudaDev 0 busId 7000 commId 0x3b31ab6ac05242ee - Init COMPLETE
gpu108-09-l:50446:50911 [1] NCCL INFO comm 0x3a4e24f0 rank 1 nranks 8 cudaDev 1 busId 46000 commId 0x3b31ab6ac05242ee - Init COMPLETE
gpu108-09-l:50448:50912 [3] NCCL INFO comm 0x399e2640 rank 3 nranks 8 cudaDev 3 busId c7000 commId 0x3b31ab6ac05242ee - Init COMPLETE
gpu108-16-r:61911:62319 [0] NCCL INFO comm 0x39660c10 rank 4 nranks 8 cudaDev 0 busId 7000 commId 0x3b31ab6ac05242ee - Init COMPLETE
gpu108-16-r:61912:62316 [1] NCCL INFO comm 0x3b52f2b0 rank 5 nranks 8 cudaDev 1 busId 46000 commId 0x3b31ab6ac05242ee - Init COMPLETE
gpu108-16-r:61914:62321 [3] NCCL INFO comm 0x3b65b0c0 rank 7 nranks 8 cudaDev 3 busId c7000 commId 0x3b31ab6ac05242ee - Init COMPLETE
gpu108-09-l:50447:50914 [2] NCCL INFO comm 0x39a80100 rank 2 nranks 8 cudaDev 2 busId 85000 commId 0x3b31ab6ac05242ee - Init COMPLETE
gpu108-16-r:61913:62320 [2] NCCL INFO comm 0x3bb40b70 rank 6 nranks 8 cudaDev 2 busId 85000 commId 0x3b31ab6ac05242ee - Init COMPLETE
>>> done with compiling and loading fused kernels. Compilation time: 6.137 seconds
time to initialize megatron (seconds): 35.413
[after megatron is initialized] datetime: 2023-03-17 14:29:39 
building GPT model ...
[2023-03-17 14:29:39,457] [INFO] [utils.py:829:see_memory_usage] Before Building Model
[2023-03-17 14:29:39,457] [INFO] [utils.py:830:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2023-03-17 14:29:39,458] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 49.38 GB, percent = 9.8%
gpu108-09-l:50445:50978 [0] NCCL INFO Using network IB
gpu108-09-l:50446:50980 [1] NCCL INFO Using network IB
gpu108-09-l:50447:50981 [2] NCCL INFO Using network IB
gpu108-09-l:50448:50979 [3] NCCL INFO Using network IB
gpu108-16-r:61911:62362 [0] NCCL INFO Using network IB
gpu108-16-r:61914:62360 [3] NCCL INFO Using network IB
gpu108-16-r:61912:62361 [1] NCCL INFO Using network IB
gpu108-16-r:61913:62363 [2] NCCL INFO Using network IB
gpu108-09-l:50448:50979 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff
gpu108-09-l:50447:50981 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff
gpu108-09-l:50445:50978 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-09-l:50446:50980 [1] NCCL INFO Setting affinity for GPU 1 to 0fffffff,00000000
gpu108-09-l:50445:50978 [0] NCCL INFO Setting affinity for GPU 0 to 0fffffff,00000000
gpu108-16-r:61914:62360 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff
gpu108-16-r:61912:62361 [1] NCCL INFO Setting affinity for GPU 1 to 0fffffff,00000000
gpu108-16-r:61913:62363 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff
gpu108-16-r:61911:62362 [0] NCCL INFO Setting affinity for GPU 0 to 0fffffff,00000000
gpu108-16-r:61911:62362 [0] NCCL INFO Trees [0] 5/-1/-1->4->0 [1] -1/-1/-1->4->5 [2] 5/0/-1->4->-1 [3] -1/-1/-1->4->5
gpu108-16-r:61911:62362 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-16-r:61912:62361 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 4/-1/-1->5->6 [2] 6/-1/-1->5->4 [3] 4/-1/-1->5->6
gpu108-16-r:61912:62361 [1] NCCL INFO P2P Chunksize set to 131072
gpu108-09-l:50445:50978 [0] NCCL INFO Channel 00/04 :    0   3   2   1   4   7   6   5
gpu108-09-l:50445:50978 [0] NCCL INFO Channel 01/04 :    0   3   6   5   4   7   2   1
gpu108-16-r:61913:62363 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 5/-1/-1->6->7 [2] 7/-1/-1->6->5 [3] 5/-1/-1->6->7
gpu108-16-r:61913:62363 [2] NCCL INFO P2P Chunksize set to 131072
gpu108-09-l:50446:50980 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 0/-1/-1->1->2 [2] 2/-1/-1->1->0 [3] 0/-1/-1->1->2
gpu108-09-l:50446:50980 [1] NCCL INFO P2P Chunksize set to 131072
gpu108-16-r:61914:62360 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 6/-1/-1->7->3 [2] -1/-1/-1->7->6 [3] 6/3/-1->7->-1
gpu108-16-r:61914:62360 [3] NCCL INFO P2P Chunksize set to 131072
gpu108-09-l:50445:50978 [0] NCCL INFO Channel 02/04 :    0   3   2   1   4   7   6   5
gpu108-09-l:50445:50978 [0] NCCL INFO Channel 03/04 :    0   3   6   5   4   7   2   1
gpu108-09-l:50445:50978 [0] NCCL INFO Trees [0] 1/4/-1->0->-1 [1] -1/-1/-1->0->1 [2] 1/-1/-1->0->4 [3] -1/-1/-1->0->1
gpu108-09-l:50445:50978 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-09-l:50447:50981 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 1/-1/-1->2->3 [2] 3/-1/-1->2->1 [3] 1/-1/-1->2->3
gpu108-09-l:50447:50981 [2] NCCL INFO P2P Chunksize set to 131072
gpu108-09-l:50448:50979 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 2/7/-1->3->-1 [2] -1/-1/-1->3->2 [3] 2/-1/-1->3->7
gpu108-09-l:50448:50979 [3] NCCL INFO P2P Chunksize set to 131072
gpu108-09-l:50445:50978 [0] NCCL INFO Channel 00/0 : 5[46000] -> 0[7000] [receive] via NET/IB/0/GDRDMA
gpu108-09-l:50446:50980 [1] NCCL INFO Channel 00/0 : 1[46000] -> 4[7000] [send] via NET/IB/0/GDRDMA
gpu108-09-l:50447:50981 [2] NCCL INFO Channel 01/0 : 7[c7000] -> 2[85000] [receive] via NET/IB/1/GDRDMA
gpu108-16-r:61912:62361 [1] NCCL INFO Channel 00/0 : 5[46000] -> 0[7000] [send] via NET/IB/0/GDRDMA
gpu108-16-r:61913:62363 [2] NCCL INFO Channel 01/0 : 3[c7000] -> 6[85000] [receive] via NET/IB/1/GDRDMA
gpu108-16-r:61911:62362 [0] NCCL INFO Channel 00/0 : 1[46000] -> 4[7000] [receive] via NET/IB/0/GDRDMA
gpu108-09-l:50445:50978 [0] NCCL INFO Channel 02/0 : 5[46000] -> 0[7000] [receive] via NET/IB/0/GDRDMA
gpu108-09-l:50446:50980 [1] NCCL INFO Channel 02/0 : 1[46000] -> 4[7000] [send] via NET/IB/0/GDRDMA
gpu108-09-l:50447:50981 [2] NCCL INFO Channel 03/0 : 7[c7000] -> 2[85000] [receive] via NET/IB/1/GDRDMA
gpu108-09-l:50445:50978 [0] NCCL INFO Channel 00/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:61912:62361 [1] NCCL INFO Channel 02/0 : 5[46000] -> 0[7000] [send] via NET/IB/0/GDRDMA
gpu108-16-r:61913:62363 [2] NCCL INFO Channel 03/0 : 3[c7000] -> 6[85000] [receive] via NET/IB/1/GDRDMA
gpu108-16-r:61911:62362 [0] NCCL INFO Channel 02/0 : 1[46000] -> 4[7000] [receive] via NET/IB/0/GDRDMA
gpu108-16-r:61911:62362 [0] NCCL INFO Channel 00/0 : 4[7000] -> 7[c7000] via P2P/IPC/read
gpu108-09-l:50445:50978 [0] NCCL INFO Channel 01/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:61911:62362 [0] NCCL INFO Channel 01/0 : 4[7000] -> 7[c7000] via P2P/IPC/read
gpu108-09-l:50445:50978 [0] NCCL INFO Channel 02/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:61911:62362 [0] NCCL INFO Channel 02/0 : 4[7000] -> 7[c7000] via P2P/IPC/read
gpu108-09-l:50448:50979 [3] NCCL INFO Channel 01/0 : 3[c7000] -> 6[85000] [send] via NET/IB/1/GDRDMA
gpu108-16-r:61914:62360 [3] NCCL INFO Channel 01/0 : 7[c7000] -> 2[85000] [send] via NET/IB/1/GDRDMA
gpu108-09-l:50445:50978 [0] NCCL INFO Channel 03/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:61911:62362 [0] NCCL INFO Channel 03/0 : 4[7000] -> 7[c7000] via P2P/IPC/read
gpu108-09-l:50448:50979 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 6[85000] [send] via NET/IB/1/GDRDMA
gpu108-16-r:61914:62360 [3] NCCL INFO Channel 03/0 : 7[c7000] -> 2[85000] [send] via NET/IB/1/GDRDMA
gpu108-09-l:50448:50979 [3] NCCL INFO Channel 00/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-09-l:50448:50979 [3] NCCL INFO Channel 02/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:61914:62360 [3] NCCL INFO Channel 00/0 : 7[c7000] -> 6[85000] via P2P/IPC/read
gpu108-16-r:61914:62360 [3] NCCL INFO Channel 02/0 : 7[c7000] -> 6[85000] via P2P/IPC/read
gpu108-16-r:61913:62363 [2] NCCL INFO Channel 00/0 : 6[85000] -> 5[46000] via P2P/IPC/read
gpu108-16-r:61912:62361 [1] NCCL INFO Channel 01/0 : 5[46000] -> 4[7000] via P2P/IPC/read
gpu108-09-l:50447:50981 [2] NCCL INFO Channel 00/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:61913:62363 [2] NCCL INFO Channel 01/0 : 6[85000] -> 5[46000] via P2P/IPC/read
gpu108-09-l:50446:50980 [1] NCCL INFO Channel 01/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:61912:62361 [1] NCCL INFO Channel 03/0 : 5[46000] -> 4[7000] via P2P/IPC/read
gpu108-09-l:50447:50981 [2] NCCL INFO Channel 01/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:61913:62363 [2] NCCL INFO Channel 02/0 : 6[85000] -> 5[46000] via P2P/IPC/read
gpu108-09-l:50446:50980 [1] NCCL INFO Channel 03/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-09-l:50447:50981 [2] NCCL INFO Channel 02/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:61913:62363 [2] NCCL INFO Channel 03/0 : 6[85000] -> 5[46000] via P2P/IPC/read
gpu108-09-l:50447:50981 [2] NCCL INFO Channel 03/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:61912:62361 [1] NCCL INFO Connected all rings
gpu108-09-l:50445:50978 [0] NCCL INFO Connected all rings
gpu108-09-l:50445:50978 [0] NCCL INFO Channel 00/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:61914:62360 [3] NCCL INFO Connected all rings
gpu108-09-l:50446:50980 [1] NCCL INFO Connected all rings
gpu108-09-l:50448:50979 [3] NCCL INFO Connected all rings
gpu108-16-r:61911:62362 [0] NCCL INFO Connected all rings
gpu108-16-r:61911:62362 [0] NCCL INFO Channel 00/0 : 4[7000] -> 5[46000] via P2P/IPC/read
gpu108-09-l:50447:50981 [2] NCCL INFO Connected all rings
gpu108-16-r:61913:62363 [2] NCCL INFO Connected all rings
gpu108-09-l:50445:50978 [0] NCCL INFO Channel 01/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:61911:62362 [0] NCCL INFO Channel 01/0 : 4[7000] -> 5[46000] via P2P/IPC/read
gpu108-09-l:50445:50978 [0] NCCL INFO Channel 02/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:61911:62362 [0] NCCL INFO Channel 02/0 : 4[7000] -> 5[46000] via P2P/IPC/read
gpu108-09-l:50445:50978 [0] NCCL INFO Channel 03/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-16-r:61911:62362 [0] NCCL INFO Channel 03/0 : 4[7000] -> 5[46000] via P2P/IPC/read
gpu108-16-r:61912:62361 [1] NCCL INFO Channel 00/0 : 5[46000] -> 6[85000] via P2P/IPC/read
gpu108-09-l:50446:50980 [1] NCCL INFO Channel 00/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-09-l:50447:50981 [2] NCCL INFO Channel 00/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:61913:62363 [2] NCCL INFO Channel 00/0 : 6[85000] -> 7[c7000] via P2P/IPC/read
gpu108-16-r:61912:62361 [1] NCCL INFO Channel 01/0 : 5[46000] -> 6[85000] via P2P/IPC/read
gpu108-09-l:50446:50980 [1] NCCL INFO Channel 01/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-09-l:50447:50981 [2] NCCL INFO Channel 01/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:61913:62363 [2] NCCL INFO Channel 01/0 : 6[85000] -> 7[c7000] via P2P/IPC/read
gpu108-16-r:61912:62361 [1] NCCL INFO Channel 02/0 : 5[46000] -> 6[85000] via P2P/IPC/read
gpu108-09-l:50446:50980 [1] NCCL INFO Channel 02/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-09-l:50447:50981 [2] NCCL INFO Channel 02/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:61913:62363 [2] NCCL INFO Channel 02/0 : 6[85000] -> 7[c7000] via P2P/IPC/read
gpu108-16-r:61912:62361 [1] NCCL INFO Channel 03/0 : 5[46000] -> 6[85000] via P2P/IPC/read
gpu108-09-l:50446:50980 [1] NCCL INFO Channel 03/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-09-l:50447:50981 [2] NCCL INFO Channel 03/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-16-r:61913:62363 [2] NCCL INFO Channel 03/0 : 6[85000] -> 7[c7000] via P2P/IPC/read
gpu108-09-l:50446:50980 [1] NCCL INFO Channel 00/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-16-r:61912:62361 [1] NCCL INFO Channel 00/0 : 5[46000] -> 4[7000] via P2P/IPC/read
gpu108-16-r:61911:62362 [0] NCCL INFO Channel 00/0 : 0[7000] -> 4[7000] [receive] via NET/IB/0/GDRDMA
gpu108-09-l:50445:50978 [0] NCCL INFO Channel 00/0 : 4[7000] -> 0[7000] [receive] via NET/IB/0/GDRDMA
gpu108-09-l:50446:50980 [1] NCCL INFO Channel 02/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-09-l:50448:50979 [3] NCCL INFO Channel 01/0 : 7[c7000] -> 3[c7000] [receive] via NET/IB/1/GDRDMA
gpu108-16-r:61912:62361 [1] NCCL INFO Channel 02/0 : 5[46000] -> 4[7000] via P2P/IPC/read
gpu108-16-r:61914:62360 [3] NCCL INFO Channel 01/0 : 3[c7000] -> 7[c7000] [receive] via NET/IB/1/GDRDMA
gpu108-16-r:61911:62362 [0] NCCL INFO Channel 02/0 : 0[7000] -> 4[7000] [receive] via NET/IB/0/GDRDMA
gpu108-09-l:50445:50978 [0] NCCL INFO Channel 02/0 : 4[7000] -> 0[7000] [receive] via NET/IB/0/GDRDMA
gpu108-09-l:50448:50979 [3] NCCL INFO Channel 03/0 : 7[c7000] -> 3[c7000] [receive] via NET/IB/1/GDRDMA
gpu108-16-r:61914:62360 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 7[c7000] [receive] via NET/IB/1/GDRDMA
gpu108-16-r:61911:62362 [0] NCCL INFO Channel 00/0 : 4[7000] -> 0[7000] [send] via NET/IB/0/GDRDMA
gpu108-09-l:50445:50978 [0] NCCL INFO Channel 00/0 : 0[7000] -> 4[7000] [send] via NET/IB/0/GDRDMA
gpu108-09-l:50448:50979 [3] NCCL INFO Channel 01/0 : 3[c7000] -> 7[c7000] [send] via NET/IB/1/GDRDMA
gpu108-16-r:61914:62360 [3] NCCL INFO Channel 01/0 : 7[c7000] -> 3[c7000] [send] via NET/IB/1/GDRDMA
gpu108-16-r:61911:62362 [0] NCCL INFO Channel 02/0 : 4[7000] -> 0[7000] [send] via NET/IB/0/GDRDMA
gpu108-09-l:50445:50978 [0] NCCL INFO Channel 02/0 : 0[7000] -> 4[7000] [send] via NET/IB/0/GDRDMA
gpu108-09-l:50448:50979 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 7[c7000] [send] via NET/IB/1/GDRDMA
gpu108-16-r:61914:62360 [3] NCCL INFO Channel 03/0 : 7[c7000] -> 3[c7000] [send] via NET/IB/1/GDRDMA
gpu108-16-r:61914:62360 [3] NCCL INFO Channel 01/0 : 7[c7000] -> 6[85000] via P2P/IPC/read
gpu108-09-l:50448:50979 [3] NCCL INFO Channel 01/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-09-l:50448:50979 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-16-r:61914:62360 [3] NCCL INFO Channel 03/0 : 7[c7000] -> 6[85000] via P2P/IPC/read
gpu108-09-l:50446:50980 [1] NCCL INFO Connected all trees
gpu108-09-l:50446:50980 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-09-l:50446:50980 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-09-l:50447:50981 [2] NCCL INFO Connected all trees
gpu108-09-l:50447:50981 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-09-l:50447:50981 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-16-r:61912:62361 [1] NCCL INFO Connected all trees
gpu108-16-r:61912:62361 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-16-r:61912:62361 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-16-r:61913:62363 [2] NCCL INFO Connected all trees
gpu108-16-r:61913:62363 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-16-r:61913:62363 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-09-l:50448:50979 [3] NCCL INFO Connected all trees
gpu108-09-l:50448:50979 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-09-l:50448:50979 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-09-l:50445:50978 [0] NCCL INFO Connected all trees
gpu108-09-l:50445:50978 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-09-l:50445:50978 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-16-r:61914:62360 [3] NCCL INFO Connected all trees
gpu108-16-r:61914:62360 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-16-r:61914:62360 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-16-r:61911:62362 [0] NCCL INFO Connected all trees
gpu108-16-r:61911:62362 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-16-r:61911:62362 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-09-l:50445:50978 [0] NCCL INFO comm 0x35e71150 rank 0 nranks 8 cudaDev 0 busId 7000 commId 0x77d1a108db1277ad - Init COMPLETE
gpu108-16-r:61911:62362 [0] NCCL INFO comm 0x399bd920 rank 4 nranks 8 cudaDev 0 busId 7000 commId 0x77d1a108db1277ad - Init COMPLETE
gpu108-09-l:50446:50980 [1] NCCL INFO comm 0x3a83f200 rank 1 nranks 8 cudaDev 1 busId 46000 commId 0x77d1a108db1277ad - Init COMPLETE
gpu108-09-l:50448:50979 [3] NCCL INFO comm 0x39d3f6f0 rank 3 nranks 8 cudaDev 3 busId c7000 commId 0x77d1a108db1277ad - Init COMPLETE
gpu108-16-r:61913:62363 [2] NCCL INFO comm 0x3be94640 rank 6 nranks 8 cudaDev 2 busId 85000 commId 0x77d1a108db1277ad - Init COMPLETE
gpu108-09-l:50447:50981 [2] NCCL INFO comm 0x39ddd130 rank 2 nranks 8 cudaDev 2 busId 85000 commId 0x77d1a108db1277ad - Init COMPLETE
gpu108-16-r:61912:62361 [1] NCCL INFO comm 0x3b88bfc0 rank 5 nranks 8 cudaDev 1 busId 46000 commId 0x77d1a108db1277ad - Init COMPLETE
gpu108-16-r:61914:62360 [3] NCCL INFO comm 0x3b9b69a0 rank 7 nranks 8 cudaDev 3 busId c7000 commId 0x77d1a108db1277ad - Init COMPLETE
[2023-03-17 14:29:41,258] [INFO] [partition_parameters.py:415:__exit__] finished initializing model with 2.44B parameters
[2023-03-17 14:29:41,291] [INFO] [utils.py:829:see_memory_usage] After Building Model
[2023-03-17 14:29:41,291] [INFO] [utils.py:830:see_memory_usage] MA 0.57 GB         Max_MA 1.69 GB         CA 5.13 GB         Max_CA 5 GB 
[2023-03-17 14:29:41,291] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 49.61 GB, percent = 9.8%
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2442842112
> learning rate decay style: cosine
DeepSpeed is enabled.
[2023-03-17 14:29:41,378] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed info: version=0.8.3+bbfd0a6, git-hash=bbfd0a6, git-branch=master
[2023-03-17 14:29:41,381] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-03-17 14:29:41,382] [INFO] [logging.py:93:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-03-17 14:29:41,382] [INFO] [logging.py:93:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-03-17 14:29:41,382] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2023-03-17 14:29:41,382] [INFO] [utils.py:55:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'apex.optimizers.fused_adam.FusedAdam'>
[2023-03-17 14:29:41,382] [INFO] [logging.py:93:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[2023-03-17 14:29:41,418] [INFO] [utils.py:829:see_memory_usage] Stage 3 initialize beginning
[2023-03-17 14:29:41,418] [INFO] [utils.py:830:see_memory_usage] MA 0.57 GB         Max_MA 0.57 GB         CA 5.13 GB         Max_CA 5 GB 
[2023-03-17 14:29:41,419] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 49.61 GB, percent = 9.8%
[2023-03-17 14:29:41,419] [INFO] [stage3.py:113:__init__] Reduce bucket size 90000000
[2023-03-17 14:29:41,419] [INFO] [stage3.py:114:__init__] Prefetch bucket size 50000000
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Emitting ninja build file /home/shaima0d/.cache/torch_extensions/py39_cu117/utils/build.ninja...
Building extension module utils...
Using envvar MAX_JOBS (15) as the number of workers...
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.2025890350341797 seconds
Loading extension module utils...
Loading extension module utils...
Time to load utils op: 0.21056866645812988 seconds
Time to load utils op: 0.21149063110351562 seconds
Loading extension module utils...
Time to load utils op: 0.20598101615905762 seconds
[2023-03-17 14:29:41,662] [INFO] [utils.py:829:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2023-03-17 14:29:41,663] [INFO] [utils.py:830:see_memory_usage] MA 0.57 GB         Max_MA 0.57 GB         CA 5.13 GB         Max_CA 5 GB 
[2023-03-17 14:29:41,663] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 49.6 GB, percent = 9.8%
Parameter Offload: Total persistent parameters: 184320 in 10 params
[2023-03-17 14:29:41,690] [INFO] [utils.py:829:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2023-03-17 14:29:41,691] [INFO] [utils.py:830:see_memory_usage] MA 0.57 GB         Max_MA 0.57 GB         CA 5.13 GB         Max_CA 5 GB 
[2023-03-17 14:29:41,691] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 49.6 GB, percent = 9.8%
[2023-03-17 14:29:41,717] [INFO] [utils.py:829:see_memory_usage] Before creating fp16 partitions
[2023-03-17 14:29:41,717] [INFO] [utils.py:830:see_memory_usage] MA 0.57 GB         Max_MA 0.57 GB         CA 5.13 GB         Max_CA 5 GB 
[2023-03-17 14:29:41,718] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 49.6 GB, percent = 9.8%
Loading extension module utils...
Loading extension module utils...
Loading extension module utils...
Loading extension module utils...
Time to load utils op: 3.0169296264648438 seconds
Time to load utils op: 3.016983985900879 seconds
Time to load utils op: 3.0169742107391357 seconds
Time to load utils op: 3.0170254707336426 seconds
[2023-03-17 14:29:44,995] [INFO] [utils.py:829:see_memory_usage] After creating fp16 partitions: 2
[2023-03-17 14:29:44,996] [INFO] [utils.py:830:see_memory_usage] MA 0.57 GB         Max_MA 0.57 GB         CA 0.57 GB         Max_CA 5 GB 
[2023-03-17 14:29:44,996] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 49.6 GB, percent = 9.8%
[2023-03-17 14:29:45,021] [INFO] [utils.py:829:see_memory_usage] Before creating fp32 partitions
[2023-03-17 14:29:45,022] [INFO] [utils.py:830:see_memory_usage] MA 0.57 GB         Max_MA 0.57 GB         CA 0.57 GB         Max_CA 1 GB 
[2023-03-17 14:29:45,022] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 49.6 GB, percent = 9.8%
[2023-03-17 14:29:45,051] [INFO] [utils.py:829:see_memory_usage] After creating fp32 partitions
[2023-03-17 14:29:45,052] [INFO] [utils.py:830:see_memory_usage] MA 1.71 GB         Max_MA 2.27 GB         CA 2.28 GB         Max_CA 2 GB 
[2023-03-17 14:29:45,052] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 49.6 GB, percent = 9.8%
[2023-03-17 14:29:45,097] [INFO] [utils.py:829:see_memory_usage] Before initializing optimizer states
[2023-03-17 14:29:45,098] [INFO] [utils.py:830:see_memory_usage] MA 1.71 GB         Max_MA 1.71 GB         CA 2.28 GB         Max_CA 2 GB 
[2023-03-17 14:29:45,098] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 49.6 GB, percent = 9.8%
[2023-03-17 14:29:45,108] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | init_optimizer_state: 8.12
[2023-03-17 14:29:45,133] [INFO] [utils.py:829:see_memory_usage] After initializing optimizer states
[2023-03-17 14:29:45,133] [INFO] [utils.py:830:see_memory_usage] MA 3.98 GB         Max_MA 5.12 GB         CA 5.7 GB         Max_CA 6 GB 
[2023-03-17 14:29:45,134] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 49.6 GB, percent = 9.8%
[2023-03-17 14:29:45,134] [INFO] [stage3.py:376:_setup_for_real_optimizer] optimizer state initialized
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Time to load utils op: 0.0003895759582519531 seconds
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
Time to load utils op: 0.0003592967987060547 seconds
Loading extension module utils...
Loading extension module utils...
Time to load utils op: 0.0003364086151123047 seconds
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Time to load utils op: 0.0004055500030517578 seconds
No modifications detected for re-loaded extension module utils, skipping build step...
Time to load utils op: 0.00037217140197753906 seconds
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
Time to load utils op: 0.0003268718719482422 seconds
No modifications detected for re-loaded extension module utils, skipping build step...
Time to load utils op: 0.0003333091735839844 seconds
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Loading extension module utils...
[2023-03-17 14:29:45,203] [INFO] [utils.py:829:see_memory_usage] After initializing ZeRO optimizer
[2023-03-17 14:29:45,204] [INFO] [utils.py:830:see_memory_usage] MA 4.72 GB         Max_MA 7.02 GB         CA 16.47 GB         Max_CA 16 GB 
[2023-03-17 14:29:45,204] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 49.6 GB, percent = 9.8%
[2023-03-17 14:29:45,204] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[2023-03-17 14:29:45,204] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-03-17 14:29:45,205] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x2b864bfda4f0>
[2023-03-17 14:29:45,205] [INFO] [logging.py:93:log_dist] [Rank 0] step=0, skipped=0, lr=[5.9999999999999995e-05, 5.9999999999999995e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:29:45,205] [INFO] [config.py:1018:print] DeepSpeedEngine configuration:
[2023-03-17 14:29:45,205] [INFO] [config.py:1022:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-03-17 14:29:45,205] [INFO] [config.py:1022:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 16, 'thread_count': 2, 'single_submit': False, 'overlap_events': True}
[2023-03-17 14:29:45,205] [INFO] [config.py:1022:print]   amp_enabled .................. False
[2023-03-17 14:29:45,205] [INFO] [config.py:1022:print]   amp_params ................... False
[2023-03-17 14:29:45,205] [INFO] [config.py:1022:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-03-17 14:29:45,205] [INFO] [config.py:1022:print]   bfloat16_enabled ............. False
[2023-03-17 14:29:45,205] [INFO] [config.py:1022:print]   checkpoint_parallel_write_pipeline  False
[2023-03-17 14:29:45,205] [INFO] [config.py:1022:print]   checkpoint_tag_validation_enabled  True
[2023-03-17 14:29:45,205] [INFO] [config.py:1022:print]   checkpoint_tag_validation_fail  False
[2023-03-17 14:29:45,205] [INFO] [config.py:1022:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x2b864bfdaa60>
[2023-03-17 14:29:45,205] [INFO] [config.py:1022:print]   communication_data_type ...... None
[2023-03-17 14:29:45,205] [INFO] [config.py:1022:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-03-17 14:29:45,205] [INFO] [config.py:1022:print]   curriculum_enabled_legacy .... False
[2023-03-17 14:29:45,205] [INFO] [config.py:1022:print]   curriculum_params_legacy ..... False
[2023-03-17 14:29:45,205] [INFO] [config.py:1022:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-03-17 14:29:45,205] [INFO] [config.py:1022:print]   data_efficiency_enabled ...... False
[2023-03-17 14:29:45,205] [INFO] [config.py:1022:print]   dataloader_drop_last ......... False
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   disable_allgather ............ False
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   dump_state ................... False
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   dynamic_loss_scale_args ...... {'init_scale': 32768, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   eigenvalue_enabled ........... False
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   eigenvalue_gas_boundary_resolution  1
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   eigenvalue_layer_num ......... 0
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   eigenvalue_max_iter .......... 100
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   eigenvalue_stability ......... 1e-06
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   eigenvalue_tol ............... 0.01
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   eigenvalue_verbose ........... False
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   elasticity_enabled ........... False
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   fp16_auto_cast ............... False
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   fp16_enabled ................. True
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   fp16_master_weights_and_gradients  False
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   global_rank .................. 0
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   grad_accum_dtype ............. None
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   gradient_accumulation_steps .. 1
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   gradient_clipping ............ 1
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   gradient_predivide_factor .... 1.0
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   initial_dynamic_scale ........ 32768
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   load_universal_checkpoint .... False
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   loss_scale ................... 0
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   memory_breakdown ............. False
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   optimizer_legacy_fusion ...... False
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   optimizer_name ............... None
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   optimizer_params ............. None
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   pld_enabled .................. False
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   pld_params ................... False
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   prescale_gradients ........... False
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   scheduler_name ............... None
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   scheduler_params ............. None
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   sparse_attention ............. None
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   sparse_gradients_enabled ..... False
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   steps_per_print .............. 1
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   train_batch_size ............. 32
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   train_micro_batch_size_per_gpu  4
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   use_node_local_storage ....... False
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   wall_clock_breakdown ......... True
[2023-03-17 14:29:45,206] [INFO] [config.py:1022:print]   world_size ................... 8
[2023-03-17 14:29:45,207] [INFO] [config.py:1022:print]   zero_allow_untested_optimizer  False
[2023-03-17 14:29:45,207] [INFO] [config.py:1022:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=90000000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=sys.maxsize max_live_parameters=3000000000 max_reuse_distance=3000000000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2023-03-17 14:29:45,207] [INFO] [config.py:1022:print]   zero_enabled ................. True
[2023-03-17 14:29:45,207] [INFO] [config.py:1022:print]   zero_force_ds_cpu_optimizer .. True
[2023-03-17 14:29:45,207] [INFO] [config.py:1022:print]   zero_optimization_stage ...... 3
[2023-03-17 14:29:45,207] [INFO] [config.py:1007:print_user_config]   json = {
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 4, 
    "steps_per_print": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 3, 
        "stage3_max_live_parameters": 3.000000e+09, 
        "stage3_max_reuse_distance": 3.000000e+09, 
        "stage3_param_persistence_threshold": 1.000000e+05, 
        "stage3_prefetch_bucket_size": 5.000000e+07, 
        "contiguous_gradients": true, 
        "overlap_comm": true, 
        "reduce_bucket_size": 9.000000e+07, 
        "sub_group_size": 1.000000e+09, 
        "offload_optimizer": {
            "device": "none", 
            "buffer_count": 4, 
            "pipeline_read": false, 
            "pipeline_write": false, 
            "pin_memory": true
        }
    }, 
    "gradient_clipping": 1, 
    "fp16": {
        "enabled": true, 
        "initial_scale_power": 15, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "wall_clock_breakdown": true, 
    "zero_allow_untested_optimizer": false, 
    "aio": {
        "block_size": 1.048576e+06, 
        "queue_depth": 16, 
        "single_submit": false, 
        "overlap_events": true, 
        "thread_count": 2
    }
}
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00030303001403808594 seconds
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-03-17 14:29:45 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      1600
    validation: 1280
    test:       1280
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.001064 seconds
    number of documents: 17868
 > dataset split:
    train:
     document indices in [0, 17511) total of 17511 documents
    validation:
     document indices in [17511, 17868) total of 357 documents
    test:
     document indices in [17868, 17868) total of 0 documents
NCCL version 2.17.1+cuda11.7
NCCL version 2.17.1+cuda11.7
NCCL version 2.17.1+cuda11.7
NCCL version 2.17.1+cuda11.7
NCCL version 2.17.1+cuda11.7
gpu108-09-l:50448:51004 [3] NCCL INFO Using network IB
gpu108-16-r:61911:62382 [0] NCCL INFO Using network IB
gpu108-09-l:50446:51005 [1] NCCL INFO Using network IB
gpu108-09-l:50445:51007 [0] NCCL INFO Using network IB
gpu108-16-r:61914:62384 [3] NCCL INFO Using network IB
gpu108-16-r:61912:62383 [1] NCCL INFO Using network IB
NCCL version 2.17.1+cuda11.7
NCCL version 2.17.1+cuda11.7
gpu108-16-r:61913:62386 [2] NCCL INFO Using network IB
gpu108-09-l:50447:51009 [2] NCCL INFO Using network IB
gpu108-16-r:61911:62382 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-09-l:50446:51005 [1] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:61911:62382 [0] NCCL INFO Setting affinity for GPU 0 to 0fffffff,00000000
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 00/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 01/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 02/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 03/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 04/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 05/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 06/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 07/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 08/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 09/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 10/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 11/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 12/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 13/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 14/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 15/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 16/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 17/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 18/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 19/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 20/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 21/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 22/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 23/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 24/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 25/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 26/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 27/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 28/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 29/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 30/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Channel 31/32 :    0
gpu108-16-r:61911:62382 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-16-r:61911:62382 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-16-r:61911:62382 [0] NCCL INFO Connected all rings
gpu108-16-r:61911:62382 [0] NCCL INFO Connected all trees
gpu108-16-r:61911:62382 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-09-l:50446:51005 [1] NCCL INFO Setting affinity for GPU 1 to 0fffffff,00000000
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 00/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 01/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 02/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 03/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 04/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 05/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 06/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 07/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 08/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 09/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 10/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 11/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 12/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 13/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 14/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 15/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 16/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 17/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 18/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 19/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 20/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 21/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 22/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 23/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 24/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 25/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 26/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 27/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 28/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 29/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 30/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Channel 31/32 :    0
gpu108-09-l:50446:51005 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-09-l:50446:51005 [1] NCCL INFO P2P Chunksize set to 131072
gpu108-09-l:50445:51007 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-09-l:50446:51005 [1] NCCL INFO Connected all rings
gpu108-09-l:50446:51005 [1] NCCL INFO Connected all trees
gpu108-09-l:50446:51005 [1] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-16-r:61912:62383 [1] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-09-l:50448:51004 [3] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-09-l:50445:51007 [0] NCCL INFO Setting affinity for GPU 0 to 0fffffff,00000000
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 00/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 01/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 02/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 03/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 04/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 05/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 06/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 07/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 08/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 09/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 10/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 11/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 12/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 13/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 14/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 15/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 16/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 17/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 18/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 19/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 20/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 21/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 22/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 23/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 24/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 25/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 26/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 27/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 28/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 29/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 30/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Channel 31/32 :    0
gpu108-09-l:50445:51007 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-09-l:50445:51007 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-16-r:61914:62384 [3] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:61912:62383 [1] NCCL INFO Setting affinity for GPU 1 to 0fffffff,00000000
gpu108-09-l:50445:51007 [0] NCCL INFO Connected all rings
gpu108-09-l:50445:51007 [0] NCCL INFO Connected all trees
gpu108-09-l:50445:51007 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 00/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 01/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 02/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 03/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 04/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 05/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 06/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 07/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 08/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 09/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 10/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 11/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 12/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 13/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 14/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 15/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 16/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 17/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 18/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 19/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 20/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 21/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 22/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 23/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 24/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 25/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 26/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 27/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 28/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 29/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 30/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Channel 31/32 :    0
gpu108-16-r:61912:62383 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-16-r:61912:62383 [1] NCCL INFO P2P Chunksize set to 131072
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 00/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 01/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 02/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 03/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 04/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 05/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 06/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 07/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 08/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 09/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 10/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 11/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 12/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 13/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 14/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 15/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 16/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 17/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 18/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 19/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 20/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 21/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 22/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 23/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 24/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 25/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 26/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 27/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 28/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 29/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 30/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Channel 31/32 :    0
gpu108-09-l:50448:51004 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-09-l:50448:51004 [3] NCCL INFO P2P Chunksize set to 131072
gpu108-16-r:61912:62383 [1] NCCL INFO Connected all rings
gpu108-16-r:61912:62383 [1] NCCL INFO Connected all trees
gpu108-16-r:61912:62383 [1] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-16-r:61914:62384 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 00/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 01/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 02/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 03/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 04/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 05/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 06/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 07/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 08/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 09/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 10/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 11/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 12/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 13/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 14/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 15/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 16/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 17/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 18/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 19/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 20/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 21/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 22/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 23/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 24/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 25/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 26/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 27/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 28/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 29/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 30/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Channel 31/32 :    0
gpu108-16-r:61914:62384 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-16-r:61914:62384 [3] NCCL INFO P2P Chunksize set to 131072
gpu108-09-l:50448:51004 [3] NCCL INFO Connected all rings
gpu108-09-l:50448:51004 [3] NCCL INFO Connected all trees
gpu108-09-l:50448:51004 [3] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-16-r:61914:62384 [3] NCCL INFO Connected all rings
gpu108-16-r:61914:62384 [3] NCCL INFO Connected all trees
gpu108-16-r:61914:62384 [3] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-09-l:50447:51009 [2] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:61913:62386 [2] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-09-l:50447:51009 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 00/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 01/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 02/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 03/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 04/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 05/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 06/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 07/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 08/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 09/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 10/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 11/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 12/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 13/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 14/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 15/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 16/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 17/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 18/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 19/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 20/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 21/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 22/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 23/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 24/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 25/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 26/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 27/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 28/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 29/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 30/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Channel 31/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-09-l:50447:51009 [2] NCCL INFO P2P Chunksize set to 131072
gpu108-16-r:61913:62386 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 00/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 01/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 02/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 03/32 :    0
gpu108-09-l:50447:51009 [2] NCCL INFO Connected all rings
gpu108-09-l:50447:51009 [2] NCCL INFO Connected all trees
gpu108-09-l:50447:51009 [2] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 04/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 05/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 06/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 07/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 08/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 09/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 10/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 11/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 12/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 13/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 14/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 15/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 16/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 17/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 18/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 19/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 20/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 21/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 22/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 23/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 24/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 25/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 26/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 27/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 28/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 29/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 30/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Channel 31/32 :    0
gpu108-16-r:61913:62386 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-16-r:61913:62386 [2] NCCL INFO P2P Chunksize set to 131072
gpu108-16-r:61913:62386 [2] NCCL INFO Connected all rings
gpu108-16-r:61913:62386 [2] NCCL INFO Connected all trees
gpu108-16-r:61913:62386 [2] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-09-l:50446:51005 [1] NCCL INFO comm 0x3a887b20 rank 0 nranks 1 cudaDev 1 busId 46000 commId 0x6457a262bd45f86d - Init COMPLETE
gpu108-16-r:61912:62383 [1] NCCL INFO comm 0x3b8d48f0 rank 0 nranks 1 cudaDev 1 busId 46000 commId 0x5bb83b4804b1065d - Init COMPLETE
gpu108-09-l:50445:51007 [0] NCCL INFO comm 0x35e9b5a0 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0x7daedf90e0739c6e - Init COMPLETE
gpu108-16-r:61911:62382 [0] NCCL INFO comm 0x39a05bc0 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0x7fc8c8529b1b0c00 - Init COMPLETE
 > loading doc-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_train_indexmap_1600ns_1024sl_1234s_doc_idx.npy
gpu108-16-r:61914:62384 [3] NCCL INFO comm 0x3ba02a80 rank 0 nranks 1 cudaDev 3 busId c7000 commId 0x30540b8363451210 - Init COMPLETE
 > loading sample-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_train_indexmap_1600ns_1024sl_1234s_sample_idx.npy
gpu108-09-l:50448:51004 [3] NCCL INFO comm 0x39d8aef0 rank 0 nranks 1 cudaDev 3 busId c7000 commId 0x88a6fd2a8ea8efeb - Init COMPLETE
 > loading shuffle-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_train_indexmap_1600ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.001 seconds
    total number of samples: 1544006
    total number of epochs: 1
gpu108-09-l:50447:51009 [2] NCCL INFO comm 0x39e25180 rank 0 nranks 1 cudaDev 2 busId 85000 commId 0x34d3769d0501baa3 - Init COMPLETE
gpu108-16-r:61913:62386 [2] NCCL INFO comm 0x3bee04e0 rank 0 nranks 1 cudaDev 2 busId 85000 commId 0xc09a024915252dba - Init COMPLETE
 > loading doc-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_valid_indexmap_1280ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_valid_indexmap_1280ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_valid_indexmap_1280ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.001 seconds
    total number of samples: 31426
    total number of epochs: 1
> finished creating GPT datasets ...
gpu108-09-l:50447:51023 [2] NCCL INFO Using network IB
gpu108-09-l:50448:51026 [3] NCCL INFO Using network IB
gpu108-16-r:61913:62399 [2] NCCL INFO Using network IB
gpu108-16-r:61912:62402 [1] NCCL INFO Using network IB
gpu108-09-l:50446:51025 [1] NCCL INFO Using network IB
gpu108-16-r:61914:62400 [3] NCCL INFO Using network IB
gpu108-09-l:50445:51024 [0] NCCL INFO Using network IB
gpu108-16-r:61911:62401 [0] NCCL INFO Using network IB
gpu108-16-r:61913:62399 [2] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:61913:62399 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 00/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 01/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 02/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 03/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 04/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 05/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 06/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 07/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 08/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 09/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 10/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 11/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 12/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 13/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 14/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 15/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 16/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 17/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 18/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 19/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 20/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 21/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 22/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 23/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 24/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 25/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 26/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 27/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 28/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 29/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 30/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Channel 31/32 :    0
gpu108-16-r:61913:62399 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-16-r:61913:62399 [2] NCCL INFO P2P Chunksize set to 131072
gpu108-09-l:50447:51023 [2] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:61913:62399 [2] NCCL INFO Connected all rings
gpu108-16-r:61913:62399 [2] NCCL INFO Connected all trees
gpu108-16-r:61913:62399 [2] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-09-l:50447:51023 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 00/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 01/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 02/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 03/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 04/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 05/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 06/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 07/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 08/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 09/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 10/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 11/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 12/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 13/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 14/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 15/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 16/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 17/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 18/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 19/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 20/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 21/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 22/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 23/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 24/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 25/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 26/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 27/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 28/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 29/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 30/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Channel 31/32 :    0
gpu108-09-l:50447:51023 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-09-l:50447:51023 [2] NCCL INFO P2P Chunksize set to 131072
gpu108-16-r:61914:62400 [3] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-09-l:50447:51023 [2] NCCL INFO Connected all rings
gpu108-09-l:50447:51023 [2] NCCL INFO Connected all trees
gpu108-09-l:50447:51023 [2] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-16-r:61912:62402 [1] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:61914:62400 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 00/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 01/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 02/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 03/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 04/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 05/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 06/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 07/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 08/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 09/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 10/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 11/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 12/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 13/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 14/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 15/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 16/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 17/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 18/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 19/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 20/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 21/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 22/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 23/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 24/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 25/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 26/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 27/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 28/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 29/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 30/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Channel 31/32 :    0
gpu108-16-r:61914:62400 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-16-r:61914:62400 [3] NCCL INFO P2P Chunksize set to 131072
gpu108-09-l:50448:51026 [3] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:61912:62402 [1] NCCL INFO Setting affinity for GPU 1 to 0fffffff,00000000
gpu108-16-r:61911:62401 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 00/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 01/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 02/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 03/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 04/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 05/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 06/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 07/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 08/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 09/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 10/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 11/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 12/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 13/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 14/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 15/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 16/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 17/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 18/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 19/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 20/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 21/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 22/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 23/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 24/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 25/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 26/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 27/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 28/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 29/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 30/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Channel 31/32 :    0
gpu108-16-r:61912:62402 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-16-r:61912:62402 [1] NCCL INFO P2P Chunksize set to 131072
gpu108-16-r:61914:62400 [3] NCCL INFO Connected all rings
gpu108-16-r:61914:62400 [3] NCCL INFO Connected all trees
gpu108-16-r:61914:62400 [3] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-09-l:50446:51025 [1] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:61912:62402 [1] NCCL INFO Connected all rings
gpu108-16-r:61912:62402 [1] NCCL INFO Connected all trees
gpu108-16-r:61912:62402 [1] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-09-l:50445:51024 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:61911:62401 [0] NCCL INFO Setting affinity for GPU 0 to 0fffffff,00000000
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 00/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 01/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 02/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 03/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 04/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 05/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 06/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 07/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 08/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 09/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 10/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 11/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 12/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 13/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 14/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 15/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 16/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 17/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 18/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 19/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 20/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 21/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 22/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 23/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 24/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 25/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 26/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 27/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 28/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 29/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 30/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Channel 31/32 :    0
gpu108-16-r:61911:62401 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-16-r:61911:62401 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-09-l:50446:51025 [1] NCCL INFO Setting affinity for GPU 1 to 0fffffff,00000000
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 00/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 01/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 02/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 03/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 04/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 05/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 06/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 07/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 08/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 09/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 10/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 11/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 12/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 13/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 14/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 15/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 16/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 17/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 18/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 19/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 20/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 21/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 22/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 23/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 24/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 25/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 26/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 27/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 28/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 29/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 30/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Channel 31/32 :    0
gpu108-09-l:50448:51026 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-09-l:50448:51026 [3] NCCL INFO P2P Chunksize set to 131072
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 00/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 01/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 02/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 03/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 04/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 05/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 06/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 07/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 08/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 09/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 10/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 11/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 12/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 13/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 14/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 15/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 16/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 17/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 18/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 19/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 20/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 21/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 22/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 23/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 24/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 25/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 26/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 27/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 28/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 29/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 30/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Channel 31/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-09-l:50446:51025 [1] NCCL INFO P2P Chunksize set to 131072
gpu108-16-r:61911:62401 [0] NCCL INFO Connected all rings
gpu108-16-r:61911:62401 [0] NCCL INFO Connected all trees
gpu108-16-r:61911:62401 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-09-l:50445:51024 [0] NCCL INFO Setting affinity for GPU 0 to 0fffffff,00000000
gpu108-09-l:50448:51026 [3] NCCL INFO Connected all rings
gpu108-09-l:50448:51026 [3] NCCL INFO Connected all trees
gpu108-09-l:50448:51026 [3] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 00/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 01/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 02/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 03/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 04/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 05/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 06/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 07/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 08/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 09/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 10/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 11/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 12/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 13/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 14/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 15/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 16/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 17/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 18/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 19/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 20/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 21/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 22/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 23/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 24/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 25/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 26/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 27/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 28/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 29/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 30/32 :    0
gpu108-09-l:50445:51024 [0] NCCL INFO Channel 31/32 :    0
gpu108-09-l:50446:51025 [1] NCCL INFO Connected all rings
gpu108-09-l:50446:51025 [1] NCCL INFO Connected all trees
gpu108-09-l:50446:51025 [1] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-09-l:50445:51024 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-09-l:50445:51024 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-09-l:50445:51024 [0] NCCL INFO Connected all rings
gpu108-09-l:50445:51024 [0] NCCL INFO Connected all trees
gpu108-09-l:50445:51024 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-09-l:50447:51023 [2] NCCL INFO comm 0x39e12e80 rank 0 nranks 1 cudaDev 2 busId 85000 commId 0x86a6342b5ae190e - Init COMPLETE
gpu108-16-r:61912:62402 [1] NCCL INFO comm 0x3b8c25f0 rank 0 nranks 1 cudaDev 1 busId 46000 commId 0x7b332cc4dd7fe0be - Init COMPLETE
gpu108-09-l:50446:51025 [1] NCCL INFO comm 0x3a875820 rank 0 nranks 1 cudaDev 1 busId 46000 commId 0xff6cc926afde9e2 - Init COMPLETE
gpu108-09-l:50448:51026 [3] NCCL INFO comm 0x39d78bf0 rank 0 nranks 1 cudaDev 3 busId c7000 commId 0xb69ecdc1bd3472d0 - Init COMPLETE
gpu108-16-r:61911:62401 [0] NCCL INFO comm 0x399f38c0 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0x4b711073b3cedc25 - Init COMPLETE
gpu108-16-r:61914:62400 [3] NCCL INFO comm 0x3b9f0780 rank 0 nranks 1 cudaDev 3 busId c7000 commId 0xbf33fc366602bea5 - Init COMPLETE
gpu108-09-l:50445:51024 [0] NCCL INFO comm 0x35e8a3f0 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0x47e6f3b10f3084a4 - Init COMPLETE
gpu108-16-r:61913:62399 [2] NCCL INFO comm 0x3bece1e0 rank 0 nranks 1 cudaDev 2 busId 85000 commId 0x7d287266115d1602 - Init COMPLETE
time (ms) | model-and-optimizer-setup: 5849.30 | train/valid/test-data-iterators-setup: 506.61
[after dataloaders are built] datetime: 2023-03-17 14:29:45 
done with setup ...
training ...
[before the start of training step] datetime: 2023-03-17 14:29:45 
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
[2023-03-17 14:29:45,896] [INFO] [checkpointing.py:553:forward] Activation Checkpointing Information
[2023-03-17 14:29:45,896] [INFO] [checkpointing.py:554:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-03-17 14:29:45,896] [INFO] [checkpointing.py:557:forward] ----contiguous Memory Checkpointing False with 1 total layers
[2023-03-17 14:29:45,896] [INFO] [checkpointing.py:560:forward] ----Synchronization False
[2023-03-17 14:29:45,896] [INFO] [checkpointing.py:561:forward] ----Profiling time in checkpointing False
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
gpu108-09-l:50447:51888 [2] NCCL INFO Using network IB
gpu108-09-l:50446:51890 [1] NCCL INFO Using network IB
gpu108-09-l:50448:51892 [3] NCCL INFO Using network IB
gpu108-16-r:61912:63269 [1] NCCL INFO Using network IB
gpu108-16-r:61914:63268 [3] NCCL INFO Using network IB
gpu108-09-l:50445:51893 [0] NCCL INFO Using network IB
gpu108-16-r:61913:63267 [2] NCCL INFO Using network IB
gpu108-16-r:61911:63271 [0] NCCL INFO Using network IB
gpu108-16-r:61912:63269 [1] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:61913:63267 [2] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:61914:63268 [3] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:61912:63269 [1] NCCL INFO Setting affinity for GPU 1 to 0fffffff,00000000
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 00/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 01/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 02/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 03/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 04/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 05/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 06/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 07/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 08/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 09/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 10/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 11/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 12/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 13/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 14/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 15/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 16/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 17/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 18/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 19/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 20/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 21/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 22/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 23/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 24/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 25/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 26/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 27/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 28/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 29/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 30/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Channel 31/32 :    0
gpu108-16-r:61912:63269 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-16-r:61912:63269 [1] NCCL INFO P2P Chunksize set to 131072
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 00/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 01/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 02/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 03/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 04/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 05/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 06/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 07/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 08/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 09/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 10/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 11/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 12/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 13/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 14/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 15/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 16/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 17/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 18/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 19/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 20/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 21/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 22/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 23/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 24/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 25/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 26/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 27/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 28/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 29/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 30/32 :    0
gpu108-16-r:61913:63267 [2] NCCL INFO Channel 31/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-16-r:61913:63267 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-16-r:61913:63267 [2] NCCL INFO P2P Chunksize set to 131072
gpu108-16-r:61914:63268 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff
gpu108-16-r:61912:63269 [1] NCCL INFO Connected all rings
gpu108-16-r:61912:63269 [1] NCCL INFO Connected all trees
gpu108-16-r:61912:63269 [1] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 00/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 01/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 02/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 03/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 04/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 05/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 06/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 07/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 08/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 09/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 10/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 11/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 12/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 13/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 14/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 15/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 16/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 17/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 18/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 19/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 20/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 21/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 22/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 23/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 24/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 25/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 26/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 27/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 28/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 29/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 30/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Channel 31/32 :    0
gpu108-16-r:61914:63268 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-16-r:61914:63268 [3] NCCL INFO P2P Chunksize set to 131072
gpu108-16-r:61911:63271 [0] NCCL INFO Setting affinity for GPU 0 to 0fffffff,00000000
gpu108-16-r:61914:63268 [3] NCCL INFO Connected all rings
gpu108-16-r:61914:63268 [3] NCCL INFO Connected all trees
gpu108-16-r:61914:63268 [3] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 00/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 01/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 02/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 03/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 04/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 05/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 06/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 07/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 08/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 09/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 10/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 11/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 12/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 13/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 14/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 15/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 16/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 17/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 18/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 19/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 20/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 21/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 22/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 23/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 24/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 25/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 26/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 27/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 28/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 29/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 30/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Channel 31/32 :    0
gpu108-16-r:61911:63271 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-16-r:61911:63271 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-16-r:61911:63271 [0] NCCL INFO Connected all rings
gpu108-16-r:61911:63271 [0] NCCL INFO Connected all trees
gpu108-16-r:61911:63271 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-16-r:61913:63267 [2] NCCL INFO Connected all rings
gpu108-16-r:61913:63267 [2] NCCL INFO Connected all trees
gpu108-16-r:61913:63267 [2] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-09-l:50447:51888 [2] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-09-l:50447:51888 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 00/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 01/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 02/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 03/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 04/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 05/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 06/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 07/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 08/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 09/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 10/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 11/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 12/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 13/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 14/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 15/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 16/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 17/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 18/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 19/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 20/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 21/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 22/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 23/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 24/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 25/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 26/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 27/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 28/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 29/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 30/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Channel 31/32 :    0
gpu108-09-l:50447:51888 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-09-l:50447:51888 [2] NCCL INFO P2P Chunksize set to 131072
gpu108-09-l:50447:51888 [2] NCCL INFO Connected all rings
gpu108-09-l:50447:51888 [2] NCCL INFO Connected all trees
gpu108-09-l:50447:51888 [2] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-09-l:50446:51890 [1] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-09-l:50448:51892 [3] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-09-l:50446:51890 [1] NCCL INFO Setting affinity for GPU 1 to 0fffffff,00000000
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 00/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 01/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 02/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 03/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 04/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 05/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 06/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 07/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 08/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 09/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 10/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 11/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 12/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 13/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 14/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 15/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 16/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 17/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 18/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 19/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 20/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 21/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 22/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 23/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 24/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 25/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 26/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 27/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 28/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 29/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 30/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Channel 31/32 :    0
gpu108-09-l:50446:51890 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-09-l:50446:51890 [1] NCCL INFO P2P Chunksize set to 131072
gpu108-09-l:50445:51893 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-09-l:50448:51892 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff
gpu108-09-l:50446:51890 [1] NCCL INFO Connected all rings
gpu108-09-l:50446:51890 [1] NCCL INFO Connected all trees
gpu108-09-l:50446:51890 [1] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-09-l:50445:51893 [0] NCCL INFO Setting affinity for GPU 0 to 0fffffff,00000000
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 00/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 01/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 02/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 03/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 04/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 05/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 06/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 07/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 08/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 09/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 10/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 11/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 12/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 13/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 14/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 15/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 16/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 17/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 18/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 19/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 20/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 21/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 22/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 23/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 24/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 25/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 26/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 27/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 28/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 29/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 30/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Channel 31/32 :    0
gpu108-09-l:50445:51893 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-09-l:50445:51893 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-09-l:50445:51893 [0] NCCL INFO Connected all rings
gpu108-09-l:50445:51893 [0] NCCL INFO Connected all trees
gpu108-09-l:50445:51893 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 00/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 01/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 02/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 03/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 04/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 05/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 06/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 07/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 08/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 09/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 10/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 11/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 12/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 13/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 14/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 15/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 16/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 17/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 18/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 19/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 20/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 21/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 22/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 23/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 24/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 25/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 26/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 27/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 28/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 29/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 30/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Channel 31/32 :    0
gpu108-09-l:50448:51892 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-09-l:50448:51892 [3] NCCL INFO P2P Chunksize set to 131072
gpu108-09-l:50448:51892 [3] NCCL INFO Connected all rings
gpu108-09-l:50448:51892 [3] NCCL INFO Connected all trees
gpu108-09-l:50448:51892 [3] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-16-r:61912:63269 [1] NCCL INFO comm 0x70dd6c20 rank 0 nranks 1 cudaDev 1 busId 46000 commId 0x2b23589f49ec160d - Init COMPLETE
gpu108-16-r:61914:63268 [3] NCCL INFO comm 0x71442350 rank 0 nranks 1 cudaDev 3 busId c7000 commId 0xbb75466d45d3dbb8 - Init COMPLETE
gpu108-16-r:61911:63271 [0] NCCL INFO comm 0x6e159ea0 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0x39f3035dd1682233 - Init COMPLETE
gpu108-16-r:61913:63267 [2] NCCL INFO comm 0x715fd160 rank 0 nranks 1 cudaDev 2 busId 85000 commId 0xdb422e253a5d319d - Init COMPLETE
gpu108-09-l:50447:51888 [2] NCCL INFO comm 0x6ed8a9a0 rank 0 nranks 1 cudaDev 2 busId 85000 commId 0x97e65b1bf8d39b82 - Init COMPLETE
gpu108-09-l:50446:51890 [1] NCCL INFO comm 0x6fdb4950 rank 0 nranks 1 cudaDev 1 busId 46000 commId 0xfec25a157872e230 - Init COMPLETE
gpu108-09-l:50445:51893 [0] NCCL INFO comm 0x6e3ccf30 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0xab2c85e6135a89be - Init COMPLETE
gpu108-09-l:50448:51892 [3] NCCL INFO comm 0x6f289c40 rank 0 nranks 1 cudaDev 3 busId c7000 commId 0xcca8e8a52aa05655 - Init COMPLETE
[2023-03-17 14:29:51,584] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.36
[2023-03-17 14:29:51,584] [INFO] [logging.py:93:log_dist] [Rank 0] step=1, skipped=0, lr=[5.9946721667563326e-05, 5.9946721667563326e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:29:51,585] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2774.06 | backward_microstep: 2734.73 | backward_inner_microstep: 2293.36 | backward_allreduce_microstep: 440.64 | step_microstep: 170.90
[2023-03-17 14:29:51,585] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 2773.97 | backward: 2734.70 | backward_inner: 2293.45 | backward_allreduce: 440.64 | step: 170.90
[Rank 0] (after 1 iterations) memory (MB) | allocated: 4831.3671875 | max allocated: 16048.474609375 | reserved: 25640.0 | max reserved: 25640.0
 iteration        1/      50 | consumed samples:          256 | consumed tokens:       262144 | elapsed time per iteration (ms): 5703.7 | learning rate: 5.995E-05 | global batch size:    32 | lm loss: 1.103350E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 5.610 | TFLOPs: 13.22 |
time (ms) | forward-compute: 2793.93 | backward-compute: 2735.09 | backward-embedding-all-reduce: 0.01 | optimizer: 171.75 | batch-generator: 6.63
[2023-03-17 14:29:55,381] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.36
[2023-03-17 14:29:55,381] [INFO] [logging.py:93:log_dist] [Rank 0] step=2, skipped=0, lr=[5.97870969354909e-05, 5.97870969354909e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:29:55,381] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1263.03 | backward_microstep: 2497.73 | backward_inner_microstep: 2027.51 | backward_allreduce_microstep: 469.54 | step_microstep: 17.86
[2023-03-17 14:29:55,382] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1262.95 | backward: 2497.70 | backward_inner: 2027.57 | backward_allreduce: 469.56 | step: 17.87
 iteration        2/      50 | consumed samples:          512 | consumed tokens:       524288 | elapsed time per iteration (ms): 3796.1 | learning rate: 5.979E-05 | global batch size:    32 | lm loss: 9.609643E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.430 | TFLOPs: 19.86 |
time (ms) | forward-compute: 1276.96 | backward-compute: 2498.13 | backward-embedding-all-reduce: 0.01 | optimizer: 18.63 | batch-generator: 5.35
[2023-03-17 14:29:59,155] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.36
[2023-03-17 14:29:59,155] [INFO] [logging.py:93:log_dist] [Rank 0] step=3, skipped=0, lr=[5.95217557696746e-05, 5.95217557696746e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:29:59,155] [INFO] [timer.py:198:stop] epoch=0/micro_step=3/global_step=3, RunningAvgSamplesPerSec=66.44550529785883, CurrSamplesPerSec=66.44550529785883, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:29:59,156] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1271.37 | backward_microstep: 2469.61 | backward_inner_microstep: 2007.36 | backward_allreduce_microstep: 461.58 | step_microstep: 18.21
[2023-03-17 14:29:59,156] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1271.29 | backward: 2469.58 | backward_inner: 2007.43 | backward_allreduce: 461.60 | step: 18.21
 iteration        3/      50 | consumed samples:          768 | consumed tokens:       786432 | elapsed time per iteration (ms): 3774.2 | learning rate: 5.952E-05 | global batch size:    32 | lm loss: 1.049972E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.479 | TFLOPs: 19.97 |
time (ms) | forward-compute: 1283.33 | backward-compute: 2470.25 | backward-embedding-all-reduce: 0.01 | optimizer: 18.71 | batch-generator: 5.73
[2023-03-17 14:30:02,908] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.42
[2023-03-17 14:30:02,908] [INFO] [logging.py:93:log_dist] [Rank 0] step=4, skipped=0, lr=[5.9151745350473036e-05, 5.9151745350473036e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:30:02,908] [INFO] [timer.py:198:stop] epoch=0/micro_step=4/global_step=4, RunningAvgSamplesPerSec=66.41678450130033, CurrSamplesPerSec=66.3880885229068, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:30:02,909] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1265.03 | backward_microstep: 2453.91 | backward_inner_microstep: 2005.91 | backward_allreduce_microstep: 447.26 | step_microstep: 18.16
[2023-03-17 14:30:02,909] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1264.95 | backward: 2453.88 | backward_inner: 2006.00 | backward_allreduce: 447.26 | step: 18.16
 iteration        4/      50 | consumed samples:         1024 | consumed tokens:      1048576 | elapsed time per iteration (ms): 3752.9 | learning rate: 5.915E-05 | global batch size:    32 | lm loss: 1.112487E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.527 | TFLOPs: 20.09 |
time (ms) | forward-compute: 1278.10 | backward-compute: 2454.40 | backward-embedding-all-reduce: 0.01 | optimizer: 18.63 | batch-generator: 5.10
[2023-03-17 14:30:06,673] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.35
[2023-03-17 14:30:06,673] [INFO] [logging.py:93:log_dist] [Rank 0] step=5, skipped=0, lr=[5.8678525939969144e-05, 5.8678525939969144e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:30:06,673] [INFO] [timer.py:198:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=66.40393093649993, CurrSamplesPerSec=66.37823872633919, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:30:06,674] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1270.43 | backward_microstep: 2460.80 | backward_inner_microstep: 2012.03 | backward_allreduce_microstep: 448.10 | step_microstep: 18.11
[2023-03-17 14:30:06,675] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1270.35 | backward: 2460.77 | backward_inner: 2012.09 | backward_allreduce: 448.12 | step: 18.11
 iteration        5/      50 | consumed samples:         1280 | consumed tokens:      1310720 | elapsed time per iteration (ms): 3765.1 | learning rate: 5.868E-05 | global batch size:    32 | lm loss: 1.062357E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.499 | TFLOPs: 20.02 |
time (ms) | forward-compute: 1283.68 | backward-compute: 2460.90 | backward-embedding-all-reduce: 0.01 | optimizer: 18.71 | batch-generator: 5.28
[2023-03-17 14:30:10,432] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.36
[2023-03-17 14:30:10,432] [INFO] [logging.py:93:log_dist] [Rank 0] step=6, skipped=0, lr=[5.810396511898279e-05, 5.810396511898279e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:30:10,433] [INFO] [timer.py:198:stop] epoch=0/micro_step=6/global_step=6, RunningAvgSamplesPerSec=66.3910604521564, CurrSamplesPerSec=66.35247891659033, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:30:10,433] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1268.08 | backward_microstep: 2457.05 | backward_inner_microstep: 2010.03 | backward_allreduce_microstep: 446.30 | step_microstep: 18.16
[2023-03-17 14:30:10,434] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1268.00 | backward: 2457.02 | backward_inner: 2010.12 | backward_allreduce: 446.31 | step: 18.17
 iteration        6/      50 | consumed samples:         1536 | consumed tokens:      1572864 | elapsed time per iteration (ms): 3759.1 | learning rate: 5.810E-05 | global batch size:    32 | lm loss: 1.015061E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.513 | TFLOPs: 20.06 |
time (ms) | forward-compute: 1281.04 | backward-compute: 2457.67 | backward-embedding-all-reduce: 0.01 | optimizer: 18.63 | batch-generator: 5.08
[2023-03-17 14:30:14,203] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.35
[2023-03-17 14:30:14,203] [INFO] [logging.py:93:log_dist] [Rank 0] step=7, skipped=0, lr=[5.743033041658253e-05, 5.743033041658253e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:30:14,203] [INFO] [timer.py:198:stop] epoch=0/micro_step=7/global_step=7, RunningAvgSamplesPerSec=66.23441081466052, CurrSamplesPerSec=65.61513544167038, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:30:14,204] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1264.53 | backward_microstep: 2471.36 | backward_inner_microstep: 2010.86 | backward_allreduce_microstep: 459.76 | step_microstep: 18.05
[2023-03-17 14:30:14,204] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1264.45 | backward: 2471.33 | backward_inner: 2010.96 | backward_allreduce: 459.77 | step: 18.05
 iteration        7/      50 | consumed samples:         1792 | consumed tokens:      1835008 | elapsed time per iteration (ms): 3770.8 | learning rate: 5.743E-05 | global batch size:    32 | lm loss: 9.588090E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.486 | TFLOPs: 19.99 |
time (ms) | forward-compute: 1278.72 | backward-compute: 2471.83 | backward-embedding-all-reduce: 0.01 | optimizer: 18.51 | batch-generator: 4.99
[2023-03-17 14:30:18,001] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.36
[2023-03-17 14:30:18,001] [INFO] [logging.py:93:log_dist] [Rank 0] step=8, skipped=0, lr=[5.666028036118432e-05, 5.666028036118432e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:30:18,001] [INFO] [timer.py:198:stop] epoch=0/micro_step=8/global_step=8, RunningAvgSamplesPerSec=66.2545011650597, CurrSamplesPerSec=66.35513601006171, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:30:18,002] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1265.26 | backward_microstep: 2497.32 | backward_inner_microstep: 2018.05 | backward_allreduce_microstep: 478.54 | step_microstep: 18.02
[2023-03-17 14:30:18,002] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1265.18 | backward: 2497.29 | backward_inner: 2018.15 | backward_allreduce: 478.53 | step: 18.02
 iteration        8/      50 | consumed samples:         2048 | consumed tokens:      2097152 | elapsed time per iteration (ms): 3797.9 | learning rate: 5.666E-05 | global batch size:    32 | lm loss: 8.921160E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.426 | TFLOPs: 19.85 |
time (ms) | forward-compute: 1280.21 | backward-compute: 2497.39 | backward-embedding-all-reduce: 0.01 | optimizer: 18.64 | batch-generator: 5.01
[2023-03-17 14:30:21,776] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.34
[2023-03-17 14:30:21,777] [INFO] [logging.py:93:log_dist] [Rank 0] step=9, skipped=0, lr=[5.579685398855441e-05, 5.579685398855441e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:30:21,777] [INFO] [timer.py:198:stop] epoch=0/micro_step=9/global_step=9, RunningAvgSamplesPerSec=66.04476954305643, CurrSamplesPerSec=64.81374449373244, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:30:21,778] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1262.99 | backward_microstep: 2478.34 | backward_inner_microstep: 2019.05 | backward_allreduce_microstep: 458.65 | step_microstep: 18.08
[2023-03-17 14:30:21,778] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1262.91 | backward: 2478.31 | backward_inner: 2019.10 | backward_allreduce: 458.68 | step: 18.08
 iteration        9/      50 | consumed samples:         2304 | consumed tokens:      2359296 | elapsed time per iteration (ms): 3775.6 | learning rate: 5.580E-05 | global batch size:    32 | lm loss: 8.339353E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.475 | TFLOPs: 19.97 |
time (ms) | forward-compute: 1276.34 | backward-compute: 2478.95 | backward-embedding-all-reduce: 0.01 | optimizer: 18.55 | batch-generator: 4.84
[2023-03-17 14:30:25,589] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.33
[2023-03-17 14:30:25,589] [INFO] [logging.py:93:log_dist] [Rank 0] step=10, skipped=0, lr=[5.4843458848123576e-05, 5.4843458848123576e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:30:25,590] [INFO] [timer.py:198:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=65.86620743078306, CurrSamplesPerSec=64.6428056088288, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:30:25,590] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1268.72 | backward_microstep: 2509.35 | backward_inner_microstep: 2018.20 | backward_allreduce_microstep: 490.43 | step_microstep: 18.00
[2023-03-17 14:30:25,591] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1268.63 | backward: 2509.31 | backward_inner: 2018.28 | backward_allreduce: 490.44 | step: 18.01
 iteration       10/      50 | consumed samples:         2560 | consumed tokens:      2621440 | elapsed time per iteration (ms): 3812.8 | learning rate: 5.484E-05 | global batch size:    32 | lm loss: 7.908068E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.393 | TFLOPs: 19.77 |
time (ms) | forward-compute: 1282.78 | backward-compute: 2509.59 | backward-embedding-all-reduce: 0.01 | optimizer: 18.64 | batch-generator: 5.14
[2023-03-17 14:30:29,365] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.35
[2023-03-17 14:30:29,365] [INFO] [logging.py:93:log_dist] [Rank 0] step=11, skipped=0, lr=[5.380385755494631e-05, 5.380385755494631e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:30:29,365] [INFO] [timer.py:198:stop] epoch=0/micro_step=11/global_step=11, RunningAvgSamplesPerSec=65.90880741931043, CurrSamplesPerSec=66.25160140363764, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:30:29,366] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1264.40 | backward_microstep: 2477.80 | backward_inner_microstep: 2017.55 | backward_allreduce_microstep: 459.51 | step_microstep: 18.04
[2023-03-17 14:30:29,366] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1264.32 | backward: 2477.75 | backward_inner: 2017.64 | backward_allreduce: 459.52 | step: 18.04
 iteration       11/      50 | consumed samples:         2816 | consumed tokens:      2883584 | elapsed time per iteration (ms): 3775.3 | learning rate: 5.380E-05 | global batch size:    32 | lm loss: 7.693277E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.476 | TFLOPs: 19.97 |
time (ms) | forward-compute: 1276.78 | backward-compute: 2478.05 | backward-embedding-all-reduce: 0.01 | optimizer: 18.68 | batch-generator: 5.38
[2023-03-17 14:30:33,142] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.36
[2023-03-17 14:30:33,142] [INFO] [logging.py:93:log_dist] [Rank 0] step=12, skipped=0, lr=[5.2682152940378117e-05, 5.2682152940378117e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:30:33,142] [INFO] [timer.py:198:stop] epoch=0/micro_step=12/global_step=12, RunningAvgSamplesPerSec=65.93108429634691, CurrSamplesPerSec=66.13225591083672, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:30:33,143] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1266.53 | backward_microstep: 2475.74 | backward_inner_microstep: 2022.42 | backward_allreduce_microstep: 452.67 | step_microstep: 18.22
[2023-03-17 14:30:33,143] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1266.45 | backward: 2475.71 | backward_inner: 2022.47 | backward_allreduce: 452.69 | step: 18.23
 iteration       12/      50 | consumed samples:         3072 | consumed tokens:      3145728 | elapsed time per iteration (ms): 3777.0 | learning rate: 5.268E-05 | global batch size:    32 | lm loss: 7.586111E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.472 | TFLOPs: 19.96 |
time (ms) | forward-compute: 1280.21 | backward-compute: 2476.28 | backward-embedding-all-reduce: 0.01 | optimizer: 18.73 | batch-generator: 5.04
[2023-03-17 14:30:36,897] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.39
[2023-03-17 14:30:36,897] [INFO] [logging.py:93:log_dist] [Rank 0] step=13, skipped=0, lr=[5.14827718600746e-05, 5.14827718600746e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:30:36,897] [INFO] [timer.py:198:stop] epoch=0/micro_step=13/global_step=13, RunningAvgSamplesPerSec=65.95476945638774, CurrSamplesPerSec=66.19256038656873, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:30:36,898] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1252.26 | backward_microstep: 2469.77 | backward_inner_microstep: 2016.85 | backward_allreduce_microstep: 452.24 | step_microstep: 18.16
[2023-03-17 14:30:36,898] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1252.18 | backward: 2469.74 | backward_inner: 2016.92 | backward_allreduce: 452.24 | step: 18.17
 iteration       13/      50 | consumed samples:         3328 | consumed tokens:      3407872 | elapsed time per iteration (ms): 3755.3 | learning rate: 5.148E-05 | global batch size:    32 | lm loss: 7.474560E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.521 | TFLOPs: 20.08 |
time (ms) | forward-compute: 1264.54 | backward-compute: 2470.35 | backward-embedding-all-reduce: 0.01 | optimizer: 18.64 | batch-generator: 5.10
[2023-03-17 14:30:40,661] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.35
[2023-03-17 14:30:40,661] [INFO] [logging.py:93:log_dist] [Rank 0] step=14, skipped=0, lr=[5.021044772321462e-05, 5.021044772321462e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:30:40,661] [INFO] [timer.py:198:stop] epoch=0/micro_step=14/global_step=14, RunningAvgSamplesPerSec=65.97838215289454, CurrSamplesPerSec=66.23924210995939, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:30:40,662] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1267.34 | backward_microstep: 2461.60 | backward_inner_microstep: 2020.98 | backward_allreduce_microstep: 440.05 | step_microstep: 18.02
[2023-03-17 14:30:40,662] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1267.27 | backward: 2461.57 | backward_inner: 2020.98 | backward_allreduce: 440.09 | step: 18.02
 iteration       14/      50 | consumed samples:         3584 | consumed tokens:      3670016 | elapsed time per iteration (ms): 3763.8 | learning rate: 5.021E-05 | global batch size:    32 | lm loss: 7.424503E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.502 | TFLOPs: 20.03 |
time (ms) | forward-compute: 1281.61 | backward-compute: 2461.78 | backward-embedding-all-reduce: 0.01 | optimizer: 18.64 | batch-generator: 5.20
[2023-03-17 14:30:44,456] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.34
[2023-03-17 14:30:44,456] [INFO] [logging.py:93:log_dist] [Rank 0] step=15, skipped=0, lr=[4.887020181189677e-05, 4.887020181189677e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:30:44,457] [INFO] [timer.py:198:stop] epoch=0/micro_step=15/global_step=15, RunningAvgSamplesPerSec=65.89299380655193, CurrSamplesPerSec=64.8853093344188, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:30:44,457] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1271.36 | backward_microstep: 2490.38 | backward_inner_microstep: 2015.84 | backward_allreduce_microstep: 473.88 | step_microstep: 18.06
[2023-03-17 14:30:44,458] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1271.28 | backward: 2490.35 | backward_inner: 2015.90 | backward_allreduce: 473.89 | step: 18.07
 iteration       15/      50 | consumed samples:         3840 | consumed tokens:      3932160 | elapsed time per iteration (ms): 3795.4 | learning rate: 4.887E-05 | global batch size:    32 | lm loss: 7.431489E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.431 | TFLOPs: 19.86 |
time (ms) | forward-compute: 1284.12 | backward-compute: 2490.88 | backward-embedding-all-reduce: 0.01 | optimizer: 18.57 | batch-generator: 5.12
[2023-03-17 14:30:48,240] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.35
[2023-03-17 14:30:48,241] [INFO] [logging.py:93:log_dist] [Rank 0] step=16, skipped=0, lr=[4.74673234644329e-05, 4.74673234644329e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:30:48,241] [INFO] [timer.py:198:stop] epoch=0/micro_step=16/global_step=16, RunningAvgSamplesPerSec=65.91204948349115, CurrSamplesPerSec=66.16078002228072, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:30:48,242] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1263.35 | backward_microstep: 2487.71 | backward_inner_microstep: 2016.39 | backward_allreduce_microstep: 470.65 | step_microstep: 18.06
[2023-03-17 14:30:48,242] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1263.27 | backward: 2487.68 | backward_inner: 2016.45 | backward_allreduce: 470.66 | step: 18.06
 iteration       16/      50 | consumed samples:         4096 | consumed tokens:      4194304 | elapsed time per iteration (ms): 3784.4 | learning rate: 4.747E-05 | global batch size:    32 | lm loss: 7.335297E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.456 | TFLOPs: 19.92 |
time (ms) | forward-compute: 1275.99 | backward-compute: 2488.11 | backward-embedding-all-reduce: 0.01 | optimizer: 18.52 | batch-generator: 5.06
[2023-03-17 14:30:52,040] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.38
[2023-03-17 14:30:52,040] [INFO] [logging.py:93:log_dist] [Rank 0] step=17, skipped=0, lr=[4.6007349200746303e-05, 4.6007349200746303e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:30:52,041] [INFO] [timer.py:198:stop] epoch=0/micro_step=17/global_step=17, RunningAvgSamplesPerSec=65.86940166452534, CurrSamplesPerSec=65.27807510558866, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:30:52,041] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1264.17 | backward_microstep: 2500.94 | backward_inner_microstep: 2017.05 | backward_allreduce_microstep: 483.25 | step_microstep: 18.09
[2023-03-17 14:30:52,042] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1264.09 | backward: 2500.92 | backward_inner: 2017.10 | backward_allreduce: 483.27 | step: 18.09
 iteration       17/      50 | consumed samples:         4352 | consumed tokens:      4456448 | elapsed time per iteration (ms): 3799.7 | learning rate: 4.601E-05 | global batch size:    32 | lm loss: 7.310195E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.422 | TFLOPs: 19.84 |
time (ms) | forward-compute: 1278.38 | backward-compute: 2500.96 | backward-embedding-all-reduce: 0.01 | optimizer: 18.60 | batch-generator: 5.07
[2023-03-17 14:30:55,835] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.33
[2023-03-17 14:30:55,835] [INFO] [logging.py:93:log_dist] [Rank 0] step=18, skipped=0, lr=[4.4496040872256956e-05, 4.4496040872256956e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:30:55,836] [INFO] [timer.py:198:stop] epoch=0/micro_step=18/global_step=18, RunningAvgSamplesPerSec=65.88698371018981, CurrSamplesPerSec=66.15184525470025, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:30:55,836] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1267.96 | backward_microstep: 2493.55 | backward_inner_microstep: 2014.75 | backward_allreduce_microstep: 478.13 | step_microstep: 18.05
[2023-03-17 14:30:55,837] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1267.88 | backward: 2493.52 | backward_inner: 2014.82 | backward_allreduce: 478.15 | step: 18.05
 iteration       18/      50 | consumed samples:         4608 | consumed tokens:      4718592 | elapsed time per iteration (ms): 3795.0 | learning rate: 4.450E-05 | global batch size:    32 | lm loss: 7.359069E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.432 | TFLOPs: 19.87 |
time (ms) | forward-compute: 1281.14 | backward-compute: 2493.50 | backward-embedding-all-reduce: 0.01 | optimizer: 18.61 | batch-generator: 5.00
[2023-03-17 14:30:59,616] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.34
[2023-03-17 14:30:59,616] [INFO] [logging.py:93:log_dist] [Rank 0] step=19, skipped=0, lr=[4.293936292248631e-05, 4.293936292248631e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:30:59,616] [INFO] [timer.py:198:stop] epoch=0/micro_step=19/global_step=19, RunningAvgSamplesPerSec=65.90524998819356, CurrSamplesPerSec=66.1988940030343, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:30:59,617] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1271.91 | backward_microstep: 2477.59 | backward_inner_microstep: 2019.87 | backward_allreduce_microstep: 457.05 | step_microstep: 18.14
[2023-03-17 14:30:59,617] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1271.83 | backward: 2477.56 | backward_inner: 2019.93 | backward_allreduce: 457.05 | step: 18.14
 iteration       19/      50 | consumed samples:         4864 | consumed tokens:      4980736 | elapsed time per iteration (ms): 3780.6 | learning rate: 4.294E-05 | global batch size:    32 | lm loss: 7.345951E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.464 | TFLOPs: 19.94 |
time (ms) | forward-compute: 1281.88 | backward-compute: 2478.32 | backward-embedding-all-reduce: 0.01 | optimizer: 18.65 | batch-generator: 5.10
[2023-03-17 14:31:03,398] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.36
[2023-03-17 14:31:03,398] [INFO] [logging.py:93:log_dist] [Rank 0] step=20, skipped=0, lr=[4.134345884812357e-05, 4.134345884812357e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:31:03,398] [INFO] [timer.py:198:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=65.88630876748961, CurrSamplesPerSec=65.56596569525917, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:31:03,399] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1271.04 | backward_microstep: 2477.09 | backward_inner_microstep: 2018.21 | backward_allreduce_microstep: 458.21 | step_microstep: 17.92
[2023-03-17 14:31:03,399] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1270.96 | backward: 2477.06 | backward_inner: 2018.29 | backward_allreduce: 458.22 | step: 17.92
 iteration       20/      50 | consumed samples:         5120 | consumed tokens:      5242880 | elapsed time per iteration (ms): 3781.8 | learning rate: 4.134E-05 | global batch size:    32 | lm loss: 7.327268E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.462 | TFLOPs: 19.94 |
time (ms) | forward-compute: 1283.81 | backward-compute: 2477.60 | backward-embedding-all-reduce: 0.01 | optimizer: 18.57 | batch-generator: 5.18
[2023-03-17 14:31:07,179] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.40
[2023-03-17 14:31:07,179] [INFO] [logging.py:93:log_dist] [Rank 0] step=21, skipped=0, lr=[3.971462695345109e-05, 3.971462695345109e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:31:07,180] [INFO] [timer.py:198:stop] epoch=0/micro_step=21/global_step=21, RunningAvgSamplesPerSec=65.91093620349807, CurrSamplesPerSec=66.3573996279137, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:31:07,180] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1273.01 | backward_microstep: 2475.51 | backward_inner_microstep: 2025.13 | backward_allreduce_microstep: 449.75 | step_microstep: 18.01
[2023-03-17 14:31:07,181] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1272.92 | backward: 2475.48 | backward_inner: 2025.17 | backward_allreduce: 449.77 | step: 18.02
 iteration       21/      50 | consumed samples:         5376 | consumed tokens:      5505024 | elapsed time per iteration (ms): 3781.4 | learning rate: 3.971E-05 | global batch size:    32 | lm loss: 7.223094E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.462 | TFLOPs: 19.94 |
time (ms) | forward-compute: 1285.34 | backward-compute: 2475.84 | backward-embedding-all-reduce: 0.01 | optimizer: 18.53 | batch-generator: 4.88
[2023-03-17 14:31:10,959] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.37
[2023-03-17 14:31:10,959] [INFO] [logging.py:93:log_dist] [Rank 0] step=22, skipped=0, lr=[3.805929549381457e-05, 3.805929549381457e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:31:10,959] [INFO] [timer.py:198:stop] epoch=0/micro_step=22/global_step=22, RunningAvgSamplesPerSec=65.94139630641848, CurrSamplesPerSec=66.52553484768805, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:31:10,960] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1263.23 | backward_microstep: 2484.07 | backward_inner_microstep: 2023.84 | backward_allreduce_microstep: 459.56 | step_microstep: 18.10
[2023-03-17 14:31:10,960] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1263.15 | backward: 2484.04 | backward_inner: 2023.91 | backward_allreduce: 459.57 | step: 18.11
 iteration       22/      50 | consumed samples:         5632 | consumed tokens:      5767168 | elapsed time per iteration (ms): 3779.5 | learning rate: 3.806E-05 | global batch size:    32 | lm loss: 7.235435E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.467 | TFLOPs: 19.95 |
time (ms) | forward-compute: 1274.42 | backward-compute: 2484.69 | backward-embedding-all-reduce: 0.01 | optimizer: 18.61 | batch-generator: 5.13
[2023-03-17 14:31:14,747] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.35
[2023-03-17 14:31:14,747] [INFO] [logging.py:93:log_dist] [Rank 0] step=23, skipped=0, lr=[3.638399730623622e-05, 3.638399730623622e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:31:14,747] [INFO] [timer.py:198:stop] epoch=0/micro_step=23/global_step=23, RunningAvgSamplesPerSec=65.94759746675066, CurrSamplesPerSec=66.07186606215448, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:31:14,748] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1260.55 | backward_microstep: 2493.65 | backward_inner_microstep: 2025.15 | backward_allreduce_microstep: 467.83 | step_microstep: 17.90
[2023-03-17 14:31:14,748] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1260.47 | backward: 2493.62 | backward_inner: 2025.22 | backward_allreduce: 467.84 | step: 17.91
 iteration       23/      50 | consumed samples:         5888 | consumed tokens:      6029312 | elapsed time per iteration (ms): 3788.0 | learning rate: 3.638E-05 | global batch size:    32 | lm loss: 7.274252E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.448 | TFLOPs: 19.90 |
time (ms) | forward-compute: 1273.40 | backward-compute: 2494.27 | backward-embedding-all-reduce: 0.01 | optimizer: 18.58 | batch-generator: 5.04
[2023-03-17 14:31:18,546] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.33
[2023-03-17 14:31:18,546] [INFO] [logging.py:93:log_dist] [Rank 0] step=24, skipped=0, lr=[3.469534402729146e-05, 3.469534402729146e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:31:18,546] [INFO] [timer.py:198:stop] epoch=0/micro_step=24/global_step=24, RunningAvgSamplesPerSec=65.91356494279387, CurrSamplesPerSec=65.20690888173748, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:31:18,547] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1268.20 | backward_microstep: 2497.50 | backward_inner_microstep: 2027.93 | backward_allreduce_microstep: 468.93 | step_microstep: 17.91
[2023-03-17 14:31:18,547] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1268.12 | backward: 2497.47 | backward_inner: 2027.98 | backward_allreduce: 468.97 | step: 17.91
 iteration       24/      50 | consumed samples:         6144 | consumed tokens:      6291456 | elapsed time per iteration (ms): 3798.9 | learning rate: 3.470E-05 | global batch size:    32 | lm loss: 7.261285E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.424 | TFLOPs: 19.85 |
time (ms) | forward-compute: 1280.71 | backward-compute: 2497.82 | backward-embedding-all-reduce: 0.01 | optimizer: 18.57 | batch-generator: 4.99
[2023-03-17 14:31:22,357] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.40
[2023-03-17 14:31:22,357] [INFO] [logging.py:93:log_dist] [Rank 0] step=25, skipped=0, lr=[3.3e-05, 3.3e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:31:22,358] [INFO] [timer.py:198:stop] epoch=0/micro_step=25/global_step=25, RunningAvgSamplesPerSec=65.88036561099639, CurrSamplesPerSec=65.15834882783865, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:31:22,358] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1273.47 | backward_microstep: 2503.39 | backward_inner_microstep: 2028.94 | backward_allreduce_microstep: 473.81 | step_microstep: 18.23
[2023-03-17 14:31:22,359] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1273.39 | backward: 2503.36 | backward_inner: 2028.99 | backward_allreduce: 473.83 | step: 18.24
 iteration       25/      50 | consumed samples:         6400 | consumed tokens:      6553600 | elapsed time per iteration (ms): 3811.6 | learning rate: 3.300E-05 | global batch size:    32 | lm loss: 7.277561E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.395 | TFLOPs: 19.78 |
time (ms) | forward-compute: 1287.10 | backward-compute: 2504.03 | backward-embedding-all-reduce: 0.01 | optimizer: 18.68 | batch-generator: 5.04
[2023-03-17 14:31:26,154] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.35
[2023-03-17 14:31:26,155] [INFO] [logging.py:93:log_dist] [Rank 0] step=26, skipped=0, lr=[3.1304655972708536e-05, 3.1304655972708536e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:31:26,155] [INFO] [timer.py:198:stop] epoch=0/micro_step=26/global_step=26, RunningAvgSamplesPerSec=65.87711026160605, CurrSamplesPerSec=65.8023259177403, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:31:26,155] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1274.07 | backward_microstep: 2491.14 | backward_inner_microstep: 2028.37 | backward_allreduce_microstep: 462.10 | step_microstep: 18.02
[2023-03-17 14:31:26,156] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1273.98 | backward: 2491.11 | backward_inner: 2028.44 | backward_allreduce: 462.11 | step: 18.03
 iteration       26/      50 | consumed samples:         6656 | consumed tokens:      6815744 | elapsed time per iteration (ms): 3797.3 | learning rate: 3.130E-05 | global batch size:    32 | lm loss: 7.272747E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.427 | TFLOPs: 19.85 |
time (ms) | forward-compute: 1285.23 | backward-compute: 2491.60 | backward-embedding-all-reduce: 0.01 | optimizer: 18.70 | batch-generator: 4.98
[2023-03-17 14:31:29,962] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.40
[2023-03-17 14:31:29,962] [INFO] [logging.py:93:log_dist] [Rank 0] step=27, skipped=0, lr=[2.961600269376378e-05, 2.961600269376378e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:31:29,963] [INFO] [timer.py:198:stop] epoch=0/micro_step=27/global_step=27, RunningAvgSamplesPerSec=65.88490820106533, CurrSamplesPerSec=66.07261415588616, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:31:29,963] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1274.26 | backward_microstep: 2500.45 | backward_inner_microstep: 2030.77 | backward_allreduce_microstep: 469.05 | step_microstep: 18.04
[2023-03-17 14:31:29,964] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1274.18 | backward: 2500.41 | backward_inner: 2030.81 | backward_allreduce: 469.07 | step: 18.04
 iteration       27/      50 | consumed samples:         6912 | consumed tokens:      7077888 | elapsed time per iteration (ms): 3807.5 | learning rate: 2.962E-05 | global batch size:    32 | lm loss: 7.300958E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.405 | TFLOPs: 19.80 |
time (ms) | forward-compute: 1286.25 | backward-compute: 2500.87 | backward-embedding-all-reduce: 0.01 | optimizer: 18.56 | batch-generator: 4.93
[2023-03-17 14:31:33,759] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.39
[2023-03-17 14:31:33,759] [INFO] [logging.py:93:log_dist] [Rank 0] step=28, skipped=0, lr=[2.7940704506185428e-05, 2.7940704506185428e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:31:33,759] [INFO] [timer.py:198:stop] epoch=0/micro_step=28/global_step=28, RunningAvgSamplesPerSec=65.88273436742021, CurrSamplesPerSec=65.82843510870066, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:31:33,760] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1272.91 | backward_microstep: 2491.03 | backward_inner_microstep: 2030.87 | backward_allreduce_microstep: 459.53 | step_microstep: 17.90
[2023-03-17 14:31:33,760] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1272.83 | backward: 2491.01 | backward_inner: 2030.92 | backward_allreduce: 459.54 | step: 17.91
 iteration       28/      50 | consumed samples:         7168 | consumed tokens:      7340032 | elapsed time per iteration (ms): 3796.6 | learning rate: 2.794E-05 | global batch size:    32 | lm loss: 7.278518E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.429 | TFLOPs: 19.86 |
time (ms) | forward-compute: 1284.84 | backward-compute: 2491.27 | backward-embedding-all-reduce: 0.01 | optimizer: 18.73 | batch-generator: 4.96
[2023-03-17 14:31:37,529] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.37
[2023-03-17 14:31:37,529] [INFO] [logging.py:93:log_dist] [Rank 0] step=29, skipped=0, lr=[2.6285373046548923e-05, 2.6285373046548923e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:31:37,529] [INFO] [timer.py:198:stop] epoch=0/micro_step=29/global_step=29, RunningAvgSamplesPerSec=65.91167844907683, CurrSamplesPerSec=66.67325429307618, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:31:37,530] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1248.91 | backward_microstep: 2487.63 | backward_inner_microstep: 2037.31 | backward_allreduce_microstep: 449.66 | step_microstep: 18.03
[2023-03-17 14:31:37,530] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1248.83 | backward: 2487.60 | backward_inner: 2037.36 | backward_allreduce: 449.68 | step: 18.03
 iteration       29/      50 | consumed samples:         7424 | consumed tokens:      7602176 | elapsed time per iteration (ms): 3770.0 | learning rate: 2.629E-05 | global batch size:    32 | lm loss: 7.232816E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.488 | TFLOPs: 20.00 |
time (ms) | forward-compute: 1261.57 | backward-compute: 2488.10 | backward-embedding-all-reduce: 0.01 | optimizer: 18.55 | batch-generator: 5.12
[2023-03-17 14:31:41,316] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.39
[2023-03-17 14:31:41,316] [INFO] [logging.py:93:log_dist] [Rank 0] step=30, skipped=0, lr=[2.465654115187642e-05, 2.465654115187642e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:31:41,316] [INFO] [timer.py:198:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=65.9347069799396, CurrSamplesPerSec=66.56261787639734, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:31:41,317] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1270.57 | backward_microstep: 2483.85 | backward_inner_microstep: 2031.22 | backward_allreduce_microstep: 451.99 | step_microstep: 18.04
[2023-03-17 14:31:41,317] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1270.50 | backward: 2483.82 | backward_inner: 2031.27 | backward_allreduce: 452.01 | step: 18.05
 iteration       30/      50 | consumed samples:         7680 | consumed tokens:      7864320 | elapsed time per iteration (ms): 3787.1 | learning rate: 2.466E-05 | global batch size:    32 | lm loss: 7.235646E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.450 | TFLOPs: 19.91 |
time (ms) | forward-compute: 1282.44 | backward-compute: 2484.38 | backward-embedding-all-reduce: 0.01 | optimizer: 18.53 | batch-generator: 5.15
[2023-03-17 14:31:45,108] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.37
[2023-03-17 14:31:45,108] [INFO] [logging.py:93:log_dist] [Rank 0] step=31, skipped=0, lr=[2.3060637077513695e-05, 2.3060637077513695e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:31:45,109] [INFO] [timer.py:198:stop] epoch=0/micro_step=31/global_step=31, RunningAvgSamplesPerSec=65.9328727766454, CurrSamplesPerSec=65.88155648425482, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:31:45,109] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1274.40 | backward_microstep: 2484.09 | backward_inner_microstep: 2032.89 | backward_allreduce_microstep: 450.56 | step_microstep: 18.02
[2023-03-17 14:31:45,110] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1274.32 | backward: 2484.05 | backward_inner: 2032.93 | backward_allreduce: 450.58 | step: 18.02
 iteration       31/      50 | consumed samples:         7936 | consumed tokens:      8126464 | elapsed time per iteration (ms): 3792.2 | learning rate: 2.306E-05 | global batch size:    32 | lm loss: 7.230833E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.438 | TFLOPs: 19.88 |
time (ms) | forward-compute: 1287.63 | backward-compute: 2484.31 | backward-embedding-all-reduce: 0.01 | optimizer: 18.53 | batch-generator: 5.13
[2023-03-17 14:31:48,921] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.39
[2023-03-17 14:31:48,921] [INFO] [logging.py:93:log_dist] [Rank 0] step=32, skipped=0, lr=[2.150395912774304e-05, 2.150395912774304e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:31:48,922] [INFO] [timer.py:198:stop] epoch=0/micro_step=32/global_step=32, RunningAvgSamplesPerSec=65.91949287265547, CurrSamplesPerSec=65.5338240722476, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:31:48,922] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1274.09 | backward_microstep: 2505.76 | backward_inner_microstep: 2032.46 | backward_allreduce_microstep: 472.64 | step_microstep: 18.36
[2023-03-17 14:31:48,923] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1274.01 | backward: 2505.72 | backward_inner: 2032.52 | backward_allreduce: 472.67 | step: 18.36
 iteration       32/      50 | consumed samples:         8192 | consumed tokens:      8388608 | elapsed time per iteration (ms): 3813.0 | learning rate: 2.150E-05 | global batch size:    32 | lm loss: 7.241117E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.392 | TFLOPs: 19.77 |
time (ms) | forward-compute: 1286.57 | backward-compute: 2505.85 | backward-embedding-all-reduce: 0.01 | optimizer: 18.80 | batch-generator: 5.04
[2023-03-17 14:31:52,739] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.37
[2023-03-17 14:31:52,739] [INFO] [logging.py:93:log_dist] [Rank 0] step=33, skipped=0, lr=[1.999265079925368e-05, 1.999265079925368e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:31:52,739] [INFO] [timer.py:198:stop] epoch=0/micro_step=33/global_step=33, RunningAvgSamplesPerSec=65.92187576817432, CurrSamplesPerSec=65.99344282929918, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:31:52,740] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1267.15 | backward_microstep: 2517.79 | backward_inner_microstep: 2029.33 | backward_allreduce_microstep: 487.83 | step_microstep: 17.94
[2023-03-17 14:31:52,740] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1267.07 | backward: 2517.75 | backward_inner: 2029.37 | backward_allreduce: 487.85 | step: 17.94
 iteration       33/      50 | consumed samples:         8448 | consumed tokens:      8650752 | elapsed time per iteration (ms): 3817.7 | learning rate: 1.999E-05 | global batch size:    32 | lm loss: 7.258313E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.382 | TFLOPs: 19.75 |
time (ms) | forward-compute: 1279.53 | backward-compute: 2517.73 | backward-embedding-all-reduce: 0.01 | optimizer: 18.71 | batch-generator: 5.09
[2023-03-17 14:31:56,547] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.38
[2023-03-17 14:31:56,547] [INFO] [logging.py:93:log_dist] [Rank 0] step=34, skipped=0, lr=[1.853267653556708e-05, 1.853267653556708e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:31:56,547] [INFO] [timer.py:198:stop] epoch=0/micro_step=34/global_step=34, RunningAvgSamplesPerSec=65.87259896957907, CurrSamplesPerSec=64.3807305432306, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:31:56,548] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1271.21 | backward_microstep: 2503.57 | backward_inner_microstep: 2027.92 | backward_allreduce_microstep: 475.00 | step_microstep: 18.04
[2023-03-17 14:31:56,548] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1271.13 | backward: 2503.54 | backward_inner: 2027.97 | backward_allreduce: 475.03 | step: 18.04
 iteration       34/      50 | consumed samples:         8704 | consumed tokens:      8912896 | elapsed time per iteration (ms): 3807.8 | learning rate: 1.853E-05 | global batch size:    32 | lm loss: 7.126628E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.404 | TFLOPs: 19.80 |
time (ms) | forward-compute: 1284.09 | backward-compute: 2503.38 | backward-embedding-all-reduce: 0.01 | optimizer: 18.57 | batch-generator: 5.01
[2023-03-17 14:32:00,366] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.32
[2023-03-17 14:32:00,366] [INFO] [logging.py:93:log_dist] [Rank 0] step=35, skipped=0, lr=[1.712979818810323e-05, 1.712979818810323e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:32:00,366] [INFO] [timer.py:198:stop] epoch=0/micro_step=35/global_step=35, RunningAvgSamplesPerSec=65.85377682321626, CurrSamplesPerSec=65.25709601493227, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:32:00,367] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1268.95 | backward_microstep: 2516.54 | backward_inner_microstep: 2033.40 | backward_allreduce_microstep: 482.48 | step_microstep: 18.11
[2023-03-17 14:32:00,367] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1268.87 | backward: 2516.51 | backward_inner: 2033.46 | backward_allreduce: 482.51 | step: 18.12
 iteration       35/      50 | consumed samples:         8960 | consumed tokens:      9175040 | elapsed time per iteration (ms): 3819.1 | learning rate: 1.713E-05 | global batch size:    32 | lm loss: 7.180930E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.379 | TFLOPs: 19.74 |
time (ms) | forward-compute: 1281.71 | backward-compute: 2516.91 | backward-embedding-all-reduce: 0.01 | optimizer: 18.62 | batch-generator: 5.13
[2023-03-17 14:32:04,155] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.40
[2023-03-17 14:32:04,155] [INFO] [logging.py:93:log_dist] [Rank 0] step=36, skipped=0, lr=[1.5789552276785377e-05, 1.5789552276785377e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:32:04,156] [INFO] [timer.py:198:stop] epoch=0/micro_step=36/global_step=36, RunningAvgSamplesPerSec=65.85724226824766, CurrSamplesPerSec=65.97180692179933, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:32:04,156] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1270.03 | backward_microstep: 2486.88 | backward_inner_microstep: 2030.82 | backward_allreduce_microstep: 455.41 | step_microstep: 18.03
[2023-03-17 14:32:04,156] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1269.95 | backward: 2486.84 | backward_inner: 2030.87 | backward_allreduce: 455.43 | step: 18.03
 iteration       36/      50 | consumed samples:         9216 | consumed tokens:      9437184 | elapsed time per iteration (ms): 3789.3 | learning rate: 1.579E-05 | global batch size:    32 | lm loss: 7.213630E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.445 | TFLOPs: 19.90 |
time (ms) | forward-compute: 1281.55 | backward-compute: 2487.28 | backward-embedding-all-reduce: 0.01 | optimizer: 18.66 | batch-generator: 5.25
[2023-03-17 14:32:07,946] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.33
[2023-03-17 14:32:07,946] [INFO] [logging.py:93:log_dist] [Rank 0] step=37, skipped=0, lr=[1.4517228139925405e-05, 1.4517228139925405e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:32:07,947] [INFO] [timer.py:198:stop] epoch=0/micro_step=37/global_step=37, RunningAvgSamplesPerSec=65.8585488582895, CurrSamplesPerSec=65.90300378818567, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:32:07,947] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1272.25 | backward_microstep: 2485.84 | backward_inner_microstep: 2032.49 | backward_allreduce_microstep: 452.68 | step_microstep: 18.06
[2023-03-17 14:32:07,948] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1272.17 | backward: 2485.81 | backward_inner: 2032.56 | backward_allreduce: 452.69 | step: 18.07
 iteration       37/      50 | consumed samples:         9472 | consumed tokens:      9699328 | elapsed time per iteration (ms): 3791.1 | learning rate: 1.452E-05 | global batch size:    32 | lm loss: 7.165027E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.441 | TFLOPs: 19.89 |
time (ms) | forward-compute: 1284.54 | backward-compute: 2486.19 | backward-embedding-all-reduce: 0.01 | optimizer: 18.60 | batch-generator: 5.17
[2023-03-17 14:32:11,737] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.38
[2023-03-17 14:32:11,737] [INFO] [logging.py:93:log_dist] [Rank 0] step=38, skipped=0, lr=[1.3317847059621894e-05, 1.3317847059621894e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:32:11,737] [INFO] [timer.py:198:stop] epoch=0/micro_step=38/global_step=38, RunningAvgSamplesPerSec=65.8689433509755, CurrSamplesPerSec=66.23482919395654, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:32:11,738] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1268.47 | backward_microstep: 2487.97 | backward_inner_microstep: 2035.14 | backward_allreduce_microstep: 452.19 | step_microstep: 18.09
[2023-03-17 14:32:11,738] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1268.38 | backward: 2487.94 | backward_inner: 2035.19 | backward_allreduce: 452.20 | step: 18.09
 iteration       38/      50 | consumed samples:         9728 | consumed tokens:      9961472 | elapsed time per iteration (ms): 3790.6 | learning rate: 1.332E-05 | global batch size:    32 | lm loss: 7.242592E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.442 | TFLOPs: 19.89 |
time (ms) | forward-compute: 1281.46 | backward-compute: 2488.74 | backward-embedding-all-reduce: 0.01 | optimizer: 18.61 | batch-generator: 5.14
[2023-03-17 14:32:15,524] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.36
[2023-03-17 14:32:15,524] [INFO] [logging.py:93:log_dist] [Rank 0] step=39, skipped=0, lr=[1.2196142445053694e-05, 1.2196142445053694e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:32:15,525] [INFO] [timer.py:198:stop] epoch=0/micro_step=39/global_step=39, RunningAvgSamplesPerSec=65.86774740902663, CurrSamplesPerSec=65.82472240300105, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:32:15,525] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1264.93 | backward_microstep: 2489.36 | backward_inner_microstep: 2031.66 | backward_allreduce_microstep: 457.04 | step_microstep: 18.14
[2023-03-17 14:32:15,526] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1264.84 | backward: 2489.32 | backward_inner: 2031.72 | backward_allreduce: 457.06 | step: 18.14
 iteration       39/      50 | consumed samples:         9984 | consumed tokens:     10223616 | elapsed time per iteration (ms): 3787.3 | learning rate: 1.220E-05 | global batch size:    32 | lm loss: 7.226746E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.449 | TFLOPs: 19.91 |
time (ms) | forward-compute: 1277.29 | backward-compute: 2489.72 | backward-embedding-all-reduce: 0.01 | optimizer: 18.58 | batch-generator: 5.02
[2023-03-17 14:32:19,348] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.34
[2023-03-17 14:32:19,349] [INFO] [logging.py:93:log_dist] [Rank 0] step=40, skipped=0, lr=[1.1156541151876422e-05, 1.1156541151876422e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:32:19,349] [INFO] [timer.py:198:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=65.83121184059767, CurrSamplesPerSec=64.5073161916489, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:32:19,350] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1272.59 | backward_microstep: 2518.20 | backward_inner_microstep: 2034.07 | backward_allreduce_microstep: 483.46 | step_microstep: 17.96
[2023-03-17 14:32:19,350] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1272.51 | backward: 2518.18 | backward_inner: 2034.13 | backward_allreduce: 483.48 | step: 17.96
 iteration       40/      50 | consumed samples:        10240 | consumed tokens:     10485760 | elapsed time per iteration (ms): 3824.4 | learning rate: 1.116E-05 | global batch size:    32 | lm loss: 7.197243E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.367 | TFLOPs: 19.71 |
time (ms) | forward-compute: 1285.53 | backward-compute: 2518.50 | backward-embedding-all-reduce: 0.01 | optimizer: 18.64 | batch-generator: 4.92
[2023-03-17 14:32:23,170] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.35
[2023-03-17 14:32:23,170] [INFO] [logging.py:93:log_dist] [Rank 0] step=41, skipped=0, lr=[1.0203146011445599e-05, 1.0203146011445599e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:32:23,170] [INFO] [timer.py:198:stop] epoch=0/micro_step=41/global_step=41, RunningAvgSamplesPerSec=65.83109593177872, CurrSamplesPerSec=65.82669169908522, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:32:23,171] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1272.50 | backward_microstep: 2514.62 | backward_inner_microstep: 2030.91 | backward_allreduce_microstep: 483.04 | step_microstep: 18.17
[2023-03-17 14:32:23,171] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1272.42 | backward: 2514.59 | backward_inner: 2030.98 | backward_allreduce: 483.05 | step: 18.17
 iteration       41/      50 | consumed samples:        10496 | consumed tokens:     10747904 | elapsed time per iteration (ms): 3821.2 | learning rate: 1.020E-05 | global batch size:    32 | lm loss: 7.188511E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.374 | TFLOPs: 19.73 |
time (ms) | forward-compute: 1285.89 | backward-compute: 2514.95 | backward-embedding-all-reduce: 0.01 | optimizer: 18.63 | batch-generator: 4.83
[2023-03-17 14:32:27,015] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.36
[2023-03-17 14:32:27,016] [INFO] [logging.py:93:log_dist] [Rank 0] step=42, skipped=0, lr=[9.33971963881569e-06, 9.33971963881569e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:32:27,016] [INFO] [timer.py:198:stop] epoch=0/micro_step=42/global_step=42, RunningAvgSamplesPerSec=65.81080671259036, CurrSamplesPerSec=65.02916623464967, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:32:27,016] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1275.21 | backward_microstep: 2536.64 | backward_inner_microstep: 2038.05 | backward_allreduce_microstep: 497.94 | step_microstep: 17.96
[2023-03-17 14:32:27,017] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1275.12 | backward: 2536.62 | backward_inner: 2038.11 | backward_allreduce: 497.96 | step: 17.96
 iteration       42/      50 | consumed samples:        10752 | consumed tokens:     11010048 | elapsed time per iteration (ms): 3845.6 | learning rate: 9.340E-06 | global batch size:    32 | lm loss: 7.153430E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.321 | TFLOPs: 19.60 |
time (ms) | forward-compute: 1288.58 | backward-compute: 2536.64 | backward-embedding-all-reduce: 0.01 | optimizer: 18.61 | batch-generator: 5.08
[2023-03-17 14:32:30,844] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.35
[2023-03-17 14:32:30,844] [INFO] [logging.py:93:log_dist] [Rank 0] step=43, skipped=0, lr=[8.569669583417477e-06, 8.569669583417477e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:32:30,845] [INFO] [timer.py:198:stop] epoch=0/micro_step=43/global_step=43, RunningAvgSamplesPerSec=65.83419752184727, CurrSamplesPerSec=66.78366096294096, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:32:30,845] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1266.82 | backward_microstep: 2528.00 | backward_inner_microstep: 2027.00 | backward_allreduce_microstep: 500.32 | step_microstep: 18.16
[2023-03-17 14:32:30,845] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1266.74 | backward: 2527.97 | backward_inner: 2027.06 | backward_allreduce: 500.35 | step: 18.17
 iteration       43/      50 | consumed samples:        11008 | consumed tokens:     11272192 | elapsed time per iteration (ms): 3828.6 | learning rate: 8.570E-06 | global batch size:    32 | lm loss: 7.205225E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.358 | TFLOPs: 19.69 |
time (ms) | forward-compute: 1279.69 | backward-compute: 2528.57 | backward-embedding-all-reduce: 0.01 | optimizer: 18.59 | batch-generator: 5.03
[2023-03-17 14:32:34,667] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.36
[2023-03-17 14:32:34,667] [INFO] [logging.py:93:log_dist] [Rank 0] step=44, skipped=0, lr=[7.896034881017213e-06, 7.896034881017213e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:32:34,668] [INFO] [timer.py:198:stop] epoch=0/micro_step=44/global_step=44, RunningAvgSamplesPerSec=65.83237118779387, CurrSamplesPerSec=65.75757863778288, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:32:34,668] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1289.69 | backward_microstep: 2498.10 | backward_inner_microstep: 2024.18 | backward_allreduce_microstep: 473.25 | step_microstep: 18.07
[2023-03-17 14:32:34,668] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1289.61 | backward: 2498.07 | backward_inner: 2024.25 | backward_allreduce: 473.26 | step: 18.07
 iteration       44/      50 | consumed samples:        11264 | consumed tokens:     11534336 | elapsed time per iteration (ms): 3823.0 | learning rate: 7.896E-06 | global batch size:    32 | lm loss: 7.243112E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.370 | TFLOPs: 19.72 |
time (ms) | forward-compute: 1304.26 | backward-compute: 2498.50 | backward-embedding-all-reduce: 0.01 | optimizer: 18.54 | batch-generator: 4.88
[2023-03-17 14:32:38,479] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.36
[2023-03-17 14:32:38,480] [INFO] [logging.py:93:log_dist] [Rank 0] step=45, skipped=0, lr=[7.3214740600308545e-06, 7.3214740600308545e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:32:38,480] [INFO] [timer.py:198:stop] epoch=0/micro_step=45/global_step=45, RunningAvgSamplesPerSec=65.82530724836329, CurrSamplesPerSec=65.52998455219931, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:32:38,480] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1279.43 | backward_microstep: 2497.75 | backward_inner_microstep: 2025.89 | backward_allreduce_microstep: 471.19 | step_microstep: 18.30
[2023-03-17 14:32:38,481] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1279.35 | backward: 2497.72 | backward_inner: 2025.96 | backward_allreduce: 471.20 | step: 18.30
 iteration       45/      50 | consumed samples:        11520 | consumed tokens:     11796480 | elapsed time per iteration (ms): 3812.3 | learning rate: 7.321E-06 | global batch size:    32 | lm loss: 7.145763E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.394 | TFLOPs: 19.78 |
time (ms) | forward-compute: 1293.44 | backward-compute: 2498.37 | backward-embedding-all-reduce: 0.01 | optimizer: 18.71 | batch-generator: 5.00
[2023-03-17 14:32:42,273] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.38
[2023-03-17 14:32:42,274] [INFO] [logging.py:93:log_dist] [Rank 0] step=46, skipped=0, lr=[6.848254649526961e-06, 6.848254649526961e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:32:42,274] [INFO] [timer.py:198:stop] epoch=0/micro_step=46/global_step=46, RunningAvgSamplesPerSec=65.82299753143977, CurrSamplesPerSec=65.72383280929336, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:32:42,274] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1268.61 | backward_microstep: 2491.16 | backward_inner_microstep: 2035.46 | backward_allreduce_microstep: 455.02 | step_microstep: 18.48
[2023-03-17 14:32:42,275] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1268.53 | backward: 2491.13 | backward_inner: 2035.53 | backward_allreduce: 455.04 | step: 18.49
 iteration       46/      50 | consumed samples:        11776 | consumed tokens:     12058624 | elapsed time per iteration (ms): 3794.1 | learning rate: 6.848E-06 | global batch size:    32 | lm loss: 7.206333E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.434 | TFLOPs: 19.87 |
time (ms) | forward-compute: 1281.46 | backward-compute: 2491.85 | backward-embedding-all-reduce: 0.01 | optimizer: 19.00 | batch-generator: 5.03
[2023-03-17 14:32:46,068] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.36
[2023-03-17 14:32:46,068] [INFO] [logging.py:93:log_dist] [Rank 0] step=47, skipped=0, lr=[6.478244230325408e-06, 6.478244230325408e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:32:46,069] [INFO] [timer.py:198:stop] epoch=0/micro_step=47/global_step=47, RunningAvgSamplesPerSec=65.82084370064497, CurrSamplesPerSec=65.72621448890416, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:32:46,069] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1266.20 | backward_microstep: 2494.45 | backward_inner_microstep: 2027.86 | backward_allreduce_microstep: 465.94 | step_microstep: 18.14
[2023-03-17 14:32:46,070] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1266.12 | backward: 2494.42 | backward_inner: 2027.92 | backward_allreduce: 465.95 | step: 18.15
 iteration       47/      50 | consumed samples:        12032 | consumed tokens:     12320768 | elapsed time per iteration (ms): 3794.6 | learning rate: 6.478E-06 | global batch size:    32 | lm loss: 7.163276E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.433 | TFLOPs: 19.87 |
time (ms) | forward-compute: 1279.36 | backward-compute: 2494.83 | backward-embedding-all-reduce: 0.01 | optimizer: 18.66 | batch-generator: 5.01
[2023-03-17 14:32:49,887] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.35
[2023-03-17 14:32:49,887] [INFO] [logging.py:93:log_dist] [Rank 0] step=48, skipped=0, lr=[6.2129030645091e-06, 6.2129030645091e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:32:49,888] [INFO] [timer.py:198:stop] epoch=0/micro_step=48/global_step=48, RunningAvgSamplesPerSec=65.83081595247057, CurrSamplesPerSec=66.28271622497195, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:32:49,888] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1261.74 | backward_microstep: 2524.03 | backward_inner_microstep: 2041.51 | backward_allreduce_microstep: 481.87 | step_microstep: 17.97
[2023-03-17 14:32:49,889] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1261.65 | backward: 2524.00 | backward_inner: 2041.56 | backward_allreduce: 481.89 | step: 17.97
 iteration       48/      50 | consumed samples:        12288 | consumed tokens:     12582912 | elapsed time per iteration (ms): 3819.1 | learning rate: 6.213E-06 | global batch size:    32 | lm loss: 7.193553E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.379 | TFLOPs: 19.74 |
time (ms) | forward-compute: 1274.50 | backward-compute: 2524.16 | backward-embedding-all-reduce: 0.01 | optimizer: 18.61 | batch-generator: 5.20
[2023-03-17 14:32:53,718] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.34
[2023-03-17 14:32:53,719] [INFO] [logging.py:93:log_dist] [Rank 0] step=49, skipped=0, lr=[6.053278332436668e-06, 6.053278332436668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:32:53,719] [INFO] [timer.py:198:stop] epoch=0/micro_step=49/global_step=49, RunningAvgSamplesPerSec=65.78834494303125, CurrSamplesPerSec=63.89221065459444, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:32:53,719] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1276.61 | backward_microstep: 2522.02 | backward_inner_microstep: 2029.19 | backward_allreduce_microstep: 492.16 | step_microstep: 18.06
[2023-03-17 14:32:53,720] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1276.53 | backward: 2521.99 | backward_inner: 2029.26 | backward_allreduce: 492.17 | step: 18.07
 iteration       49/      50 | consumed samples:        12544 | consumed tokens:     12845056 | elapsed time per iteration (ms): 3831.2 | learning rate: 6.053E-06 | global batch size:    32 | lm loss: 7.179792E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.352 | TFLOPs: 19.68 |
time (ms) | forward-compute: 1288.69 | backward-compute: 2522.21 | backward-embedding-all-reduce: 0.01 | optimizer: 18.54 | batch-generator: 5.10
[2023-03-17 14:32:57,541] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 11.38
[2023-03-17 14:32:57,541] [INFO] [logging.py:93:log_dist] [Rank 0] step=50, skipped=0, lr=[6e-06, 6e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 14:32:57,541] [INFO] [timer.py:198:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=65.78096691824713, CurrSamplesPerSec=65.43605664785426, MemAllocated=4.72GB, MaxMemAllocated=15.67GB
[2023-03-17 14:32:57,542] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 1278.98 | backward_microstep: 2510.09 | backward_inner_microstep: 2032.04 | backward_allreduce_microstep: 477.38 | step_microstep: 17.96
[2023-03-17 14:32:57,542] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 1278.90 | backward: 2510.06 | backward_inner: 2032.11 | backward_allreduce: 477.40 | step: 17.96
 iteration       50/      50 | consumed samples:        12800 | consumed tokens:     13107200 | elapsed time per iteration (ms): 3822.4 | learning rate: 6.000E-06 | global batch size:    32 | lm loss: 7.173653E+00 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.372 | TFLOPs: 19.72 |
time (ms) | forward-compute: 1291.50 | backward-compute: 2510.39 | backward-embedding-all-reduce: 0.01 | optimizer: 18.69 | batch-generator: 5.32
[after training is done] datetime: 2023-03-17 14:32:57 
------------------------------------------------------------------------------------------------------------------
 validation loss at the end of training for val data | lm loss value: 7.349543E+00 | lm loss PPL: 1.555486E+03 | 
------------------------------------------------------------------------------------------------------------------
gpu108-09-l:50445:50923 [0] NCCL INFO [Service thread] Connection closed by localRank 0
gpu108-16-r:61913:62330 [2] NCCL INFO [Service thread] Connection closed by localRank 2
gpu108-16-r:61911:62328 [0] NCCL INFO [Service thread] Connection closed by localRank 0
gpu108-16-r:61914:62329 [3] NCCL INFO [Service thread] Connection closed by localRank 3
gpu108-16-r:61912:62331 [1] NCCL INFO [Service thread] Connection closed by localRank 1
gpu108-09-l:50447:50926 [2] NCCL INFO [Service thread] Connection closed by localRank 2
gpu108-09-l:50448:50925 [3] NCCL INFO [Service thread] Connection closed by localRank 3
gpu108-09-l:50446:50924 [1] NCCL INFO [Service thread] Connection closed by localRank 1
gpu108-16-r:61913:61913 [2] NCCL INFO comm 0x3bb40b70 rank 6 nranks 8 cudaDev 2 busId 85000 - Abort COMPLETE
gpu108-09-l:50445:50445 [0] NCCL INFO comm 0x3a07ea30 rank 0 nranks 8 cudaDev 0 busId 7000 - Abort COMPLETE
gpu108-16-r:61911:61911 [0] NCCL INFO comm 0x39660c10 rank 4 nranks 8 cudaDev 0 busId 7000 - Abort COMPLETE
gpu108-16-r:61912:61912 [1] NCCL INFO comm 0x3b52f2b0 rank 5 nranks 8 cudaDev 1 busId 46000 - Abort COMPLETE
gpu108-16-r:61914:61914 [3] NCCL INFO comm 0x3b65b0c0 rank 7 nranks 8 cudaDev 3 busId c7000 - Abort COMPLETE
gpu108-09-l:50447:50447 [2] NCCL INFO comm 0x39a80100 rank 2 nranks 8 cudaDev 2 busId 85000 - Abort COMPLETE
gpu108-09-l:50446:50446 [1] NCCL INFO comm 0x3a4e24f0 rank 1 nranks 8 cudaDev 1 busId 46000 - Abort COMPLETE
gpu108-09-l:50448:50448 [3] NCCL INFO comm 0x399e2640 rank 3 nranks 8 cudaDev 3 busId c7000 - Abort COMPLETE
