------- JOB Configuration ---------
scontrol show job 24296264
JobId=24296264 JobName=g4
   UserId=shaima0d(174988) GroupId=g-shaima0d(1174988) MCS_label=N/A
   Priority=9697 Nice=0 Account=a100_training_acc QOS=a100_training_qos
   JobState=RUNNING Reason=None Dependency=(null)
   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=00:00:00 TimeLimit=00:15:00 TimeMin=N/A
   SubmitTime=2023-03-17T12:12:27 EligibleTime=2023-03-17T12:12:27
   AccrueTime=2023-03-17T12:12:27
   StartTime=2023-03-17T12:12:27 EndTime=2023-03-17T12:27:27 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2023-03-17T12:12:27 Scheduler=Main
   Partition=a100_training AllocNode:Sid=login510-27:152451
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=gpu108-23-l,gpu108-23-r
   BatchHost=gpu108-23-l
   NumNodes=2 NumCPUs=120 NumTasks=8 CPUs/Task=15 ReqB:S:C:T=0:0:*:*
   TRES=cpu=120,mem=240G,node=2,billing=120,gres/gpu=8
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
   MinCPUsNode=15 MinMemoryCPU=2G MinTmpDiskNode=0
   Features=(a100)&el7 DelayBoot=00:00:00
   Reservation=A100
   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
   Command=/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/G8N2.slurm
   WorkDir=/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed
   StdErr=/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/slurm-24296264.out
   StdIn=/dev/null
   StdOut=/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/slurm-24296264.out
   Power=
   TresPerJob=gres:gpu:8
   TresPerNode=gres:gpu:4
   

------- GPU Configuration ---------
nvidia-smi -L
GPU 0: NVIDIA A100-SXM4-80GB (UUID: GPU-f6478d10-0ba0-3194-0595-a91ce84bec28)
GPU 1: NVIDIA A100-SXM4-80GB (UUID: GPU-4d84d3cc-a6cc-59d3-9289-f19830776190)
GPU 2: NVIDIA A100-SXM4-80GB (UUID: GPU-da60fc37-921e-1556-d1cb-9c227f05435d)
GPU 3: NVIDIA A100-SXM4-80GB (UUID: GPU-c07dd77b-3ff7-dd4a-f2ed-9b6266da187a)
------- NVLink Configuration ------
nvidia-smi topo -m
	[4mGPU0	GPU1	GPU2	GPU3	mlx5_0	mlx5_1	CPU Affinity	NUMA Affinity[0m
GPU0	 X 	NV4	NV4	NV4	NODE	SYS	32-59	1
GPU1	NV4	 X 	NV4	NV4	PHB	SYS	32-59	1
GPU2	NV4	NV4	 X 	NV4	SYS	NODE	0-31	0
GPU3	NV4	NV4	NV4	 X 	SYS	PHB	0-31	0
mlx5_0	NODE	PHB	SYS	SYS	 X 	SYS		
mlx5_1	SYS	SYS	NODE	PHB	SYS	 X 		

Legend:

  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing at most a single PCIe bridge
  NV#  = Connection traversing a bonded set of # NVLinks
------- Infiniband Configuration --
ibv_devinfo
hca_id:	mlx5_0
	transport:			InfiniBand (0)
	fw_ver:				20.34.1002
	node_guid:			88e9:a4ff:ff1a:5eec
	sys_image_guid:			88e9:a4ff:ff1a:5eec
	vendor_id:			0x02c9
	vendor_part_id:			4123
	hw_ver:				0x0
	board_id:			MT_0000000451
	phys_port_cnt:			1
		port:	1
			state:			PORT_ACTIVE (4)
			max_mtu:		4096 (5)
			active_mtu:		4096 (5)
			sm_lid:			1
			port_lid:		673
			port_lmc:		0x00
			link_layer:		InfiniBand

hca_id:	mlx5_1
	transport:			InfiniBand (0)
	fw_ver:				20.34.1002
	node_guid:			88e9:a4ff:ff1a:5ec8
	sys_image_guid:			88e9:a4ff:ff1a:5ec8
	vendor_id:			0x02c9
	vendor_part_id:			4123
	hw_ver:				0x0
	board_id:			MT_0000000451
	phys_port_cnt:			1
		port:	1
			state:			PORT_ACTIVE (4)
			max_mtu:		4096 (5)
			active_mtu:		4096 (5)
			sm_lid:			1
			port_lid:		672
			port_lmc:		0x00
			link_layer:		InfiniBand

Loading module for CUDA 11.7.0
CUDA 11.7.0 is now loaded
GNU 11.1.0 is now loaded
Loading module for nccl-2.17.1.1
nccl-2.17.1.1 modules now loaded
Loading module for gdrcopy 2.0_cuda11.7.0
gdrcopy 2.0_cuda11.7.0 modules now loaded
Loading module for ucx-gpu 1.14.0
ucx-gpu 1.14.0 modules now loaded
Loading module for OPENMPI 4.1.4
OPENMPI 4.1.4 modules now loaded
Loading module for pytorch-1.13.1_cuda11.7.0
pytorch-1.13.1_cuda11.7.0 modules now loaded
Loading module for deepspeed-0.8.3
deepspeed-0.8.3 modules now loaded
Loading module for apex-22.03
apex-22.03 modules now loaded
Currently Loaded Modulefiles:
  1) dl/2023             5) nccl/2.17.1.1       9) pytorch/1.13.1
  2) python/3.9.16       6) gdrcopy/2.0        10) deepspeed/0.8.3
  3) cuda/11.7.0         7) ucx/1.14.0         11) apex/22.03
  4) gcc/11.1.0          8) openmpi-gpu/4.1.4
[2023-03-17 12:12:31,344] [INFO] [runner.py:550:main] cmd = srun -n 8 --nodes 2 --gpus 8 /sw/csgv/dl/apps/python/3.9.16/bin/python3.9 -u pretrain_gpt.py --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 1 --hidden-size 1024 --num-attention-heads 32 --seq-length 1024 --loss-scale 15 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 32 --train-iters 50 --lr 6.0e-5 --min-lr 6.0e-6 --lr-decay-style cosine --log-interval 1 --eval-iters 40 --eval-interval 1000 --data-path /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document --vocab-file /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json --merge-file /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt --save-interval 1000 --split 98,2,0 --clip-grad 1.0 --weight-decay 0.1 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.006 --fp16 --checkpoint-activations --tensorboard-dir ds_z_off-none_stage_3_nl1_hs1024_mb4_seq1024_gb32_nodes2 --deepspeed-activation-checkpointing --zero-stage=3 --deepspeed_config=ds_config.json --no-pipeline-parallel --deepspeed --no-masked-softmax-fusion --no-bias-dropout-fusion --no-bias-gelu-fusion --exit-interval 5000
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [93m[NO][0m
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [93m[NO][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. False
  bias_gelu_fusion ................................ False
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  data_parallel_size .............................. 1
  data_path ....................................... ['/ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 40
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 5000
  expert_interval ................................. 2
  ffn_hidden_size ................................. 4096
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 1024
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 32
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 15.0
  loss_scale_window ............................... 1000
  lr .............................................. 6e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ None
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... False
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 32
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 1
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. ds_z_off-none_stage_3_nl1_hs1024_mb4_seq1024_gb32_nodes2
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 50
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 1
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 3
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. False
  bias_gelu_fusion ................................ False
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  data_parallel_size .............................. 1
  data_path ....................................... ['/ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 40
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 5000
  expert_interval ................................. 2
  ffn_hidden_size ................................. 4096
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 1024
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 32
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 15.0
  loss_scale_window ............................... 1000
  lr .............................................. 6e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ None
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... False
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 32
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 1
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. ds_z_off-none_stage_3_nl1_hs1024_mb4_seq1024_gb32_nodes2
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 50
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 1
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 3
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. False
  bias_gelu_fusion ................................ False
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  data_parallel_size .............................. 1
  data_path ....................................... ['/ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 40
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 5000
  expert_interval ................................. 2
  ffn_hidden_size ................................. 4096
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 1024
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 32
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 15.0
  loss_scale_window ............................... 1000
  lr .............................................. 6e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ None
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... False
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 32
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 1
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. ds_z_off-none_stage_3_nl1_hs1024_mb4_seq1024_gb32_nodes2
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 50
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 1
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 3
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. False
  bias_gelu_fusion ................................ False
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  data_parallel_size .............................. 1
  data_path ....................................... ['/ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 40
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 5000
  expert_interval ................................. 2
  ffn_hidden_size ................................. 4096
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 1024
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 32
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 15.0
  loss_scale_window ............................... 1000
  lr .............................................. 6e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ None
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... False
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 32
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 1
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. ds_z_off-none_stage_3_nl1_hs1024_mb4_seq1024_gb32_nodes2
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 50
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 1
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 3
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> setting tensorboard ...
> setting tensorboard ...
> setting tensorboard ...
> setting tensorboard ...
[2023-03-17 12:12:35,328] [INFO] [comm.py:634:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-03-17 12:12:35,328] [INFO] [comm.py:634:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-03-17 12:12:35,329] [INFO] [comm.py:634:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-03-17 12:12:35,329] [INFO] [comm.py:634:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. False
  bias_gelu_fusion ................................ False
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  data_parallel_size .............................. 1
  data_path ....................................... ['/ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 40
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 5000
  expert_interval ................................. 2
  ffn_hidden_size ................................. 4096
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 1024
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 32
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 15.0
  loss_scale_window ............................... 1000
  lr .............................................. 6e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ None
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... False
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 32
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 1
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. ds_z_off-none_stage_3_nl1_hs1024_mb4_seq1024_gb32_nodes2
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 50
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 1
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 3
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. False
  bias_gelu_fusion ................................ False
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  data_parallel_size .............................. 1
  data_path ....................................... ['/ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 40
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 5000
  expert_interval ................................. 2
  ffn_hidden_size ................................. 4096
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 1024
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 32
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 15.0
  loss_scale_window ............................... 1000
  lr .............................................. 6e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ None
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... False
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 32
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 1
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. ds_z_off-none_stage_3_nl1_hs1024_mb4_seq1024_gb32_nodes2
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 50
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 1
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 3
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. False
  bias_gelu_fusion ................................ False
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  data_parallel_size .............................. 1
  data_path ....................................... ['/ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 40
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 5000
  expert_interval ................................. 2
  ffn_hidden_size ................................. 4096
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 1024
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 32
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 15.0
  loss_scale_window ............................... 1000
  lr .............................................. 6e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ None
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... False
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 32
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 1
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. ds_z_off-none_stage_3_nl1_hs1024_mb4_seq1024_gb32_nodes2
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 50
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 1
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 3
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. False
  bias_gelu_fusion ................................ False
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  data_parallel_size .............................. 1
  data_path ....................................... ['/ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 40
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 5000
  expert_interval ................................. 2
  ffn_hidden_size ................................. 4096
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 1024
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 32
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 15.0
  loss_scale_window ............................... 1000
  lr .............................................. 6e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ None
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... False
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 32
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 1
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. ds_z_off-none_stage_3_nl1_hs1024_mb4_seq1024_gb32_nodes2
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 50
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 1
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 3
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> setting tensorboard ...
> setting tensorboard ...
> setting tensorboard ...
> setting tensorboard ...
[2023-03-17 12:12:37,050] [INFO] [comm.py:634:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-03-17 12:12:37,066] [INFO] [comm.py:634:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-03-17 12:12:37,066] [INFO] [comm.py:634:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-03-17 12:12:37,067] [INFO] [comm.py:634:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-03-17 12:12:37,728] [INFO] [comm.py:688:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=8, master_addr=10.109.8.133, master_port=29500
[2023-03-17 12:12:37,728] [INFO] [comm.py:688:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=8, master_addr=10.109.8.133, master_port=29500
[2023-03-17 12:12:37,728] [INFO] [comm.py:688:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=0, world_size=8, master_addr=10.109.8.133, master_port=29500
[2023-03-17 12:12:37,728] [INFO] [comm.py:688:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=8, master_addr=10.109.8.133, master_port=29500
[2023-03-17 12:12:37,728] [INFO] [comm.py:688:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=8, master_addr=10.109.8.133, master_port=29500
[2023-03-17 12:12:37,728] [INFO] [comm.py:688:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=3, world_size=8, master_addr=10.109.8.133, master_port=29500
[2023-03-17 12:12:37,729] [INFO] [comm.py:652:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-03-17 12:12:37,730] [INFO] [comm.py:688:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=1, world_size=8, master_addr=10.109.8.133, master_port=29500
[2023-03-17 12:12:37,732] [INFO] [comm.py:688:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=2, world_size=8, master_addr=10.109.8.133, master_port=29500
Hi, I am 0 and pinning GPU 0
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
Hi, I am 5 and pinning GPU 1
Hi, I am 6 and pinning GPU 2
Hi, I am 2 and pinning GPU 2
Hi, I am 1 and pinning GPU 1
Hi, I am 3 and pinning GPU 3
Hi, I am 4 and pinning GPU 0
Hi, I am 7 and pinning GPU 3
Helloworld from 1 1
> setting random seeds to 1234 ...
[2023-03-17 12:12:39,027] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> compiling dataset index builder ...
Helloworld from 5 1
Helloworld from 6 2
Helloworld from 7 3
Helloworld from 4 0
Helloworld from 2 2
Helloworld from 3 3
make: Entering directory `/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/megatron/data'
make: Nothing to be done for `default'.
make: Leaving directory `/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/megatron/data'
>>> done with dataset index builder. Compilation time: 0.041 seconds
WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (15) as the number of workers...
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
gpu108-23-l:42417:42417 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-23-l:42417:42417 [0] NCCL INFO Bootstrap : Using ib0:10.109.136.133<0>
gpu108-23-l:42417:42417 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu108-23-l:42417:42417 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu108-23-l:42417:42417 [0] NCCL INFO cudaDriverVersion 11080
NCCL version 2.17.1+cuda11.7
gpu108-23-l:42419:42419 [2] NCCL INFO cudaDriverVersion 11080
gpu108-23-l:42419:42419 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-23-l:42419:42419 [2] NCCL INFO Bootstrap : Using ib0:10.109.136.133<0>
gpu108-23-r:64496:64496 [0] NCCL INFO cudaDriverVersion 11080
gpu108-23-l:42419:42419 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu108-23-l:42419:42419 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu108-23-r:64497:64497 [1] NCCL INFO cudaDriverVersion 11080
gpu108-23-l:42418:42418 [1] NCCL INFO cudaDriverVersion 11080
gpu108-23-r:64497:64497 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-23-l:42420:42420 [3] NCCL INFO cudaDriverVersion 11080
gpu108-23-r:64496:64496 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-23-l:42418:42418 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-23-l:42420:42420 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-23-r:64497:64497 [1] NCCL INFO Bootstrap : Using ib0:10.109.136.134<0>
gpu108-23-r:64496:64496 [0] NCCL INFO Bootstrap : Using ib0:10.109.136.134<0>
gpu108-23-r:64496:64496 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu108-23-r:64496:64496 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu108-23-r:64497:64497 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu108-23-r:64497:64497 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu108-23-l:42418:42418 [1] NCCL INFO Bootstrap : Using ib0:10.109.136.133<0>
gpu108-23-l:42420:42420 [3] NCCL INFO Bootstrap : Using ib0:10.109.136.133<0>
gpu108-23-l:42418:42418 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu108-23-l:42418:42418 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu108-23-l:42420:42420 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu108-23-l:42420:42420 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu108-23-r:64498:64498 [2] NCCL INFO cudaDriverVersion 11080
gpu108-23-r:64498:64498 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-23-r:64498:64498 [2] NCCL INFO Bootstrap : Using ib0:10.109.136.134<0>
gpu108-23-r:64498:64498 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu108-23-r:64498:64498 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu108-23-r:64499:64499 [3] NCCL INFO cudaDriverVersion 11080
gpu108-23-r:64499:64499 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-23-r:64499:64499 [3] NCCL INFO Bootstrap : Using ib0:10.109.136.134<0>
gpu108-23-r:64499:64499 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu108-23-r:64499:64499 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu108-23-l:42417:42873 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-23-l:42420:42874 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-23-l:42419:42875 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-23-l:42418:42877 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-23-r:64496:64892 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-23-l:42417:42873 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.136.133<0>
gpu108-23-l:42417:42873 [0] NCCL INFO Using network IB
gpu108-23-l:42420:42874 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.136.133<0>
gpu108-23-l:42420:42874 [3] NCCL INFO Using network IB
gpu108-23-l:42419:42875 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.136.133<0>
gpu108-23-l:42419:42875 [2] NCCL INFO Using network IB
gpu108-23-l:42418:42877 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.136.133<0>
gpu108-23-l:42418:42877 [1] NCCL INFO Using network IB
gpu108-23-r:64499:64893 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-23-r:64497:64894 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-23-r:64498:64895 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-23-r:64496:64892 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.136.134<0>
gpu108-23-r:64496:64892 [0] NCCL INFO Using network IB
gpu108-23-r:64499:64893 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.136.134<0>
gpu108-23-r:64499:64893 [3] NCCL INFO Using network IB
gpu108-23-r:64497:64894 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.136.134<0>
gpu108-23-r:64497:64894 [1] NCCL INFO Using network IB
gpu108-23-r:64498:64895 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.136.134<0>
gpu108-23-r:64498:64895 [2] NCCL INFO Using network IB
gpu108-23-r:64499:64893 [3] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu108-23-r:64499:64893 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff
gpu108-23-l:42420:42874 [3] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu108-23-l:42420:42874 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff
gpu108-23-r:64497:64894 [1] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu108-23-r:64496:64892 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu108-23-r:64497:64894 [1] NCCL INFO Setting affinity for GPU 1 to 0fffffff,00000000
gpu108-23-r:64496:64892 [0] NCCL INFO Setting affinity for GPU 0 to 0fffffff,00000000
gpu108-23-r:64498:64895 [2] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu108-23-r:64498:64895 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff
gpu108-23-l:42418:42877 [1] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu108-23-l:42417:42873 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-l:42418:42877 [1] NCCL INFO Setting affinity for GPU 1 to 0fffffff,00000000
gpu108-23-l:42419:42875 [2] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu108-23-l:42419:42875 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff
gpu108-23-l:42417:42873 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu108-23-l:42417:42873 [0] NCCL INFO Setting affinity for GPU 0 to 0fffffff,00000000
gpu108-23-l:42417:42873 [0] NCCL INFO Channel 00/04 :    0   3   2   1   4   7   6   5
gpu108-23-l:42417:42873 [0] NCCL INFO Channel 01/04 :    0   3   6   5   4   7   2   1
gpu108-23-l:42418:42877 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 0/-1/-1->1->2 [2] 2/-1/-1->1->0 [3] 0/-1/-1->1->2
gpu108-23-l:42418:42877 [1] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:64498:64895 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 5/-1/-1->6->7 [2] 7/-1/-1->6->5 [3] 5/-1/-1->6->7
gpu108-23-r:64498:64895 [2] NCCL INFO P2P Chunksize set to 131072
gpu108-23-l:42417:42873 [0] NCCL INFO Channel 02/04 :    0   3   2   1   4   7   6   5
gpu108-23-l:42417:42873 [0] NCCL INFO Channel 03/04 :    0   3   6   5   4   7   2   1
gpu108-23-l:42417:42873 [0] NCCL INFO Trees [0] 1/4/-1->0->-1 [1] -1/-1/-1->0->1 [2] 1/-1/-1->0->4 [3] -1/-1/-1->0->1
gpu108-23-l:42417:42873 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-23-l:42419:42875 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 1/-1/-1->2->3 [2] 3/-1/-1->2->1 [3] 1/-1/-1->2->3
gpu108-23-l:42419:42875 [2] NCCL INFO P2P Chunksize set to 131072
gpu108-23-l:42420:42874 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 2/7/-1->3->-1 [2] -1/-1/-1->3->2 [3] 2/-1/-1->3->7
gpu108-23-l:42420:42874 [3] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:64499:64893 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 6/-1/-1->7->3 [2] -1/-1/-1->7->6 [3] 6/3/-1->7->-1
gpu108-23-r:64499:64893 [3] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:64496:64892 [0] NCCL INFO Trees [0] 5/-1/-1->4->0 [1] -1/-1/-1->4->5 [2] 5/0/-1->4->-1 [3] -1/-1/-1->4->5
gpu108-23-r:64496:64892 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:64497:64894 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 4/-1/-1->5->6 [2] 6/-1/-1->5->4 [3] 4/-1/-1->5->6
gpu108-23-r:64497:64894 [1] NCCL INFO P2P Chunksize set to 131072
gpu108-23-l:42418:42877 [1] NCCL INFO Channel 00/0 : 1[46000] -> 4[7000] [send] via NET/IB/0/GDRDMA
gpu108-23-r:64496:64892 [0] NCCL INFO Channel 00/0 : 1[46000] -> 4[7000] [receive] via NET/IB/0/GDRDMA
gpu108-23-r:64497:64894 [1] NCCL INFO Channel 00/0 : 5[46000] -> 0[7000] [send] via NET/IB/0/GDRDMA
gpu108-23-r:64498:64895 [2] NCCL INFO Channel 01/0 : 3[c7000] -> 6[85000] [receive] via NET/IB/1/GDRDMA
gpu108-23-l:42417:42873 [0] NCCL INFO Channel 00/0 : 5[46000] -> 0[7000] [receive] via NET/IB/0/GDRDMA
gpu108-23-l:42419:42875 [2] NCCL INFO Channel 01/0 : 7[c7000] -> 2[85000] [receive] via NET/IB/1/GDRDMA
gpu108-23-l:42418:42877 [1] NCCL INFO Channel 02/0 : 1[46000] -> 4[7000] [send] via NET/IB/0/GDRDMA
gpu108-23-r:64496:64892 [0] NCCL INFO Channel 02/0 : 1[46000] -> 4[7000] [receive] via NET/IB/0/GDRDMA
gpu108-23-r:64496:64892 [0] NCCL INFO Channel 00/0 : 4[7000] -> 7[c7000] via P2P/IPC/read
gpu108-23-r:64497:64894 [1] NCCL INFO Channel 02/0 : 5[46000] -> 0[7000] [send] via NET/IB/0/GDRDMA
gpu108-23-r:64498:64895 [2] NCCL INFO Channel 03/0 : 3[c7000] -> 6[85000] [receive] via NET/IB/1/GDRDMA
gpu108-23-l:42419:42875 [2] NCCL INFO Channel 03/0 : 7[c7000] -> 2[85000] [receive] via NET/IB/1/GDRDMA
gpu108-23-l:42417:42873 [0] NCCL INFO Channel 02/0 : 5[46000] -> 0[7000] [receive] via NET/IB/0/GDRDMA
gpu108-23-l:42417:42873 [0] NCCL INFO Channel 00/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:64496:64892 [0] NCCL INFO Channel 01/0 : 4[7000] -> 7[c7000] via P2P/IPC/read
gpu108-23-l:42417:42873 [0] NCCL INFO Channel 01/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:64496:64892 [0] NCCL INFO Channel 02/0 : 4[7000] -> 7[c7000] via P2P/IPC/read
gpu108-23-l:42417:42873 [0] NCCL INFO Channel 02/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-l:42420:42874 [3] NCCL INFO Channel 01/0 : 3[c7000] -> 6[85000] [send] via NET/IB/1/GDRDMA
gpu108-23-r:64499:64893 [3] NCCL INFO Channel 01/0 : 7[c7000] -> 2[85000] [send] via NET/IB/1/GDRDMA
gpu108-23-r:64496:64892 [0] NCCL INFO Channel 03/0 : 4[7000] -> 7[c7000] via P2P/IPC/read
gpu108-23-l:42417:42873 [0] NCCL INFO Channel 03/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-l:42420:42874 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 6[85000] [send] via NET/IB/1/GDRDMA
gpu108-23-l:42420:42874 [3] NCCL INFO Channel 00/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:64499:64893 [3] NCCL INFO Channel 03/0 : 7[c7000] -> 2[85000] [send] via NET/IB/1/GDRDMA
gpu108-23-r:64499:64893 [3] NCCL INFO Channel 00/0 : 7[c7000] -> 6[85000] via P2P/IPC/read
gpu108-23-l:42420:42874 [3] NCCL INFO Channel 02/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:64499:64893 [3] NCCL INFO Channel 02/0 : 7[c7000] -> 6[85000] via P2P/IPC/read
gpu108-23-l:42419:42875 [2] NCCL INFO Channel 00/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:64498:64895 [2] NCCL INFO Channel 00/0 : 6[85000] -> 5[46000] via P2P/IPC/read
gpu108-23-l:42419:42875 [2] NCCL INFO Channel 01/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:64498:64895 [2] NCCL INFO Channel 01/0 : 6[85000] -> 5[46000] via P2P/IPC/read
gpu108-23-l:42418:42877 [1] NCCL INFO Channel 01/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:64497:64894 [1] NCCL INFO Channel 01/0 : 5[46000] -> 4[7000] via P2P/IPC/read
gpu108-23-l:42419:42875 [2] NCCL INFO Channel 02/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:64498:64895 [2] NCCL INFO Channel 02/0 : 6[85000] -> 5[46000] via P2P/IPC/read
gpu108-23-l:42418:42877 [1] NCCL INFO Channel 03/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:64497:64894 [1] NCCL INFO Channel 03/0 : 5[46000] -> 4[7000] via P2P/IPC/read
gpu108-23-l:42419:42875 [2] NCCL INFO Channel 03/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:64498:64895 [2] NCCL INFO Channel 03/0 : 6[85000] -> 5[46000] via P2P/IPC/read
gpu108-23-l:42418:42877 [1] NCCL INFO Connected all rings
gpu108-23-l:42420:42874 [3] NCCL INFO Connected all rings
gpu108-23-r:64497:64894 [1] NCCL INFO Connected all rings
gpu108-23-r:64499:64893 [3] NCCL INFO Connected all rings
gpu108-23-l:42417:42873 [0] NCCL INFO Connected all rings
gpu108-23-l:42417:42873 [0] NCCL INFO Channel 00/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:64496:64892 [0] NCCL INFO Connected all rings
gpu108-23-r:64496:64892 [0] NCCL INFO Channel 00/0 : 4[7000] -> 5[46000] via P2P/IPC/read
gpu108-23-r:64498:64895 [2] NCCL INFO Connected all rings
gpu108-23-l:42419:42875 [2] NCCL INFO Connected all rings
gpu108-23-l:42417:42873 [0] NCCL INFO Channel 01/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:64496:64892 [0] NCCL INFO Channel 01/0 : 4[7000] -> 5[46000] via P2P/IPC/read
gpu108-23-l:42417:42873 [0] NCCL INFO Channel 02/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:64496:64892 [0] NCCL INFO Channel 02/0 : 4[7000] -> 5[46000] via P2P/IPC/read
gpu108-23-l:42417:42873 [0] NCCL INFO Channel 03/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:64496:64892 [0] NCCL INFO Channel 03/0 : 4[7000] -> 5[46000] via P2P/IPC/read
gpu108-23-l:42418:42877 [1] NCCL INFO Channel 00/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:64497:64894 [1] NCCL INFO Channel 00/0 : 5[46000] -> 6[85000] via P2P/IPC/read
gpu108-23-l:42419:42875 [2] NCCL INFO Channel 00/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:64498:64895 [2] NCCL INFO Channel 00/0 : 6[85000] -> 7[c7000] via P2P/IPC/read
gpu108-23-l:42418:42877 [1] NCCL INFO Channel 01/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:64497:64894 [1] NCCL INFO Channel 01/0 : 5[46000] -> 6[85000] via P2P/IPC/read
gpu108-23-l:42419:42875 [2] NCCL INFO Channel 01/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:64498:64895 [2] NCCL INFO Channel 01/0 : 6[85000] -> 7[c7000] via P2P/IPC/read
gpu108-23-l:42418:42877 [1] NCCL INFO Channel 02/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:64497:64894 [1] NCCL INFO Channel 02/0 : 5[46000] -> 6[85000] via P2P/IPC/read
gpu108-23-l:42419:42875 [2] NCCL INFO Channel 02/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:64498:64895 [2] NCCL INFO Channel 02/0 : 6[85000] -> 7[c7000] via P2P/IPC/read
gpu108-23-l:42418:42877 [1] NCCL INFO Channel 03/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:64497:64894 [1] NCCL INFO Channel 03/0 : 5[46000] -> 6[85000] via P2P/IPC/read
gpu108-23-l:42419:42875 [2] NCCL INFO Channel 03/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:64498:64895 [2] NCCL INFO Channel 03/0 : 6[85000] -> 7[c7000] via P2P/IPC/read
gpu108-23-l:42418:42877 [1] NCCL INFO Channel 00/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:64497:64894 [1] NCCL INFO Channel 00/0 : 5[46000] -> 4[7000] via P2P/IPC/read
gpu108-23-l:42417:42873 [0] NCCL INFO Channel 00/0 : 4[7000] -> 0[7000] [receive] via NET/IB/0/GDRDMA
gpu108-23-r:64496:64892 [0] NCCL INFO Channel 00/0 : 0[7000] -> 4[7000] [receive] via NET/IB/0/GDRDMA
gpu108-23-l:42418:42877 [1] NCCL INFO Channel 02/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-l:42420:42874 [3] NCCL INFO Channel 01/0 : 7[c7000] -> 3[c7000] [receive] via NET/IB/1/GDRDMA
gpu108-23-r:64497:64894 [1] NCCL INFO Channel 02/0 : 5[46000] -> 4[7000] via P2P/IPC/read
gpu108-23-r:64499:64893 [3] NCCL INFO Channel 01/0 : 3[c7000] -> 7[c7000] [receive] via NET/IB/1/GDRDMA
gpu108-23-l:42417:42873 [0] NCCL INFO Channel 02/0 : 4[7000] -> 0[7000] [receive] via NET/IB/0/GDRDMA
gpu108-23-r:64496:64892 [0] NCCL INFO Channel 02/0 : 0[7000] -> 4[7000] [receive] via NET/IB/0/GDRDMA
gpu108-23-l:42420:42874 [3] NCCL INFO Channel 03/0 : 7[c7000] -> 3[c7000] [receive] via NET/IB/1/GDRDMA
gpu108-23-r:64499:64893 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 7[c7000] [receive] via NET/IB/1/GDRDMA
gpu108-23-l:42417:42873 [0] NCCL INFO Channel 00/0 : 0[7000] -> 4[7000] [send] via NET/IB/0/GDRDMA
gpu108-23-r:64496:64892 [0] NCCL INFO Channel 00/0 : 4[7000] -> 0[7000] [send] via NET/IB/0/GDRDMA
gpu108-23-l:42420:42874 [3] NCCL INFO Channel 01/0 : 3[c7000] -> 7[c7000] [send] via NET/IB/1/GDRDMA
gpu108-23-r:64499:64893 [3] NCCL INFO Channel 01/0 : 7[c7000] -> 3[c7000] [send] via NET/IB/1/GDRDMA
gpu108-23-l:42417:42873 [0] NCCL INFO Channel 02/0 : 0[7000] -> 4[7000] [send] via NET/IB/0/GDRDMA
gpu108-23-r:64496:64892 [0] NCCL INFO Channel 02/0 : 4[7000] -> 0[7000] [send] via NET/IB/0/GDRDMA
gpu108-23-l:42420:42874 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 7[c7000] [send] via NET/IB/1/GDRDMA
gpu108-23-r:64499:64893 [3] NCCL INFO Channel 03/0 : 7[c7000] -> 3[c7000] [send] via NET/IB/1/GDRDMA
gpu108-23-l:42420:42874 [3] NCCL INFO Channel 01/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:64499:64893 [3] NCCL INFO Channel 01/0 : 7[c7000] -> 6[85000] via P2P/IPC/read
gpu108-23-l:42420:42874 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:64499:64893 [3] NCCL INFO Channel 03/0 : 7[c7000] -> 6[85000] via P2P/IPC/read
gpu108-23-l:42418:42877 [1] NCCL INFO Connected all trees
gpu108-23-l:42418:42877 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-23-l:42418:42877 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-23-l:42419:42875 [2] NCCL INFO Connected all trees
gpu108-23-r:64498:64895 [2] NCCL INFO Connected all trees
gpu108-23-l:42419:42875 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-23-l:42419:42875 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-23-r:64498:64895 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-23-r:64498:64895 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-23-r:64497:64894 [1] NCCL INFO Connected all trees
gpu108-23-r:64497:64894 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-23-r:64497:64894 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-23-r:64496:64892 [0] NCCL INFO Connected all trees
gpu108-23-r:64496:64892 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-23-r:64496:64892 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-23-l:42417:42873 [0] NCCL INFO Connected all trees
gpu108-23-l:42417:42873 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-23-l:42417:42873 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-23-r:64499:64893 [3] NCCL INFO Connected all trees
gpu108-23-r:64499:64893 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-23-r:64499:64893 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-23-l:42420:42874 [3] NCCL INFO Connected all trees
gpu108-23-l:42420:42874 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-23-l:42420:42874 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-23-r:64496:64892 [0] NCCL INFO comm 0x3b19a830 rank 4 nranks 8 cudaDev 0 busId 7000 commId 0x24f8012c65b8a9ff - Init COMPLETE
gpu108-23-r:64498:64895 [2] NCCL INFO comm 0x3b226090 rank 6 nranks 8 cudaDev 2 busId 85000 commId 0x24f8012c65b8a9ff - Init COMPLETE
gpu108-23-r:64497:64894 [1] NCCL INFO comm 0x3ad9f8b0 rank 5 nranks 8 cudaDev 1 busId 46000 commId 0x24f8012c65b8a9ff - Init COMPLETE
gpu108-23-r:64499:64893 [3] NCCL INFO comm 0x3af4da90 rank 7 nranks 8 cudaDev 3 busId c7000 commId 0x24f8012c65b8a9ff - Init COMPLETE
gpu108-23-l:42417:42873 [0] NCCL INFO comm 0x39d76030 rank 0 nranks 8 cudaDev 0 busId 7000 commId 0x24f8012c65b8a9ff - Init COMPLETE
gpu108-23-l:42419:42875 [2] NCCL INFO comm 0x38dd5980 rank 2 nranks 8 cudaDev 2 busId 85000 commId 0x24f8012c65b8a9ff - Init COMPLETE
gpu108-23-l:42418:42877 [1] NCCL INFO comm 0x3bcfac90 rank 1 nranks 8 cudaDev 1 busId 46000 commId 0x24f8012c65b8a9ff - Init COMPLETE
gpu108-23-l:42420:42874 [3] NCCL INFO comm 0x396d4c70 rank 3 nranks 8 cudaDev 3 busId c7000 commId 0x24f8012c65b8a9ff - Init COMPLETE
0 0 Helloworld here 1
3 3 Helloworld here 1
2 2 Helloworld here 1
1 1 Helloworld here 1
4 0 Helloworld here 1
7 3 Helloworld here 1
5 1 Helloworld here 1
6 2 Helloworld here 1
0 0 Helloworld here 2
1 1 Helloworld here 2
3 3 Helloworld here 2
4 0 Helloworld here 2
>>> done with compiling and loading fused kernels. Compilation time: 4.379 seconds
2 2 Helloworld here 2
5 1 Helloworld here 2
6 2 Helloworld here 2
7 3 Helloworld here 2
time to initialize megatron (seconds): 11.449
[after megatron is initialized] datetime: 2023-03-17 12:12:43 
building GPT model ...
[2023-03-17 12:12:43,492] [INFO] [utils.py:829:see_memory_usage] Before Building Model
[2023-03-17 12:12:43,492] [INFO] [utils.py:830:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2023-03-17 12:12:43,493] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 35.03 GB, percent = 7.0%
gpu108-23-l:42420:42905 [3] NCCL INFO Using network IB
gpu108-23-l:42417:42903 [0] NCCL INFO Using network IB
gpu108-23-l:42418:42906 [1] NCCL INFO Using network IB
gpu108-23-l:42419:42904 [2] NCCL INFO Using network IB
gpu108-23-r:64497:64920 [1] NCCL INFO Using network IB
gpu108-23-r:64499:64921 [3] NCCL INFO Using network IB
gpu108-23-r:64496:64919 [0] NCCL INFO Using network IB
gpu108-23-r:64498:64922 [2] NCCL INFO Using network IB
gpu108-23-l:42420:42905 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff
gpu108-23-l:42418:42906 [1] NCCL INFO Setting affinity for GPU 1 to 0fffffff,00000000
gpu108-23-l:42417:42903 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-l:42419:42904 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff
gpu108-23-l:42417:42903 [0] NCCL INFO Setting affinity for GPU 0 to 0fffffff,00000000
gpu108-23-r:64498:64922 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff
gpu108-23-r:64497:64920 [1] NCCL INFO Setting affinity for GPU 1 to 0fffffff,00000000
gpu108-23-r:64499:64921 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff
gpu108-23-r:64496:64919 [0] NCCL INFO Setting affinity for GPU 0 to 0fffffff,00000000
gpu108-23-r:64496:64919 [0] NCCL INFO Trees [0] 5/-1/-1->4->0 [1] -1/-1/-1->4->5 [2] 5/0/-1->4->-1 [3] -1/-1/-1->4->5
gpu108-23-r:64496:64919 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-23-l:42417:42903 [0] NCCL INFO Channel 00/04 :    0   3   2   1   4   7   6   5
gpu108-23-l:42417:42903 [0] NCCL INFO Channel 01/04 :    0   3   6   5   4   7   2   1
gpu108-23-l:42417:42903 [0] NCCL INFO Channel 02/04 :    0   3   2   1   4   7   6   5
gpu108-23-l:42418:42906 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 0/-1/-1->1->2 [2] 2/-1/-1->1->0 [3] 0/-1/-1->1->2
gpu108-23-l:42418:42906 [1] NCCL INFO P2P Chunksize set to 131072
gpu108-23-l:42419:42904 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 1/-1/-1->2->3 [2] 3/-1/-1->2->1 [3] 1/-1/-1->2->3
gpu108-23-l:42419:42904 [2] NCCL INFO P2P Chunksize set to 131072
gpu108-23-l:42420:42905 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 2/7/-1->3->-1 [2] -1/-1/-1->3->2 [3] 2/-1/-1->3->7
gpu108-23-l:42420:42905 [3] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:64497:64920 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 4/-1/-1->5->6 [2] 6/-1/-1->5->4 [3] 4/-1/-1->5->6
gpu108-23-r:64497:64920 [1] NCCL INFO P2P Chunksize set to 131072
gpu108-23-l:42417:42903 [0] NCCL INFO Channel 03/04 :    0   3   6   5   4   7   2   1
gpu108-23-l:42417:42903 [0] NCCL INFO Trees [0] 1/4/-1->0->-1 [1] -1/-1/-1->0->1 [2] 1/-1/-1->0->4 [3] -1/-1/-1->0->1
gpu108-23-l:42417:42903 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:64498:64922 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 5/-1/-1->6->7 [2] 7/-1/-1->6->5 [3] 5/-1/-1->6->7
gpu108-23-r:64498:64922 [2] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:64499:64921 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 6/-1/-1->7->3 [2] -1/-1/-1->7->6 [3] 6/3/-1->7->-1
gpu108-23-r:64499:64921 [3] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:64497:64920 [1] NCCL INFO Channel 00/0 : 5[46000] -> 0[7000] [send] via NET/IB/0/GDRDMA
gpu108-23-r:64498:64922 [2] NCCL INFO Channel 01/0 : 3[c7000] -> 6[85000] [receive] via NET/IB/1/GDRDMA
gpu108-23-l:42417:42903 [0] NCCL INFO Channel 00/0 : 5[46000] -> 0[7000] [receive] via NET/IB/0/GDRDMA
gpu108-23-l:42418:42906 [1] NCCL INFO Channel 00/0 : 1[46000] -> 4[7000] [send] via NET/IB/0/GDRDMA
gpu108-23-l:42419:42904 [2] NCCL INFO Channel 01/0 : 7[c7000] -> 2[85000] [receive] via NET/IB/1/GDRDMA
gpu108-23-r:64496:64919 [0] NCCL INFO Channel 00/0 : 1[46000] -> 4[7000] [receive] via NET/IB/0/GDRDMA
gpu108-23-r:64497:64920 [1] NCCL INFO Channel 02/0 : 5[46000] -> 0[7000] [send] via NET/IB/0/GDRDMA
gpu108-23-r:64498:64922 [2] NCCL INFO Channel 03/0 : 3[c7000] -> 6[85000] [receive] via NET/IB/1/GDRDMA
gpu108-23-l:42417:42903 [0] NCCL INFO Channel 02/0 : 5[46000] -> 0[7000] [receive] via NET/IB/0/GDRDMA
gpu108-23-l:42418:42906 [1] NCCL INFO Channel 02/0 : 1[46000] -> 4[7000] [send] via NET/IB/0/GDRDMA
gpu108-23-l:42419:42904 [2] NCCL INFO Channel 03/0 : 7[c7000] -> 2[85000] [receive] via NET/IB/1/GDRDMA
gpu108-23-l:42417:42903 [0] NCCL INFO Channel 00/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:64496:64919 [0] NCCL INFO Channel 02/0 : 1[46000] -> 4[7000] [receive] via NET/IB/0/GDRDMA
gpu108-23-r:64496:64919 [0] NCCL INFO Channel 00/0 : 4[7000] -> 7[c7000] via P2P/IPC/read
gpu108-23-l:42417:42903 [0] NCCL INFO Channel 01/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:64496:64919 [0] NCCL INFO Channel 01/0 : 4[7000] -> 7[c7000] via P2P/IPC/read
gpu108-23-l:42417:42903 [0] NCCL INFO Channel 02/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:64496:64919 [0] NCCL INFO Channel 02/0 : 4[7000] -> 7[c7000] via P2P/IPC/read
gpu108-23-l:42420:42905 [3] NCCL INFO Channel 01/0 : 3[c7000] -> 6[85000] [send] via NET/IB/1/GDRDMA
gpu108-23-r:64499:64921 [3] NCCL INFO Channel 01/0 : 7[c7000] -> 2[85000] [send] via NET/IB/1/GDRDMA
gpu108-23-l:42417:42903 [0] NCCL INFO Channel 03/0 : 0[7000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:64496:64919 [0] NCCL INFO Channel 03/0 : 4[7000] -> 7[c7000] via P2P/IPC/read
gpu108-23-l:42420:42905 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 6[85000] [send] via NET/IB/1/GDRDMA
gpu108-23-r:64499:64921 [3] NCCL INFO Channel 03/0 : 7[c7000] -> 2[85000] [send] via NET/IB/1/GDRDMA
gpu108-23-l:42420:42905 [3] NCCL INFO Channel 00/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:64499:64921 [3] NCCL INFO Channel 00/0 : 7[c7000] -> 6[85000] via P2P/IPC/read
gpu108-23-l:42420:42905 [3] NCCL INFO Channel 02/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:64499:64921 [3] NCCL INFO Channel 02/0 : 7[c7000] -> 6[85000] via P2P/IPC/read
gpu108-23-r:64498:64922 [2] NCCL INFO Channel 00/0 : 6[85000] -> 5[46000] via P2P/IPC/read
gpu108-23-l:42419:42904 [2] NCCL INFO Channel 00/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:64497:64920 [1] NCCL INFO Channel 01/0 : 5[46000] -> 4[7000] via P2P/IPC/read
gpu108-23-r:64498:64922 [2] NCCL INFO Channel 01/0 : 6[85000] -> 5[46000] via P2P/IPC/read
gpu108-23-l:42418:42906 [1] NCCL INFO Channel 01/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-l:42419:42904 [2] NCCL INFO Channel 01/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:64497:64920 [1] NCCL INFO Channel 03/0 : 5[46000] -> 4[7000] via P2P/IPC/read
gpu108-23-r:64498:64922 [2] NCCL INFO Channel 02/0 : 6[85000] -> 5[46000] via P2P/IPC/read
gpu108-23-l:42418:42906 [1] NCCL INFO Channel 03/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-l:42419:42904 [2] NCCL INFO Channel 02/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:64498:64922 [2] NCCL INFO Channel 03/0 : 6[85000] -> 5[46000] via P2P/IPC/read
gpu108-23-l:42419:42904 [2] NCCL INFO Channel 03/0 : 2[85000] -> 1[46000] via P2P/IPC/read
gpu108-23-l:42417:42903 [0] NCCL INFO Connected all rings
gpu108-23-l:42417:42903 [0] NCCL INFO Channel 00/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:64497:64920 [1] NCCL INFO Connected all rings
gpu108-23-r:64499:64921 [3] NCCL INFO Connected all rings
gpu108-23-r:64496:64919 [0] NCCL INFO Connected all rings
gpu108-23-r:64496:64919 [0] NCCL INFO Channel 00/0 : 4[7000] -> 5[46000] via P2P/IPC/read
gpu108-23-l:42420:42905 [3] NCCL INFO Connected all rings
gpu108-23-r:64498:64922 [2] NCCL INFO Connected all rings
gpu108-23-l:42418:42906 [1] NCCL INFO Connected all rings
gpu108-23-l:42419:42904 [2] NCCL INFO Connected all rings
gpu108-23-l:42417:42903 [0] NCCL INFO Channel 01/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:64496:64919 [0] NCCL INFO Channel 01/0 : 4[7000] -> 5[46000] via P2P/IPC/read
gpu108-23-l:42417:42903 [0] NCCL INFO Channel 02/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:64496:64919 [0] NCCL INFO Channel 02/0 : 4[7000] -> 5[46000] via P2P/IPC/read
gpu108-23-l:42417:42903 [0] NCCL INFO Channel 03/0 : 0[7000] -> 1[46000] via P2P/IPC/read
gpu108-23-r:64496:64919 [0] NCCL INFO Channel 03/0 : 4[7000] -> 5[46000] via P2P/IPC/read
gpu108-23-r:64497:64920 [1] NCCL INFO Channel 00/0 : 5[46000] -> 6[85000] via P2P/IPC/read
gpu108-23-l:42418:42906 [1] NCCL INFO Channel 00/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-l:42419:42904 [2] NCCL INFO Channel 00/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:64498:64922 [2] NCCL INFO Channel 00/0 : 6[85000] -> 7[c7000] via P2P/IPC/read
gpu108-23-r:64497:64920 [1] NCCL INFO Channel 01/0 : 5[46000] -> 6[85000] via P2P/IPC/read
gpu108-23-l:42418:42906 [1] NCCL INFO Channel 01/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-l:42419:42904 [2] NCCL INFO Channel 01/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:64498:64922 [2] NCCL INFO Channel 01/0 : 6[85000] -> 7[c7000] via P2P/IPC/read
gpu108-23-r:64497:64920 [1] NCCL INFO Channel 02/0 : 5[46000] -> 6[85000] via P2P/IPC/read
gpu108-23-l:42418:42906 [1] NCCL INFO Channel 02/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-l:42419:42904 [2] NCCL INFO Channel 02/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:64498:64922 [2] NCCL INFO Channel 02/0 : 6[85000] -> 7[c7000] via P2P/IPC/read
gpu108-23-r:64497:64920 [1] NCCL INFO Channel 03/0 : 5[46000] -> 6[85000] via P2P/IPC/read
gpu108-23-l:42418:42906 [1] NCCL INFO Channel 03/0 : 1[46000] -> 2[85000] via P2P/IPC/read
gpu108-23-l:42419:42904 [2] NCCL INFO Channel 03/0 : 2[85000] -> 3[c7000] via P2P/IPC/read
gpu108-23-r:64498:64922 [2] NCCL INFO Channel 03/0 : 6[85000] -> 7[c7000] via P2P/IPC/read
gpu108-23-l:42418:42906 [1] NCCL INFO Channel 00/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-r:64497:64920 [1] NCCL INFO Channel 00/0 : 5[46000] -> 4[7000] via P2P/IPC/read
gpu108-23-r:64496:64919 [0] NCCL INFO Channel 00/0 : 0[7000] -> 4[7000] [receive] via NET/IB/0/GDRDMA
gpu108-23-l:42418:42906 [1] NCCL INFO Channel 02/0 : 1[46000] -> 0[7000] via P2P/IPC/read
gpu108-23-l:42417:42903 [0] NCCL INFO Channel 00/0 : 4[7000] -> 0[7000] [receive] via NET/IB/0/GDRDMA
gpu108-23-r:64497:64920 [1] NCCL INFO Channel 02/0 : 5[46000] -> 4[7000] via P2P/IPC/read
gpu108-23-r:64499:64921 [3] NCCL INFO Channel 01/0 : 3[c7000] -> 7[c7000] [receive] via NET/IB/1/GDRDMA
gpu108-23-l:42420:42905 [3] NCCL INFO Channel 01/0 : 7[c7000] -> 3[c7000] [receive] via NET/IB/1/GDRDMA
gpu108-23-r:64496:64919 [0] NCCL INFO Channel 02/0 : 0[7000] -> 4[7000] [receive] via NET/IB/0/GDRDMA
gpu108-23-l:42417:42903 [0] NCCL INFO Channel 02/0 : 4[7000] -> 0[7000] [receive] via NET/IB/0/GDRDMA
gpu108-23-r:64499:64921 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 7[c7000] [receive] via NET/IB/1/GDRDMA
gpu108-23-l:42420:42905 [3] NCCL INFO Channel 03/0 : 7[c7000] -> 3[c7000] [receive] via NET/IB/1/GDRDMA
gpu108-23-r:64496:64919 [0] NCCL INFO Channel 00/0 : 4[7000] -> 0[7000] [send] via NET/IB/0/GDRDMA
gpu108-23-l:42417:42903 [0] NCCL INFO Channel 00/0 : 0[7000] -> 4[7000] [send] via NET/IB/0/GDRDMA
gpu108-23-r:64499:64921 [3] NCCL INFO Channel 01/0 : 7[c7000] -> 3[c7000] [send] via NET/IB/1/GDRDMA
gpu108-23-l:42420:42905 [3] NCCL INFO Channel 01/0 : 3[c7000] -> 7[c7000] [send] via NET/IB/1/GDRDMA
gpu108-23-r:64496:64919 [0] NCCL INFO Channel 02/0 : 4[7000] -> 0[7000] [send] via NET/IB/0/GDRDMA
gpu108-23-l:42417:42903 [0] NCCL INFO Channel 02/0 : 0[7000] -> 4[7000] [send] via NET/IB/0/GDRDMA
gpu108-23-r:64499:64921 [3] NCCL INFO Channel 03/0 : 7[c7000] -> 3[c7000] [send] via NET/IB/1/GDRDMA
gpu108-23-l:42420:42905 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 7[c7000] [send] via NET/IB/1/GDRDMA
gpu108-23-l:42420:42905 [3] NCCL INFO Channel 01/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:64499:64921 [3] NCCL INFO Channel 01/0 : 7[c7000] -> 6[85000] via P2P/IPC/read
gpu108-23-r:64499:64921 [3] NCCL INFO Channel 03/0 : 7[c7000] -> 6[85000] via P2P/IPC/read
gpu108-23-l:42420:42905 [3] NCCL INFO Channel 03/0 : 3[c7000] -> 2[85000] via P2P/IPC/read
gpu108-23-r:64497:64920 [1] NCCL INFO Connected all trees
gpu108-23-r:64497:64920 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-23-r:64497:64920 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-23-r:64498:64922 [2] NCCL INFO Connected all trees
gpu108-23-r:64498:64922 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-23-r:64498:64922 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-23-l:42418:42906 [1] NCCL INFO Connected all trees
gpu108-23-l:42418:42906 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-23-l:42418:42906 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-23-l:42419:42904 [2] NCCL INFO Connected all trees
gpu108-23-l:42419:42904 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-23-l:42419:42904 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-23-l:42417:42903 [0] NCCL INFO Connected all trees
gpu108-23-l:42417:42903 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-23-l:42417:42903 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-23-r:64496:64919 [0] NCCL INFO Connected all trees
gpu108-23-r:64496:64919 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-23-r:64496:64919 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-23-l:42420:42905 [3] NCCL INFO Connected all trees
gpu108-23-r:64499:64921 [3] NCCL INFO Connected all trees
gpu108-23-l:42420:42905 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-23-l:42420:42905 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-23-r:64499:64921 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-23-r:64499:64921 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gpu108-23-l:42417:42903 [0] NCCL INFO comm 0x327186d0 rank 0 nranks 8 cudaDev 0 busId 7000 commId 0x9c2bd16b52beb85c - Init COMPLETE
gpu108-23-l:42420:42905 [3] NCCL INFO comm 0x34f95d50 rank 3 nranks 8 cudaDev 3 busId c7000 commId 0x9c2bd16b52beb85c - Init COMPLETE
gpu108-23-r:64496:64919 [0] NCCL INFO comm 0x365f4850 rank 4 nranks 8 cudaDev 0 busId 7000 commId 0x9c2bd16b52beb85c - Init COMPLETE
gpu108-23-l:42418:42906 [1] NCCL INFO comm 0x36fe7510 rank 1 nranks 8 cudaDev 1 busId 46000 commId 0x9c2bd16b52beb85c - Init COMPLETE
gpu108-23-l:42419:42904 [2] NCCL INFO comm 0x3334ad60 rank 2 nranks 8 cudaDev 2 busId 85000 commId 0x9c2bd16b52beb85c - Init COMPLETE
gpu108-23-r:64499:64921 [3] NCCL INFO comm 0x3368f130 rank 7 nranks 8 cudaDev 3 busId c7000 commId 0x9c2bd16b52beb85c - Init COMPLETE
gpu108-23-r:64497:64920 [1] NCCL INFO comm 0x36efe5d0 rank 5 nranks 8 cudaDev 1 busId 46000 commId 0x9c2bd16b52beb85c - Init COMPLETE
gpu108-23-r:64498:64922 [2] NCCL INFO comm 0x35a0b040 rank 6 nranks 8 cudaDev 2 busId 85000 commId 0x9c2bd16b52beb85c - Init COMPLETE
[2023-03-17 12:12:45,321] [INFO] [partition_parameters.py:415:__exit__] finished initializing model with 0.07B parameters
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[2023-03-17 12:12:45,357] [INFO] [utils.py:829:see_memory_usage] After Building Model
[2023-03-17 12:12:45,358] [INFO] [utils.py:830:see_memory_usage] MA 0.02 GB         Max_MA 0.11 GB         CA 0.13 GB         Max_CA 0 GB 
[2023-03-17 12:12:45,358] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 35.25 GB, percent = 7.0%
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 65158144
> learning rate decay style: cosine
DeepSpeed is enabled.
[2023-03-17 12:12:45,359] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed info: version=0.8.3+bbfd0a6, git-hash=bbfd0a6, git-branch=master
[2023-03-17 12:12:45,361] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-03-17 12:12:45,361] [INFO] [logging.py:93:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-03-17 12:12:45,361] [INFO] [logging.py:93:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-03-17 12:12:45,361] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2023-03-17 12:12:45,361] [INFO] [utils.py:55:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'apex.optimizers.fused_adam.FusedAdam'>
[2023-03-17 12:12:45,361] [INFO] [logging.py:93:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer
[2023-03-17 12:12:45,389] [INFO] [utils.py:829:see_memory_usage] Stage 3 initialize beginning
[2023-03-17 12:12:45,390] [INFO] [utils.py:830:see_memory_usage] MA 0.02 GB         Max_MA 0.02 GB         CA 0.13 GB         Max_CA 0 GB 
[2023-03-17 12:12:45,390] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 35.25 GB, percent = 7.0%
[2023-03-17 12:12:45,391] [INFO] [stage3.py:113:__init__] Reduce bucket size 90000000
[2023-03-17 12:12:45,391] [INFO] [stage3.py:114:__init__] Prefetch bucket size 50000000
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Emitting ninja build file /home/shaima0d/.cache/torch_extensions/py39_cu117/utils/build.ninja...
Building extension module utils...
Using envvar MAX_JOBS (15) as the number of workers...
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.19023752212524414 seconds
Loading extension module utils...
Loading extension module utils...
Time to load utils op: 0.2115793228149414 seconds
Time to load utils op: 0.21201729774475098 seconds
Loading extension module utils...
Time to load utils op: 0.20499968528747559 seconds
[2023-03-17 12:12:45,624] [INFO] [utils.py:829:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2023-03-17 12:12:45,624] [INFO] [utils.py:830:see_memory_usage] MA 0.02 GB         Max_MA 0.02 GB         CA 0.13 GB         Max_CA 0 GB 
[2023-03-17 12:12:45,624] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 35.25 GB, percent = 7.0%
Parameter Offload: Total persistent parameters: 15360 in 10 params
[2023-03-17 12:12:45,650] [INFO] [utils.py:829:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2023-03-17 12:12:45,650] [INFO] [utils.py:830:see_memory_usage] MA 0.02 GB         Max_MA 0.02 GB         CA 0.13 GB         Max_CA 0 GB 
[2023-03-17 12:12:45,651] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 35.25 GB, percent = 7.0%
[2023-03-17 12:12:45,676] [INFO] [utils.py:829:see_memory_usage] Before creating fp16 partitions
[2023-03-17 12:12:45,676] [INFO] [utils.py:830:see_memory_usage] MA 0.02 GB         Max_MA 0.02 GB         CA 0.13 GB         Max_CA 0 GB 
[2023-03-17 12:12:45,676] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 35.25 GB, percent = 7.0%
Loading extension module utils...
Loading extension module utils...
Loading extension module utils...
Loading extension module utils...
Time to load utils op: 3.016965389251709 seconds
Time to load utils op: 3.0169014930725098 seconds
Time to load utils op: 3.016908884048462 seconds
Time to load utils op: 3.016888380050659 seconds
[2023-03-17 12:12:48,432] [INFO] [utils.py:829:see_memory_usage] After creating fp16 partitions: 2
[2023-03-17 12:12:48,433] [INFO] [utils.py:830:see_memory_usage] MA 0.02 GB         Max_MA 0.02 GB         CA 0.02 GB         Max_CA 0 GB 
[2023-03-17 12:12:48,433] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 35.3 GB, percent = 7.0%
[2023-03-17 12:12:48,458] [INFO] [utils.py:829:see_memory_usage] Before creating fp32 partitions
[2023-03-17 12:12:48,458] [INFO] [utils.py:830:see_memory_usage] MA 0.02 GB         Max_MA 0.02 GB         CA 0.02 GB         Max_CA 0 GB 
[2023-03-17 12:12:48,458] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 35.3 GB, percent = 7.0%
[2023-03-17 12:12:48,484] [INFO] [utils.py:829:see_memory_usage] After creating fp32 partitions
[2023-03-17 12:12:48,485] [INFO] [utils.py:830:see_memory_usage] MA 0.05 GB         Max_MA 0.06 GB         CA 0.06 GB         Max_CA 0 GB 
[2023-03-17 12:12:48,485] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 35.3 GB, percent = 7.0%
[2023-03-17 12:12:48,510] [INFO] [utils.py:829:see_memory_usage] Before initializing optimizer states
[2023-03-17 12:12:48,511] [INFO] [utils.py:830:see_memory_usage] MA 0.05 GB         Max_MA 0.05 GB         CA 0.06 GB         Max_CA 0 GB 
[2023-03-17 12:12:48,511] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 35.3 GB, percent = 7.0%
[2023-03-17 12:12:48,513] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | init_optimizer_state: 0.74
[2023-03-17 12:12:48,538] [INFO] [utils.py:829:see_memory_usage] After initializing optimizer states
[2023-03-17 12:12:48,538] [INFO] [utils.py:830:see_memory_usage] MA 0.11 GB         Max_MA 0.14 GB         CA 0.16 GB         Max_CA 0 GB 
[2023-03-17 12:12:48,539] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 35.3 GB, percent = 7.0%
[2023-03-17 12:12:48,539] [INFO] [stage3.py:376:_setup_for_real_optimizer] optimizer state initialized
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Time to load utils op: 0.0003428459167480469 seconds
Loading extension module utils...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Time to load utils op: 0.0003554821014404297 seconds
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Time to load utils op: 0.00038552284240722656 seconds
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
Time to load utils op: 0.00036907196044921875 seconds
Loading extension module utils...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
Time to load utils op: 0.0003719329833984375 seconds
Loading extension module utils...
Time to load utils op: 0.00034332275390625 seconds
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.000331878662109375 seconds
[2023-03-17 12:12:48,574] [INFO] [utils.py:829:see_memory_usage] After initializing ZeRO optimizer
[2023-03-17 12:12:48,575] [INFO] [utils.py:830:see_memory_usage] MA 0.29 GB         Max_MA 0.48 GB         CA 0.62 GB         Max_CA 1 GB 
[2023-03-17 12:12:48,575] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 35.31 GB, percent = 7.0%
[2023-03-17 12:12:48,575] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[2023-03-17 12:12:48,575] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-03-17 12:12:48,575] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x2b542ffbd550>
[2023-03-17 12:12:48,575] [INFO] [logging.py:93:log_dist] [Rank 0] step=0, skipped=0, lr=[5.9999999999999995e-05, 5.9999999999999995e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:48,575] [INFO] [config.py:1018:print] DeepSpeedEngine configuration:
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 16, 'thread_count': 2, 'single_submit': False, 'overlap_events': True}
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   amp_enabled .................. False
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   amp_params ................... False
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   bfloat16_enabled ............. False
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   checkpoint_parallel_write_pipeline  False
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   checkpoint_tag_validation_enabled  True
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   checkpoint_tag_validation_fail  False
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x2b542ffbdac0>
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   communication_data_type ...... None
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   curriculum_enabled_legacy .... False
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   curriculum_params_legacy ..... False
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   data_efficiency_enabled ...... False
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   dataloader_drop_last ......... False
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   disable_allgather ............ False
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   dump_state ................... False
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   dynamic_loss_scale_args ...... {'init_scale': 32768, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   eigenvalue_enabled ........... False
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   eigenvalue_gas_boundary_resolution  1
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   eigenvalue_layer_num ......... 0
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   eigenvalue_max_iter .......... 100
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   eigenvalue_stability ......... 1e-06
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   eigenvalue_tol ............... 0.01
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   eigenvalue_verbose ........... False
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   elasticity_enabled ........... False
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   fp16_auto_cast ............... False
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   fp16_enabled ................. True
[2023-03-17 12:12:48,576] [INFO] [config.py:1022:print]   fp16_master_weights_and_gradients  False
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   global_rank .................. 0
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   grad_accum_dtype ............. None
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   gradient_accumulation_steps .. 1
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   gradient_clipping ............ 1
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   gradient_predivide_factor .... 1.0
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   initial_dynamic_scale ........ 32768
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   load_universal_checkpoint .... False
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   loss_scale ................... 0
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   memory_breakdown ............. False
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   optimizer_legacy_fusion ...... False
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   optimizer_name ............... None
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   optimizer_params ............. None
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   pld_enabled .................. False
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   pld_params ................... False
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   prescale_gradients ........... False
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   scheduler_name ............... None
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   scheduler_params ............. None
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   sparse_attention ............. None
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   sparse_gradients_enabled ..... False
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   steps_per_print .............. 1
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   train_batch_size ............. 32
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   train_micro_batch_size_per_gpu  4
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   use_node_local_storage ....... False
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   wall_clock_breakdown ......... True
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   world_size ................... 8
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   zero_allow_untested_optimizer  False
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=90000000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=sys.maxsize max_live_parameters=3000000000 max_reuse_distance=3000000000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   zero_enabled ................. True
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   zero_force_ds_cpu_optimizer .. True
[2023-03-17 12:12:48,577] [INFO] [config.py:1022:print]   zero_optimization_stage ...... 3
[2023-03-17 12:12:48,577] [INFO] [config.py:1007:print_user_config]   json = {
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 4, 
    "steps_per_print": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 3, 
        "stage3_max_live_parameters": 3.000000e+09, 
        "stage3_max_reuse_distance": 3.000000e+09, 
        "stage3_param_persistence_threshold": 1.000000e+05, 
        "stage3_prefetch_bucket_size": 5.000000e+07, 
        "contiguous_gradients": true, 
        "overlap_comm": true, 
        "reduce_bucket_size": 9.000000e+07, 
        "sub_group_size": 1.000000e+09, 
        "offload_optimizer": {
            "device": "none", 
            "buffer_count": 4, 
            "pipeline_read": false, 
            "pipeline_write": false, 
            "pin_memory": true
        }
    }, 
    "gradient_clipping": 1, 
    "fp16": {
        "enabled": true, 
        "initial_scale_power": 15, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "wall_clock_breakdown": true, 
    "zero_allow_untested_optimizer": false, 
    "aio": {
        "block_size": 1.048576e+06, 
        "queue_depth": 16, 
        "single_submit": false, 
        "overlap_events": true, 
        "thread_count": 2
    }
}
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0003006458282470703 seconds
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-03-17 12:12:48 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      1600
    validation: 1280
    test:       1280
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.000869 seconds
    number of documents: 17868
 > dataset split:
    train:
     document indices in [0, 17511) total of 17511 documents
    validation:
     document indices in [17511, 17868) total of 357 documents
    test:
     document indices in [17868, 17868) total of 0 documents
NCCL version 2.17.1+cuda11.7
NCCL version 2.17.1+cuda11.7
NCCL version 2.17.1+cuda11.7
gpu108-23-r:64499:64942 [3] NCCL INFO Using network IB
gpu108-23-l:42420:42927 [3] NCCL INFO Using network IB
gpu108-23-r:64496:64941 [0] NCCL INFO Using network IB
gpu108-23-l:42417:42929 [0] NCCL INFO Using network IB
NCCL version 2.17.1+cuda11.7
gpu108-23-r:64497:64944 [1] NCCL INFO Using network IB
NCCL version 2.17.1+cuda11.7
gpu108-23-r:64498:64946 [2] NCCL INFO Using network IB
NCCL version 2.17.1+cuda11.7
gpu108-23-l:42419:42931 [2] NCCL INFO Using network IB
NCCL version 2.17.1+cuda11.7
gpu108-23-l:42418:42933 [1] NCCL INFO Using network IB
gpu108-23-l:42420:42927 [3] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-l:42420:42927 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 00/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 01/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 02/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 03/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 04/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 05/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 06/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 07/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 08/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 09/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 10/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 11/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 12/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 13/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 14/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 15/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 16/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 17/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 18/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 19/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 20/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 21/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 22/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 23/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 24/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 25/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 26/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 27/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 28/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 29/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 30/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Channel 31/32 :    0
gpu108-23-l:42420:42927 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-l:42420:42927 [3] NCCL INFO P2P Chunksize set to 131072
gpu108-23-l:42420:42927 [3] NCCL INFO Connected all rings
gpu108-23-l:42420:42927 [3] NCCL INFO Connected all trees
gpu108-23-l:42420:42927 [3] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-l:42417:42929 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-l:42417:42929 [0] NCCL INFO Setting affinity for GPU 0 to 0fffffff,00000000
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 00/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 01/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 02/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 03/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 04/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 05/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 06/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 07/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 08/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 09/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 10/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 11/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 12/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 13/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 14/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 15/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 16/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 17/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 18/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 19/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 20/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 21/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 22/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 23/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 24/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 25/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 26/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 27/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 28/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 29/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 30/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Channel 31/32 :    0
gpu108-23-l:42417:42929 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-l:42417:42929 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-23-l:42417:42929 [0] NCCL INFO Connected all rings
gpu108-23-l:42417:42929 [0] NCCL INFO Connected all trees
gpu108-23-l:42417:42929 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:64499:64942 [3] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:64499:64942 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 00/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 01/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 02/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 03/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 04/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 05/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 06/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 07/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 08/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 09/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 10/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 11/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 12/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 13/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 14/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 15/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 16/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 17/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 18/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 19/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 20/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 21/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 22/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 23/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 24/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 25/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 26/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 27/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 28/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 29/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 30/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Channel 31/32 :    0
gpu108-23-r:64499:64942 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-r:64499:64942 [3] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:64499:64942 [3] NCCL INFO Connected all rings
gpu108-23-r:64499:64942 [3] NCCL INFO Connected all trees
gpu108-23-r:64499:64942 [3] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:64496:64941 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:64496:64941 [0] NCCL INFO Setting affinity for GPU 0 to 0fffffff,00000000
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 00/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 01/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 02/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 03/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 04/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 05/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 06/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 07/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 08/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 09/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 10/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 11/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 12/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 13/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 14/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 15/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 16/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 17/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 18/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 19/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 20/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 21/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 22/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 23/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 24/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 25/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 26/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 27/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 28/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 29/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 30/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Channel 31/32 :    0
gpu108-23-r:64496:64941 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-r:64496:64941 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:64496:64941 [0] NCCL INFO Connected all rings
gpu108-23-r:64496:64941 [0] NCCL INFO Connected all trees
gpu108-23-r:64496:64941 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-l:42419:42931 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 00/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 01/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 02/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 03/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 04/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 05/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 06/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 07/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 08/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 09/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 10/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 11/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 12/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 13/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 14/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 15/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 16/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 17/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 18/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 19/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 20/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 21/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 22/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 23/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 24/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 25/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 26/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 27/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 28/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 29/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 30/32 :    0
gpu108-23-l:42419:42931 [2] NCCL INFO Channel 31/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-l:42419:42931 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-l:42419:42931 [2] NCCL INFO P2P Chunksize set to 131072
gpu108-23-l:42419:42931 [2] NCCL INFO Connected all rings
gpu108-23-l:42419:42931 [2] NCCL INFO Connected all trees
gpu108-23-l:42419:42931 [2] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-l:42418:42933 [1] NCCL INFO Setting affinity for GPU 1 to 0fffffff,00000000
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 00/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 01/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 02/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 03/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 04/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 05/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 06/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 07/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 08/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 09/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 10/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 11/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 12/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 13/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 14/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 15/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 16/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 17/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 18/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 19/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 20/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 21/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 22/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 23/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 24/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 25/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 26/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 27/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 28/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 29/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 30/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Channel 31/32 :    0
gpu108-23-l:42418:42933 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-l:42418:42933 [1] NCCL INFO P2P Chunksize set to 131072
gpu108-23-l:42418:42933 [1] NCCL INFO Connected all rings
gpu108-23-l:42418:42933 [1] NCCL INFO Connected all trees
gpu108-23-l:42418:42933 [1] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:64497:64944 [1] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:64497:64944 [1] NCCL INFO Setting affinity for GPU 1 to 0fffffff,00000000
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 00/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 01/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 02/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 03/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 04/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 05/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 06/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 07/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 08/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 09/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 10/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 11/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 12/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 13/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 14/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 15/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 16/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 17/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 18/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 19/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 20/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 21/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 22/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 23/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 24/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 25/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 26/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 27/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 28/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 29/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 30/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Channel 31/32 :    0
gpu108-23-r:64497:64944 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-r:64497:64944 [1] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:64498:64946 [2] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:64497:64944 [1] NCCL INFO Connected all rings
gpu108-23-r:64497:64944 [1] NCCL INFO Connected all trees
gpu108-23-r:64497:64944 [1] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:64498:64946 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 00/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 01/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 02/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 03/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 04/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 05/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 06/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 07/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 08/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 09/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 10/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 11/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 12/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 13/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 14/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 15/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 16/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 17/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 18/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 19/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 20/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 21/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 22/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 23/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 24/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 25/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 26/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 27/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 28/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 29/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 30/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Channel 31/32 :    0
gpu108-23-r:64498:64946 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-r:64498:64946 [2] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:64498:64946 [2] NCCL INFO Connected all rings
gpu108-23-r:64498:64946 [2] NCCL INFO Connected all trees
gpu108-23-r:64498:64946 [2] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-l:42420:42927 [3] NCCL INFO comm 0x34fbe860 rank 0 nranks 1 cudaDev 3 busId c7000 commId 0xf63bcacc17c83d66 - Init COMPLETE
gpu108-23-l:42417:42929 [0] NCCL INFO comm 0x32741a90 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0x10f88a203b02ef1c - Init COMPLETE
 > loading doc-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_train_indexmap_1600ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_train_indexmap_1600ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_train_indexmap_1600ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.001 seconds
    total number of samples: 1544006
    total number of epochs: 1
gpu108-23-l:42419:42931 [2] NCCL INFO comm 0x3336fae0 rank 0 nranks 1 cudaDev 2 busId 85000 commId 0x618306354c683f4d - Init COMPLETE
gpu108-23-l:42418:42933 [1] NCCL INFO comm 0x3700b840 rank 0 nranks 1 cudaDev 1 busId 46000 commId 0x7d97d6ac5ae50ca0 - Init COMPLETE
gpu108-23-r:64499:64942 [3] NCCL INFO comm 0x336b3460 rank 0 nranks 1 cudaDev 3 busId c7000 commId 0x7635c400e2be3b47 - Init COMPLETE
gpu108-23-r:64496:64941 [0] NCCL INFO comm 0x36618b80 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0x215a8ada2f37e2b6 - Init COMPLETE
gpu108-23-r:64497:64944 [1] NCCL INFO comm 0x36f23350 rank 0 nranks 1 cudaDev 1 busId 46000 commId 0xcb2537301560480e - Init COMPLETE
gpu108-23-r:64498:64946 [2] NCCL INFO comm 0x35a2fdc0 rank 0 nranks 1 cudaDev 2 busId 85000 commId 0xe6a70f323fad3eb1 - Init COMPLETE
 > loading doc-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_valid_indexmap_1280ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_valid_indexmap_1280ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_valid_indexmap_1280ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.001 seconds
    total number of samples: 31426
    total number of epochs: 1
> finished creating GPT datasets ...
gpu108-23-l:42419:42946 [2] NCCL INFO Using network IB
gpu108-23-l:42418:42945 [1] NCCL INFO Using network IB
gpu108-23-l:42420:42947 [3] NCCL INFO Using network IB
gpu108-23-l:42417:42949 [0] NCCL INFO Using network IB
gpu108-23-r:64497:64960 [1] NCCL INFO Using network IB
gpu108-23-r:64499:64961 [3] NCCL INFO Using network IB
gpu108-23-r:64496:64962 [0] NCCL INFO Using network IB
gpu108-23-r:64498:64963 [2] NCCL INFO Using network IB
gpu108-23-l:42418:42945 [1] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-l:42418:42945 [1] NCCL INFO Setting affinity for GPU 1 to 0fffffff,00000000
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 00/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 01/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 02/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 03/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 04/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 05/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 06/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 07/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 08/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 09/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 10/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 11/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 12/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 13/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 14/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 15/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 16/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 17/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 18/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 19/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 20/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 21/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 22/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 23/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 24/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 25/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 26/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 27/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 28/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 29/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 30/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Channel 31/32 :    0
gpu108-23-l:42418:42945 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-l:42418:42945 [1] NCCL INFO P2P Chunksize set to 131072
gpu108-23-l:42418:42945 [1] NCCL INFO Connected all rings
gpu108-23-l:42418:42945 [1] NCCL INFO Connected all trees
gpu108-23-l:42418:42945 [1] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:64499:64961 [3] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-l:42417:42949 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:64499:64961 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 00/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 01/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 02/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 03/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 04/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 05/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 06/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 07/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 08/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 09/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 10/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 11/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 12/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 13/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 14/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 15/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 16/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 17/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 18/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 19/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 20/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 21/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 22/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 23/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 24/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 25/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 26/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 27/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 28/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 29/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 30/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Channel 31/32 :    0
gpu108-23-r:64499:64961 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-r:64499:64961 [3] NCCL INFO P2P Chunksize set to 131072
gpu108-23-l:42420:42947 [3] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:64499:64961 [3] NCCL INFO Connected all rings
gpu108-23-r:64499:64961 [3] NCCL INFO Connected all trees
gpu108-23-l:42417:42949 [0] NCCL INFO Setting affinity for GPU 0 to 0fffffff,00000000
gpu108-23-r:64499:64961 [3] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:64497:64960 [1] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 00/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 01/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 02/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 03/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 04/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 05/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 06/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 07/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 08/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 09/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 10/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 11/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 12/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 13/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 14/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 15/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 16/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 17/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 18/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 19/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 20/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 21/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 22/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 23/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 24/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 25/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 26/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 27/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 28/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 29/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 30/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Channel 31/32 :    0
gpu108-23-l:42417:42949 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-l:42417:42949 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-23-l:42419:42946 [2] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-l:42420:42947 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff
gpu108-23-l:42417:42949 [0] NCCL INFO Connected all rings
gpu108-23-l:42417:42949 [0] NCCL INFO Connected all trees
gpu108-23-l:42417:42949 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:64498:64963 [2] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 00/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 01/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 02/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 03/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 04/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 05/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 06/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 07/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 08/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 09/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 10/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 11/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 12/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 13/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 14/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 15/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 16/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 17/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 18/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 19/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 20/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 21/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 22/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 23/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 24/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 25/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 26/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 27/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 28/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 29/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 30/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Channel 31/32 :    0
gpu108-23-l:42420:42947 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-l:42420:42947 [3] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:64497:64960 [1] NCCL INFO Setting affinity for GPU 1 to 0fffffff,00000000
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 00/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 01/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 02/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 03/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 04/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 05/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 06/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 07/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 08/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 09/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 10/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 11/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 12/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 13/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 14/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 15/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 16/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 17/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 18/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 19/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 20/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 21/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 22/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 23/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 24/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 25/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 26/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 27/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 28/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 29/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 30/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Channel 31/32 :    0
gpu108-23-r:64497:64960 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-r:64497:64960 [1] NCCL INFO P2P Chunksize set to 131072
gpu108-23-l:42419:42946 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff
gpu108-23-l:42420:42947 [3] NCCL INFO Connected all rings
gpu108-23-l:42420:42947 [3] NCCL INFO Connected all trees
gpu108-23-l:42420:42947 [3] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:64497:64960 [1] NCCL INFO Connected all rings
gpu108-23-r:64497:64960 [1] NCCL INFO Connected all trees
gpu108-23-r:64497:64960 [1] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:64498:64963 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff
gpu108-23-r:64496:64962 [0] NCCL INFO Setting affinity for GPU 0 to 0fffffff,00000000
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 00/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 01/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 02/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 03/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 04/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 05/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 06/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 07/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 08/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 09/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 10/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 11/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 12/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 13/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 14/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 15/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 16/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 17/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 18/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 19/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 00/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 01/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 02/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 20/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 21/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 22/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 23/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 24/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 25/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 26/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 03/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 04/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 05/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 06/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 07/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 08/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 09/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 10/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 11/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 27/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 28/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 29/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 30/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Channel 31/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 12/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 13/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 14/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 15/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 16/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 17/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 18/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 19/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 20/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 21/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 22/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 23/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 24/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 25/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 26/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 27/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 28/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 29/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 30/32 :    0
gpu108-23-r:64498:64963 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-r:64498:64963 [2] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:64496:64962 [0] NCCL INFO Channel 31/32 :    0
gpu108-23-r:64496:64962 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-r:64496:64962 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:64498:64963 [2] NCCL INFO Connected all rings
gpu108-23-r:64498:64963 [2] NCCL INFO Connected all trees
gpu108-23-r:64498:64963 [2] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:64496:64962 [0] NCCL INFO Connected all rings
gpu108-23-r:64496:64962 [0] NCCL INFO Connected all trees
gpu108-23-r:64496:64962 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 00/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 01/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 02/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 03/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 04/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 05/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 06/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 07/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 08/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 09/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 10/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 11/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 12/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 13/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 14/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 15/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 16/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 17/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 18/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 19/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 20/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 21/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 22/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 23/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 24/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 25/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 26/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 27/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 28/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 29/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 30/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Channel 31/32 :    0
gpu108-23-l:42419:42946 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-l:42419:42946 [2] NCCL INFO P2P Chunksize set to 131072
gpu108-23-l:42419:42946 [2] NCCL INFO Connected all rings
gpu108-23-l:42419:42946 [2] NCCL INFO Connected all trees
gpu108-23-l:42419:42946 [2] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-l:42418:42945 [1] NCCL INFO comm 0x3700e9b0 rank 0 nranks 1 cudaDev 1 busId 46000 commId 0x3cdbcdb24612f8a6 - Init COMPLETE
gpu108-23-l:42417:42949 [0] NCCL INFO comm 0x327308e0 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0xe6f871b97644b3a1 - Init COMPLETE
gpu108-23-r:64497:64960 [1] NCCL INFO comm 0x36f264c0 rank 0 nranks 1 cudaDev 1 busId 46000 commId 0x53a12cd551b4862d - Init COMPLETE
gpu108-23-l:42420:42947 [3] NCCL INFO comm 0x34fc19d0 rank 0 nranks 1 cudaDev 3 busId c7000 commId 0x22be92cdf3024d50 - Init COMPLETE
gpu108-23-r:64496:64962 [0] NCCL INFO comm 0x3661bcf0 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0x25f6451208a5af85 - Init COMPLETE
gpu108-23-r:64499:64961 [3] NCCL INFO comm 0x336b65d0 rank 0 nranks 1 cudaDev 3 busId c7000 commId 0xc7005c5fd76cbd4c - Init COMPLETE
gpu108-23-r:64498:64963 [2] NCCL INFO comm 0x35a32f30 rank 0 nranks 1 cudaDev 2 busId 85000 commId 0xd2d5648ecd711f - Init COMPLETE
gpu108-23-l:42419:42946 [2] NCCL INFO comm 0x33372c50 rank 0 nranks 1 cudaDev 2 busId 85000 commId 0x88a5dc8a3f429c88 - Init COMPLETE
[after dataloaders are built] datetime: 2023-03-17 12:12:49 
done with setup ...
training ...
time (ms) | model-and-optimizer-setup: 5100.43 | train/valid/test-data-iterators-setup: 519.60
[before the start of training step] datetime: 2023-03-17 12:12:49 
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
[2023-03-17 12:12:49,189] [INFO] [checkpointing.py:553:forward] Activation Checkpointing Information
[2023-03-17 12:12:49,189] [INFO] [checkpointing.py:554:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-03-17 12:12:49,189] [INFO] [checkpointing.py:557:forward] ----contiguous Memory Checkpointing False with 1 total layers
[2023-03-17 12:12:49,189] [INFO] [checkpointing.py:560:forward] ----Synchronization False
[2023-03-17 12:12:49,189] [INFO] [checkpointing.py:561:forward] ----Profiling time in checkpointing False
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
gpu108-23-r:64497:638 [1] NCCL INFO Using network IB
gpu108-23-l:42420:43842 [3] NCCL INFO Using network IB
gpu108-23-r:64498:641 [2] NCCL INFO Using network IB
gpu108-23-r:64499:644 [3] NCCL INFO Using network IB
gpu108-23-l:42418:43846 [1] NCCL INFO Using network IB
gpu108-23-r:64496:642 [0] NCCL INFO Using network IB
gpu108-23-l:42419:43848 [2] NCCL INFO Using network IB
gpu108-23-l:42417:43845 [0] NCCL INFO Using network IB
gpu108-23-r:64497:638 [1] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:64497:638 [1] NCCL INFO Setting affinity for GPU 1 to 0fffffff,00000000
gpu108-23-r:64497:638 [1] NCCL INFO Channel 00/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 01/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 02/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 03/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 04/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 05/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 06/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 07/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 08/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 09/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 10/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 11/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 12/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 13/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 14/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 15/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 16/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 17/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 18/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 19/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 20/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 21/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 22/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 23/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 24/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 25/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 26/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 27/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 28/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 29/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 30/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Channel 31/32 :    0
gpu108-23-r:64497:638 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-r:64497:638 [1] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:64497:638 [1] NCCL INFO Connected all rings
gpu108-23-r:64497:638 [1] NCCL INFO Connected all trees
gpu108-23-r:64497:638 [1] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:64498:641 [2] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:64499:644 [3] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:64498:641 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff
gpu108-23-r:64498:641 [2] NCCL INFO Channel 00/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 01/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 02/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 03/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 04/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 05/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 06/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 07/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 08/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 09/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 10/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 11/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 12/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 13/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 14/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 15/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 16/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 17/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 18/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 19/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 20/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 21/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 22/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 23/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 24/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 25/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 26/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 27/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 28/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 29/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 30/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Channel 31/32 :    0
gpu108-23-r:64498:641 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-r:64498:641 [2] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:64496:642 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:64499:644 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff
gpu108-23-r:64499:644 [3] NCCL INFO Channel 00/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 01/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 02/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 03/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 04/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 05/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 06/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 07/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 08/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 09/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 10/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 11/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 12/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 13/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 14/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 15/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 16/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 17/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 18/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 19/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 20/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 21/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 22/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 23/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 24/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 25/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 26/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 27/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 28/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 29/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 30/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Channel 31/32 :    0
gpu108-23-r:64499:644 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-r:64499:644 [3] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:64496:642 [0] NCCL INFO Setting affinity for GPU 0 to 0fffffff,00000000
gpu108-23-r:64499:644 [3] NCCL INFO Connected all rings
gpu108-23-r:64499:644 [3] NCCL INFO Connected all trees
gpu108-23-r:64499:644 [3] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:64496:642 [0] NCCL INFO Channel 00/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 01/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 02/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 03/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 04/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 05/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 06/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 07/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 08/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 09/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 10/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 11/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 12/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 13/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 14/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 15/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 16/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 17/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 18/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 19/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 20/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 21/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 22/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 23/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 24/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 25/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 26/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 27/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 28/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 29/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 30/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Channel 31/32 :    0
gpu108-23-r:64496:642 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-r:64496:642 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:64496:642 [0] NCCL INFO Connected all rings
gpu108-23-r:64496:642 [0] NCCL INFO Connected all trees
gpu108-23-r:64496:642 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-l:42420:43842 [3] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:64498:641 [2] NCCL INFO Connected all rings
gpu108-23-r:64498:641 [2] NCCL INFO Connected all trees
gpu108-23-r:64498:641 [2] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-l:42420:43842 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 00/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 01/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 02/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 03/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 04/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 05/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 06/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 07/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 08/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 09/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 10/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 11/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 12/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 13/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 14/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 15/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 16/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 17/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 18/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 19/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 20/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 21/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 22/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 23/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 24/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 25/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 26/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 27/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 28/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 29/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 30/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Channel 31/32 :    0
gpu108-23-l:42420:43842 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-l:42420:43842 [3] NCCL INFO P2P Chunksize set to 131072
gpu108-23-l:42418:43846 [1] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-l:42420:43842 [3] NCCL INFO Connected all rings
gpu108-23-l:42420:43842 [3] NCCL INFO Connected all trees
gpu108-23-l:42420:43842 [3] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-l:42418:43846 [1] NCCL INFO Setting affinity for GPU 1 to 0fffffff,00000000
gpu108-23-l:42419:43848 [2] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 00/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 01/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 02/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 03/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 04/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 05/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 06/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 07/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 08/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 09/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 10/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 11/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 12/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 13/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 14/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 15/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 16/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 17/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 18/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 19/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 20/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 21/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 22/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 23/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 24/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 25/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 26/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 27/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 28/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 29/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 30/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Channel 31/32 :    0
gpu108-23-l:42418:43846 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-l:42418:43846 [1] NCCL INFO P2P Chunksize set to 131072
gpu108-23-l:42417:43845 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-l:42418:43846 [1] NCCL INFO Connected all rings
gpu108-23-l:42418:43846 [1] NCCL INFO Connected all trees
gpu108-23-l:42418:43846 [1] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-l:42419:43848 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff
gpu108-23-l:42417:43845 [0] NCCL INFO Setting affinity for GPU 0 to 0fffffff,00000000
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 00/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 01/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 00/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 01/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 02/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 03/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 04/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 05/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 02/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 03/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 04/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 05/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 06/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 07/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 08/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 09/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 06/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 07/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 08/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 09/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 10/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 11/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 12/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 13/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 14/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 15/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 10/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 11/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 12/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 13/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 14/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 15/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 16/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 17/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 18/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 19/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 20/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 21/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 16/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 17/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 18/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 19/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 20/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 21/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 22/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 23/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 24/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 25/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 26/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 27/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 28/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 29/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 30/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 22/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 23/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 24/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 25/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 26/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 27/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 28/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 29/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 30/32 :    0
gpu108-23-l:42417:43845 [0] NCCL INFO Channel 31/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Channel 31/32 :    0
gpu108-23-l:42419:43848 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-l:42419:43848 [2] NCCL INFO P2P Chunksize set to 131072
gpu108-23-l:42417:43845 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-l:42417:43845 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-23-l:42419:43848 [2] NCCL INFO Connected all rings
gpu108-23-l:42419:43848 [2] NCCL INFO Connected all trees
gpu108-23-l:42419:43848 [2] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-l:42417:43845 [0] NCCL INFO Connected all rings
gpu108-23-l:42417:43845 [0] NCCL INFO Connected all trees
gpu108-23-l:42417:43845 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:64497:638 [1] NCCL INFO comm 0x6ec2e530 rank 0 nranks 1 cudaDev 1 busId 46000 commId 0x6bdd410475fd6b65 - Init COMPLETE
gpu108-23-r:64496:642 [0] NCCL INFO comm 0x70303fe0 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0x2d7445e7c6c5ba26 - Init COMPLETE
gpu108-23-r:64499:644 [3] NCCL INFO comm 0x6f450e30 rank 0 nranks 1 cudaDev 3 busId c7000 commId 0x1892571f0061a13d - Init COMPLETE
gpu108-23-r:64498:641 [2] NCCL INFO comm 0x6f1ab380 rank 0 nranks 1 cudaDev 2 busId 85000 commId 0x7a2ea212b2bd7d34 - Init COMPLETE
gpu108-23-l:42418:43846 [1] NCCL INFO comm 0x71126ea0 rank 0 nranks 1 cudaDev 1 busId 46000 commId 0x84cb72dcda0c3430 - Init COMPLETE
gpu108-23-l:42420:43842 [3] NCCL INFO comm 0x6e4fc990 rank 0 nranks 1 cudaDev 3 busId c7000 commId 0x49f9f45e9afb40d1 - Init COMPLETE
gpu108-23-l:42417:43845 [0] NCCL INFO comm 0x6e1298f0 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0xb8f97c0f683c1c39 - Init COMPLETE
gpu108-23-l:42419:43848 [2] NCCL INFO comm 0x6ddc6780 rank 0 nranks 1 cudaDev 2 busId 85000 commId 0xc9d4bc3677287f86 - Init COMPLETE
[2023-03-17 12:12:50,482] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 1.02
[2023-03-17 12:12:50,483] [INFO] [logging.py:93:log_dist] [Rank 0] step=1, skipped=0, lr=[5.9946721667563326e-05, 5.9946721667563326e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:50,483] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 906.43 | backward_microstep: 227.13 | backward_inner_microstep: 188.85 | backward_allreduce_microstep: 37.50 | step_microstep: 155.51
[2023-03-17 12:12:50,484] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 906.34 | backward: 227.10 | backward_inner: 188.95 | backward_allreduce: 37.50 | step: 155.52
 iteration        1/      50 | consumed samples:          256 | consumed tokens:       262144 | elapsed time per iteration (ms): 1306.4 | learning rate: 5.995E-05 | global batch size:    32 | lm loss: 1.084315E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.496 | TFLOPs: 1.34 |
[Rank 0] (after 1 iterations) memory (MB) | allocated: 300.0537109375 | max allocated: 2629.6298828125 | reserved: 3816.0 | max reserved: 3816.0
time (ms) | forward-compute: 918.49 | backward-compute: 229.19 | backward-embedding-all-reduce: 0.01 | optimizer: 156.12 | batch-generator: 7.09
[2023-03-17 12:12:50,843] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.66
[2023-03-17 12:12:50,843] [INFO] [logging.py:93:log_dist] [Rank 0] step=2, skipped=0, lr=[5.97870969354909e-05, 5.97870969354909e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:50,844] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 128.09 | backward_microstep: 216.07 | backward_inner_microstep: 180.31 | backward_allreduce_microstep: 35.01 | step_microstep: 3.90
[2023-03-17 12:12:50,844] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 128.01 | backward: 216.05 | backward_inner: 180.42 | backward_allreduce: 34.99 | step: 3.90
 iteration        2/      50 | consumed samples:          512 | consumed tokens:       524288 | elapsed time per iteration (ms): 360.5 | learning rate: 5.979E-05 | global batch size:    32 | lm loss: 1.084111E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 88.755 | TFLOPs: 4.85 |
time (ms) | forward-compute: 137.31 | backward-compute: 216.36 | backward-embedding-all-reduce: 0.01 | optimizer: 4.67 | batch-generator: 4.84
[2023-03-17 12:12:51,202] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.66
[2023-03-17 12:12:51,203] [INFO] [logging.py:93:log_dist] [Rank 0] step=3, skipped=0, lr=[5.95217557696746e-05, 5.95217557696746e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:51,203] [INFO] [timer.py:198:stop] epoch=0/micro_step=3/global_step=3, RunningAvgSamplesPerSec=676.241600999617, CurrSamplesPerSec=676.241600999617, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:12:51,204] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.87 | backward_microstep: 215.62 | backward_inner_microstep: 179.93 | backward_allreduce_microstep: 34.91 | step_microstep: 4.20
[2023-03-17 12:12:51,204] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.78 | backward: 215.60 | backward_inner: 180.05 | backward_allreduce: 34.90 | step: 4.21
 iteration        3/      50 | consumed samples:          768 | consumed tokens:       786432 | elapsed time per iteration (ms): 359.4 | learning rate: 5.952E-05 | global batch size:    32 | lm loss: 1.083868E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.049 | TFLOPs: 4.86 |
time (ms) | forward-compute: 136.75 | backward-compute: 216.19 | backward-embedding-all-reduce: 0.01 | optimizer: 4.71 | batch-generator: 4.79
[2023-03-17 12:12:51,561] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:12:51,561] [INFO] [logging.py:93:log_dist] [Rank 0] step=4, skipped=0, lr=[5.9151745350473036e-05, 5.9151745350473036e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:51,562] [INFO] [timer.py:198:stop] epoch=0/micro_step=4/global_step=4, RunningAvgSamplesPerSec=676.7377080176776, CurrSamplesPerSec=677.2345434821001, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:12:51,562] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.69 | backward_microstep: 215.63 | backward_inner_microstep: 180.08 | backward_allreduce_microstep: 34.81 | step_microstep: 4.10
[2023-03-17 12:12:51,563] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.61 | backward: 215.61 | backward_inner: 180.20 | backward_allreduce: 34.79 | step: 4.11
 iteration        4/      50 | consumed samples:         1024 | consumed tokens:      1048576 | elapsed time per iteration (ms): 358.7 | learning rate: 5.915E-05 | global batch size:    32 | lm loss: 1.083617E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.205 | TFLOPs: 4.87 |
time (ms) | forward-compute: 136.51 | backward-compute: 215.84 | backward-embedding-all-reduce: 0.01 | optimizer: 4.68 | batch-generator: 4.82
[2023-03-17 12:12:51,920] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.72
[2023-03-17 12:12:51,920] [INFO] [logging.py:93:log_dist] [Rank 0] step=5, skipped=0, lr=[5.8678525939969144e-05, 5.8678525939969144e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:51,921] [INFO] [timer.py:198:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=677.7783119388363, CurrSamplesPerSec=679.8691500731953, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:12:51,921] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.74 | backward_microstep: 215.54 | backward_inner_microstep: 180.08 | backward_allreduce_microstep: 34.74 | step_microstep: 4.26
[2023-03-17 12:12:51,922] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.66 | backward: 215.51 | backward_inner: 180.18 | backward_allreduce: 34.73 | step: 4.27
 iteration        5/      50 | consumed samples:         1280 | consumed tokens:      1310720 | elapsed time per iteration (ms): 358.8 | learning rate: 5.868E-05 | global batch size:    32 | lm loss: 1.083390E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.175 | TFLOPs: 4.87 |
time (ms) | forward-compute: 136.89 | backward-compute: 215.56 | backward-embedding-all-reduce: 0.01 | optimizer: 4.66 | batch-generator: 4.69
[2023-03-17 12:12:52,278] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.68
[2023-03-17 12:12:52,278] [INFO] [logging.py:93:log_dist] [Rank 0] step=6, skipped=0, lr=[5.810396511898279e-05, 5.810396511898279e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:52,278] [INFO] [timer.py:198:stop] epoch=0/micro_step=6/global_step=6, RunningAvgSamplesPerSec=678.022265189393, CurrSamplesPerSec=678.7551797553365, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:12:52,279] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.41 | backward_microstep: 214.96 | backward_inner_microstep: 179.83 | backward_allreduce_microstep: 34.36 | step_microstep: 3.97
[2023-03-17 12:12:52,280] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.33 | backward: 214.94 | backward_inner: 179.96 | backward_allreduce: 34.34 | step: 3.98
 iteration        6/      50 | consumed samples:         1536 | consumed tokens:      1572864 | elapsed time per iteration (ms): 358.1 | learning rate: 5.810E-05 | global batch size:    32 | lm loss: 1.083029E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.369 | TFLOPs: 4.88 |
time (ms) | forward-compute: 136.57 | backward-compute: 214.65 | backward-embedding-all-reduce: 0.01 | optimizer: 4.91 | batch-generator: 5.00
[2023-03-17 12:12:52,636] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.67
[2023-03-17 12:12:52,636] [INFO] [logging.py:93:log_dist] [Rank 0] step=7, skipped=0, lr=[5.743033041658253e-05, 5.743033041658253e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:52,636] [INFO] [timer.py:198:stop] epoch=0/micro_step=7/global_step=7, RunningAvgSamplesPerSec=679.1515269276855, CurrSamplesPerSec=683.7064423943884, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:12:52,637] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.76 | backward_microstep: 214.39 | backward_inner_microstep: 179.90 | backward_allreduce_microstep: 33.68 | step_microstep: 4.07
[2023-03-17 12:12:52,637] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.68 | backward: 214.37 | backward_inner: 180.05 | backward_allreduce: 33.66 | step: 4.07
 iteration        7/      50 | consumed samples:         1792 | consumed tokens:      1835008 | elapsed time per iteration (ms): 357.5 | learning rate: 5.743E-05 | global batch size:    32 | lm loss: 1.082633E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.501 | TFLOPs: 4.89 |
time (ms) | forward-compute: 136.23 | backward-compute: 214.90 | backward-embedding-all-reduce: 0.01 | optimizer: 4.64 | batch-generator: 4.60
[2023-03-17 12:12:52,994] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.67
[2023-03-17 12:12:52,994] [INFO] [logging.py:93:log_dist] [Rank 0] step=8, skipped=0, lr=[5.666028036118432e-05, 5.666028036118432e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:52,994] [INFO] [timer.py:198:stop] epoch=0/micro_step=8/global_step=8, RunningAvgSamplesPerSec=679.4337854700357, CurrSamplesPerSec=680.848604749078, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:12:52,995] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.37 | backward_microstep: 215.86 | backward_inner_microstep: 179.98 | backward_allreduce_microstep: 35.09 | step_microstep: 4.12
[2023-03-17 12:12:52,995] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.29 | backward: 215.83 | backward_inner: 180.10 | backward_allreduce: 35.06 | step: 4.13
 iteration        8/      50 | consumed samples:         2048 | consumed tokens:      2097152 | elapsed time per iteration (ms): 358.3 | learning rate: 5.666E-05 | global batch size:    32 | lm loss: 1.082305E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.315 | TFLOPs: 4.88 |
time (ms) | forward-compute: 136.68 | backward-compute: 215.17 | backward-embedding-all-reduce: 0.01 | optimizer: 4.67 | batch-generator: 4.63
[2023-03-17 12:12:53,351] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.66
[2023-03-17 12:12:53,352] [INFO] [logging.py:93:log_dist] [Rank 0] step=9, skipped=0, lr=[5.579685398855441e-05, 5.579685398855441e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:53,352] [INFO] [timer.py:198:stop] epoch=0/micro_step=9/global_step=9, RunningAvgSamplesPerSec=679.963622336395, CurrSamplesPerSec=683.1600785886617, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:12:53,352] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.52 | backward_microstep: 214.97 | backward_inner_microstep: 179.81 | backward_allreduce_microstep: 34.39 | step_microstep: 4.05
[2023-03-17 12:12:53,353] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.44 | backward: 214.94 | backward_inner: 179.93 | backward_allreduce: 34.37 | step: 4.06
 iteration        9/      50 | consumed samples:         2304 | consumed tokens:      2359296 | elapsed time per iteration (ms): 357.5 | learning rate: 5.580E-05 | global batch size:    32 | lm loss: 1.081878E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.517 | TFLOPs: 4.89 |
time (ms) | forward-compute: 136.47 | backward-compute: 214.59 | backward-embedding-all-reduce: 0.01 | optimizer: 4.67 | batch-generator: 4.83
[2023-03-17 12:12:53,708] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.67
[2023-03-17 12:12:53,709] [INFO] [logging.py:93:log_dist] [Rank 0] step=10, skipped=0, lr=[5.4843458848123576e-05, 5.4843458848123576e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:53,709] [INFO] [timer.py:198:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=679.8751768163806, CurrSamplesPerSec=679.2567018396214, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:12:53,710] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.35 | backward_microstep: 214.68 | backward_inner_microstep: 179.77 | backward_allreduce_microstep: 34.14 | step_microstep: 4.00
[2023-03-17 12:12:53,710] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.27 | backward: 214.66 | backward_inner: 179.90 | backward_allreduce: 34.12 | step: 4.00
 iteration       10/      50 | consumed samples:         2560 | consumed tokens:      2621440 | elapsed time per iteration (ms): 357.1 | learning rate: 5.484E-05 | global batch size:    32 | lm loss: 1.081315E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.616 | TFLOPs: 4.89 |
time (ms) | forward-compute: 136.37 | backward-compute: 214.34 | backward-embedding-all-reduce: 0.01 | optimizer: 4.64 | batch-generator: 4.67
[2023-03-17 12:12:54,066] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.67
[2023-03-17 12:12:54,066] [INFO] [logging.py:93:log_dist] [Rank 0] step=11, skipped=0, lr=[5.380385755494631e-05, 5.380385755494631e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:54,067] [INFO] [timer.py:198:stop] epoch=0/micro_step=11/global_step=11, RunningAvgSamplesPerSec=679.8863696229206, CurrSamplesPerSec=679.9759253442494, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:12:54,067] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.70 | backward_microstep: 214.68 | backward_inner_microstep: 179.58 | backward_allreduce_microstep: 34.32 | step_microstep: 4.44
[2023-03-17 12:12:54,068] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.62 | backward: 214.65 | backward_inner: 179.70 | backward_allreduce: 34.30 | step: 4.44
 iteration       11/      50 | consumed samples:         2816 | consumed tokens:      2883584 | elapsed time per iteration (ms): 357.3 | learning rate: 5.380E-05 | global batch size:    32 | lm loss: 1.080804E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.556 | TFLOPs: 4.89 |
time (ms) | forward-compute: 136.06 | backward-compute: 214.61 | backward-embedding-all-reduce: 0.01 | optimizer: 4.92 | batch-generator: 4.82
[2023-03-17 12:12:54,423] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:12:54,423] [INFO] [logging.py:93:log_dist] [Rank 0] step=12, skipped=0, lr=[5.2682152940378117e-05, 5.2682152940378117e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:54,424] [INFO] [timer.py:198:stop] epoch=0/micro_step=12/global_step=12, RunningAvgSamplesPerSec=679.890846848727, CurrSamplesPerSec=679.9311445346734, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:12:54,424] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.31 | backward_microstep: 214.66 | backward_inner_microstep: 179.60 | backward_allreduce_microstep: 34.30 | step_microstep: 4.09
[2023-03-17 12:12:54,425] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.22 | backward: 214.64 | backward_inner: 179.73 | backward_allreduce: 34.27 | step: 4.09
 iteration       12/      50 | consumed samples:         3072 | consumed tokens:      3145728 | elapsed time per iteration (ms): 357.3 | learning rate: 5.268E-05 | global batch size:    32 | lm loss: 1.080132E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.555 | TFLOPs: 4.89 |
time (ms) | forward-compute: 136.15 | backward-compute: 214.70 | backward-embedding-all-reduce: 0.01 | optimizer: 4.70 | batch-generator: 4.86
[2023-03-17 12:12:54,780] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.67
[2023-03-17 12:12:54,780] [INFO] [logging.py:93:log_dist] [Rank 0] step=13, skipped=0, lr=[5.14827718600746e-05, 5.14827718600746e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:54,780] [INFO] [timer.py:198:stop] epoch=0/micro_step=13/global_step=13, RunningAvgSamplesPerSec=680.0276028393229, CurrSamplesPerSec=681.398194685593, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:12:54,781] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.42 | backward_microstep: 214.31 | backward_inner_microstep: 179.84 | backward_allreduce_microstep: 33.69 | step_microstep: 4.23
[2023-03-17 12:12:54,781] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.34 | backward: 214.28 | backward_inner: 179.97 | backward_allreduce: 33.67 | step: 4.23
 iteration       13/      50 | consumed samples:         3328 | consumed tokens:      3407872 | elapsed time per iteration (ms): 356.7 | learning rate: 5.148E-05 | global batch size:    32 | lm loss: 1.079512E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.704 | TFLOPs: 4.90 |
time (ms) | forward-compute: 135.87 | backward-compute: 214.32 | backward-embedding-all-reduce: 0.02 | optimizer: 4.78 | batch-generator: 4.74
[2023-03-17 12:12:55,136] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.67
[2023-03-17 12:12:55,137] [INFO] [logging.py:93:log_dist] [Rank 0] step=14, skipped=0, lr=[5.021044772321462e-05, 5.021044772321462e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:55,137] [INFO] [timer.py:198:stop] epoch=0/micro_step=14/global_step=14, RunningAvgSamplesPerSec=680.1910128160893, CurrSamplesPerSec=681.9937195760206, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:12:55,138] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.37 | backward_microstep: 214.17 | backward_inner_microstep: 179.86 | backward_allreduce_microstep: 33.55 | step_microstep: 3.92
[2023-03-17 12:12:55,138] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.29 | backward: 214.14 | backward_inner: 179.97 | backward_allreduce: 33.53 | step: 3.93
 iteration       14/      50 | consumed samples:         3584 | consumed tokens:      3670016 | elapsed time per iteration (ms): 356.6 | learning rate: 5.021E-05 | global batch size:    32 | lm loss: 1.078674E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.742 | TFLOPs: 4.90 |
time (ms) | forward-compute: 135.93 | backward-compute: 214.23 | backward-embedding-all-reduce: 0.01 | optimizer: 4.65 | batch-generator: 4.96
[2023-03-17 12:12:55,494] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.66
[2023-03-17 12:12:55,494] [INFO] [logging.py:93:log_dist] [Rank 0] step=15, skipped=0, lr=[4.887020181189677e-05, 4.887020181189677e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:55,494] [INFO] [timer.py:198:stop] epoch=0/micro_step=15/global_step=15, RunningAvgSamplesPerSec=680.3102453760727, CurrSamplesPerSec=681.7443034631287, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:12:55,495] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.72 | backward_microstep: 214.66 | backward_inner_microstep: 179.85 | backward_allreduce_microstep: 34.03 | step_microstep: 3.94
[2023-03-17 12:12:55,495] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.64 | backward: 214.64 | backward_inner: 179.98 | backward_allreduce: 34.01 | step: 3.94
 iteration       15/      50 | consumed samples:         3840 | consumed tokens:      3932160 | elapsed time per iteration (ms): 357.1 | learning rate: 4.887E-05 | global batch size:    32 | lm loss: 1.077656E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.606 | TFLOPs: 4.89 |
time (ms) | forward-compute: 136.17 | backward-compute: 214.54 | backward-embedding-all-reduce: 0.01 | optimizer: 4.67 | batch-generator: 4.80
[2023-03-17 12:12:55,852] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.67
[2023-03-17 12:12:55,852] [INFO] [logging.py:93:log_dist] [Rank 0] step=16, skipped=0, lr=[4.74673234644329e-05, 4.74673234644329e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:55,852] [INFO] [timer.py:198:stop] epoch=0/micro_step=16/global_step=16, RunningAvgSamplesPerSec=680.1972094168156, CurrSamplesPerSec=678.7311527803063, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:12:55,853] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.58 | backward_microstep: 214.15 | backward_inner_microstep: 180.04 | backward_allreduce_microstep: 33.34 | step_microstep: 4.10
[2023-03-17 12:12:55,853] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.49 | backward: 214.13 | backward_inner: 180.17 | backward_allreduce: 33.31 | step: 4.11
 iteration       16/      50 | consumed samples:         4096 | consumed tokens:      4194304 | elapsed time per iteration (ms): 358.3 | learning rate: 4.747E-05 | global batch size:    32 | lm loss: 1.076603E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.304 | TFLOPs: 4.88 |
time (ms) | forward-compute: 137.10 | backward-compute: 214.77 | backward-embedding-all-reduce: 0.01 | optimizer: 4.69 | batch-generator: 4.84
[2023-03-17 12:12:56,210] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.66
[2023-03-17 12:12:56,210] [INFO] [logging.py:93:log_dist] [Rank 0] step=17, skipped=0, lr=[4.6007349200746303e-05, 4.6007349200746303e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:56,210] [INFO] [timer.py:198:stop] epoch=0/micro_step=17/global_step=17, RunningAvgSamplesPerSec=680.327947073219, CurrSamplesPerSec=682.1635654856597, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:12:56,211] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.62 | backward_microstep: 214.67 | backward_inner_microstep: 180.34 | backward_allreduce_microstep: 33.55 | step_microstep: 4.21
[2023-03-17 12:12:56,211] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.54 | backward: 214.65 | backward_inner: 180.47 | backward_allreduce: 33.53 | step: 4.22
 iteration       17/      50 | consumed samples:         4352 | consumed tokens:      4456448 | elapsed time per iteration (ms): 358.0 | learning rate: 4.601E-05 | global batch size:    32 | lm loss: 1.075349E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.391 | TFLOPs: 4.88 |
time (ms) | forward-compute: 136.65 | backward-compute: 214.72 | backward-embedding-all-reduce: 0.01 | optimizer: 4.82 | batch-generator: 4.82
[2023-03-17 12:12:56,567] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.67
[2023-03-17 12:12:56,567] [INFO] [logging.py:93:log_dist] [Rank 0] step=18, skipped=0, lr=[4.4496040872256956e-05, 4.4496040872256956e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:56,568] [INFO] [timer.py:198:stop] epoch=0/micro_step=18/global_step=18, RunningAvgSamplesPerSec=680.6375595621827, CurrSamplesPerSec=685.3157959233691, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:12:56,568] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.69 | backward_microstep: 214.94 | backward_inner_microstep: 179.92 | backward_allreduce_microstep: 34.25 | step_microstep: 4.07
[2023-03-17 12:12:56,569] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.61 | backward: 214.92 | backward_inner: 180.04 | backward_allreduce: 34.22 | step: 4.08
 iteration       18/      50 | consumed samples:         4608 | consumed tokens:      4718592 | elapsed time per iteration (ms): 357.1 | learning rate: 4.450E-05 | global batch size:    32 | lm loss: 1.074199E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.610 | TFLOPs: 4.89 |
time (ms) | forward-compute: 136.46 | backward-compute: 214.30 | backward-embedding-all-reduce: 0.01 | optimizer: 4.61 | batch-generator: 4.83
[2023-03-17 12:12:56,925] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.66
[2023-03-17 12:12:56,925] [INFO] [logging.py:93:log_dist] [Rank 0] step=19, skipped=0, lr=[4.293936292248631e-05, 4.293936292248631e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:56,926] [INFO] [timer.py:198:stop] epoch=0/micro_step=19/global_step=19, RunningAvgSamplesPerSec=680.1800465337429, CurrSamplesPerSec=672.9425968543337, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:12:56,926] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.66 | backward_microstep: 214.68 | backward_inner_microstep: 180.01 | backward_allreduce_microstep: 33.90 | step_microstep: 3.96
[2023-03-17 12:12:56,927] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.58 | backward: 214.65 | backward_inner: 180.13 | backward_allreduce: 33.88 | step: 3.96
 iteration       19/      50 | consumed samples:         4864 | consumed tokens:      4980736 | elapsed time per iteration (ms): 358.0 | learning rate: 4.294E-05 | global batch size:    32 | lm loss: 1.072808E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.385 | TFLOPs: 4.88 |
time (ms) | forward-compute: 136.64 | backward-compute: 214.75 | backward-embedding-all-reduce: 0.01 | optimizer: 4.82 | batch-generator: 4.82
[2023-03-17 12:12:57,282] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.67
[2023-03-17 12:12:57,283] [INFO] [logging.py:93:log_dist] [Rank 0] step=20, skipped=0, lr=[4.134345884812357e-05, 4.134345884812357e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:57,283] [INFO] [timer.py:198:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=680.1187274822772, CurrSamplesPerSec=679.0779925827359, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:12:57,284] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.37 | backward_microstep: 214.80 | backward_inner_microstep: 179.75 | backward_allreduce_microstep: 34.26 | step_microstep: 4.09
[2023-03-17 12:12:57,284] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.29 | backward: 214.77 | backward_inner: 179.87 | backward_allreduce: 34.24 | step: 4.10
 iteration       20/      50 | consumed samples:         5120 | consumed tokens:      5242880 | elapsed time per iteration (ms): 357.4 | learning rate: 4.134E-05 | global batch size:    32 | lm loss: 1.071156E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.535 | TFLOPs: 4.89 |
time (ms) | forward-compute: 136.01 | backward-compute: 215.00 | backward-embedding-all-reduce: 0.01 | optimizer: 4.62 | batch-generator: 4.66
[2023-03-17 12:12:57,640] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.67
[2023-03-17 12:12:57,640] [INFO] [logging.py:93:log_dist] [Rank 0] step=21, skipped=0, lr=[3.971462695345109e-05, 3.971462695345109e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:57,640] [INFO] [timer.py:198:stop] epoch=0/micro_step=21/global_step=21, RunningAvgSamplesPerSec=680.2679610448781, CurrSamplesPerSec=682.9654084529976, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:12:57,641] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.66 | backward_microstep: 214.61 | backward_inner_microstep: 179.61 | backward_allreduce_microstep: 34.24 | step_microstep: 4.08
[2023-03-17 12:12:57,641] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.57 | backward: 214.58 | backward_inner: 179.72 | backward_allreduce: 34.23 | step: 4.08
 iteration       21/      50 | consumed samples:         5376 | consumed tokens:      5505024 | elapsed time per iteration (ms): 357.1 | learning rate: 3.971E-05 | global batch size:    32 | lm loss: 1.069096E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.613 | TFLOPs: 4.89 |
time (ms) | forward-compute: 136.40 | backward-compute: 214.33 | backward-embedding-all-reduce: 0.01 | optimizer: 4.63 | batch-generator: 4.72
[2023-03-17 12:12:57,997] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.66
[2023-03-17 12:12:57,997] [INFO] [logging.py:93:log_dist] [Rank 0] step=22, skipped=0, lr=[3.805929549381457e-05, 3.805929549381457e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:57,998] [INFO] [timer.py:198:stop] epoch=0/micro_step=22/global_step=22, RunningAvgSamplesPerSec=680.4335441874849, CurrSamplesPerSec=683.5950107211434, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:12:57,998] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.36 | backward_microstep: 214.82 | backward_inner_microstep: 180.32 | backward_allreduce_microstep: 33.73 | step_microstep: 4.13
[2023-03-17 12:12:57,999] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.29 | backward: 214.79 | backward_inner: 180.45 | backward_allreduce: 33.69 | step: 4.14
 iteration       22/      50 | consumed samples:         5632 | consumed tokens:      5767168 | elapsed time per iteration (ms): 357.5 | learning rate: 3.806E-05 | global batch size:    32 | lm loss: 1.067315E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.512 | TFLOPs: 4.89 |
time (ms) | forward-compute: 136.43 | backward-compute: 214.67 | backward-embedding-all-reduce: 0.01 | optimizer: 4.68 | batch-generator: 4.77
[2023-03-17 12:12:58,354] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.66
[2023-03-17 12:12:58,355] [INFO] [logging.py:93:log_dist] [Rank 0] step=23, skipped=0, lr=[3.638399730623622e-05, 3.638399730623622e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:58,355] [INFO] [timer.py:198:stop] epoch=0/micro_step=23/global_step=23, RunningAvgSamplesPerSec=680.3884156613985, CurrSamplesPerSec=679.4871005629582, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:12:58,356] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.41 | backward_microstep: 215.08 | backward_inner_microstep: 179.82 | backward_allreduce_microstep: 34.50 | step_microstep: 3.89
[2023-03-17 12:12:58,356] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.33 | backward: 215.06 | backward_inner: 179.94 | backward_allreduce: 34.47 | step: 3.89
 iteration       23/      50 | consumed samples:         5888 | consumed tokens:      6029312 | elapsed time per iteration (ms): 357.4 | learning rate: 3.638E-05 | global batch size:    32 | lm loss: 1.065398E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.535 | TFLOPs: 4.89 |
time (ms) | forward-compute: 136.29 | backward-compute: 214.45 | backward-embedding-all-reduce: 0.01 | optimizer: 4.91 | batch-generator: 4.73
[2023-03-17 12:12:58,712] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.66
[2023-03-17 12:12:58,712] [INFO] [logging.py:93:log_dist] [Rank 0] step=24, skipped=0, lr=[3.469534402729146e-05, 3.469534402729146e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:58,712] [INFO] [timer.py:198:stop] epoch=0/micro_step=24/global_step=24, RunningAvgSamplesPerSec=680.340497653891, CurrSamplesPerSec=679.3357763245804, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:12:58,713] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.67 | backward_microstep: 214.38 | backward_inner_microstep: 179.77 | backward_allreduce_microstep: 33.84 | step_microstep: 4.11
[2023-03-17 12:12:58,713] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.59 | backward: 214.36 | backward_inner: 179.89 | backward_allreduce: 33.82 | step: 4.11
 iteration       24/      50 | consumed samples:         6144 | consumed tokens:      6291456 | elapsed time per iteration (ms): 357.4 | learning rate: 3.470E-05 | global batch size:    32 | lm loss: 1.062967E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.539 | TFLOPs: 4.89 |
time (ms) | forward-compute: 136.26 | backward-compute: 214.69 | backward-embedding-all-reduce: 0.01 | optimizer: 4.66 | batch-generator: 4.66
[2023-03-17 12:12:59,069] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.67
[2023-03-17 12:12:59,070] [INFO] [logging.py:93:log_dist] [Rank 0] step=25, skipped=0, lr=[3.3e-05, 3.3e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:59,070] [INFO] [timer.py:198:stop] epoch=0/micro_step=25/global_step=25, RunningAvgSamplesPerSec=680.0924729571941, CurrSamplesPerSec=674.6813180184482, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:12:59,071] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.38 | backward_microstep: 214.68 | backward_inner_microstep: 179.76 | backward_allreduce_microstep: 34.14 | step_microstep: 4.09
[2023-03-17 12:12:59,071] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.30 | backward: 214.66 | backward_inner: 179.90 | backward_allreduce: 34.10 | step: 4.10
 iteration       25/      50 | consumed samples:         6400 | consumed tokens:      6553600 | elapsed time per iteration (ms): 357.6 | learning rate: 3.300E-05 | global batch size:    32 | lm loss: 1.060745E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.493 | TFLOPs: 4.89 |
time (ms) | forward-compute: 136.71 | backward-compute: 214.48 | backward-embedding-all-reduce: 0.01 | optimizer: 4.67 | batch-generator: 4.72
[2023-03-17 12:12:59,427] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.67
[2023-03-17 12:12:59,427] [INFO] [logging.py:93:log_dist] [Rank 0] step=26, skipped=0, lr=[3.1304655972708536e-05, 3.1304655972708536e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:59,427] [INFO] [timer.py:198:stop] epoch=0/micro_step=26/global_step=26, RunningAvgSamplesPerSec=680.2240489777714, CurrSamplesPerSec=683.2644118186076, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:12:59,428] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.49 | backward_microstep: 215.10 | backward_inner_microstep: 179.73 | backward_allreduce_microstep: 34.60 | step_microstep: 3.94
[2023-03-17 12:12:59,428] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.41 | backward: 215.07 | backward_inner: 179.85 | backward_allreduce: 34.58 | step: 3.95
 iteration       26/      50 | consumed samples:         6656 | consumed tokens:      6815744 | elapsed time per iteration (ms): 357.0 | learning rate: 3.130E-05 | global batch size:    32 | lm loss: 1.058196E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.626 | TFLOPs: 4.89 |
time (ms) | forward-compute: 136.29 | backward-compute: 214.14 | backward-embedding-all-reduce: 0.01 | optimizer: 4.87 | batch-generator: 4.81
[2023-03-17 12:12:59,784] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.67
[2023-03-17 12:12:59,784] [INFO] [logging.py:93:log_dist] [Rank 0] step=27, skipped=0, lr=[2.961600269376378e-05, 2.961600269376378e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:12:59,785] [INFO] [timer.py:198:stop] epoch=0/micro_step=27/global_step=27, RunningAvgSamplesPerSec=680.264041390272, CurrSamplesPerSec=681.2252720480753, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:12:59,785] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.50 | backward_microstep: 215.10 | backward_inner_microstep: 180.59 | backward_allreduce_microstep: 33.74 | step_microstep: 3.95
[2023-03-17 12:12:59,786] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.41 | backward: 215.07 | backward_inner: 180.71 | backward_allreduce: 33.72 | step: 3.96
 iteration       27/      50 | consumed samples:         6912 | consumed tokens:      7077888 | elapsed time per iteration (ms): 357.3 | learning rate: 2.962E-05 | global batch size:    32 | lm loss: 1.055830E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.549 | TFLOPs: 4.89 |
time (ms) | forward-compute: 136.05 | backward-compute: 214.78 | backward-embedding-all-reduce: 0.01 | optimizer: 4.73 | batch-generator: 4.64
[2023-03-17 12:13:00,141] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.67
[2023-03-17 12:13:00,141] [INFO] [logging.py:93:log_dist] [Rank 0] step=28, skipped=0, lr=[2.7940704506185428e-05, 2.7940704506185428e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:13:00,141] [INFO] [timer.py:198:stop] epoch=0/micro_step=28/global_step=28, RunningAvgSamplesPerSec=680.2560054020304, CurrSamplesPerSec=680.0551673819308, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:13:00,142] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.30 | backward_microstep: 214.43 | backward_inner_microstep: 179.87 | backward_allreduce_microstep: 33.78 | step_microstep: 4.07
[2023-03-17 12:13:00,142] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.22 | backward: 214.40 | backward_inner: 180.00 | backward_allreduce: 33.75 | step: 4.07
 iteration       28/      50 | consumed samples:         7168 | consumed tokens:      7340032 | elapsed time per iteration (ms): 356.9 | learning rate: 2.794E-05 | global batch size:    32 | lm loss: 1.052796E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.663 | TFLOPs: 4.89 |
time (ms) | forward-compute: 135.95 | backward-compute: 214.49 | backward-embedding-all-reduce: 0.02 | optimizer: 4.70 | batch-generator: 4.70
[2023-03-17 12:13:00,499] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.66
[2023-03-17 12:13:00,499] [INFO] [logging.py:93:log_dist] [Rank 0] step=29, skipped=0, lr=[2.6285373046548923e-05, 2.6285373046548923e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:13:00,500] [INFO] [timer.py:198:stop] epoch=0/micro_step=29/global_step=29, RunningAvgSamplesPerSec=680.3191855120202, CurrSamplesPerSec=681.9659976627204, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:13:00,500] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.40 | backward_microstep: 214.90 | backward_inner_microstep: 179.83 | backward_allreduce_microstep: 34.29 | step_microstep: 4.04
[2023-03-17 12:13:00,501] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.32 | backward: 214.88 | backward_inner: 179.96 | backward_allreduce: 34.27 | step: 4.04
 iteration       29/      50 | consumed samples:         7424 | consumed tokens:      7602176 | elapsed time per iteration (ms): 358.4 | learning rate: 2.629E-05 | global batch size:    32 | lm loss: 1.049798E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.289 | TFLOPs: 4.87 |
time (ms) | forward-compute: 136.92 | backward-compute: 215.07 | backward-embedding-all-reduce: 0.01 | optimizer: 4.66 | batch-generator: 4.66
[2023-03-17 12:13:00,856] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:13:00,857] [INFO] [logging.py:93:log_dist] [Rank 0] step=30, skipped=0, lr=[2.465654115187642e-05, 2.465654115187642e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:13:00,857] [INFO] [timer.py:198:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=680.3026099562032, CurrSamplesPerSec=679.855375061417, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:13:00,857] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.37 | backward_microstep: 214.21 | backward_inner_microstep: 179.80 | backward_allreduce_microstep: 33.62 | step_microstep: 4.24
[2023-03-17 12:13:00,858] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.29 | backward: 214.18 | backward_inner: 179.93 | backward_allreduce: 33.60 | step: 4.24
 iteration       30/      50 | consumed samples:         7680 | consumed tokens:      7864320 | elapsed time per iteration (ms): 357.1 | learning rate: 2.466E-05 | global batch size:    32 | lm loss: 1.046920E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.604 | TFLOPs: 4.89 |
time (ms) | forward-compute: 136.19 | backward-compute: 214.36 | backward-embedding-all-reduce: 0.01 | optimizer: 4.83 | batch-generator: 4.90
[2023-03-17 12:13:01,214] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.67
[2023-03-17 12:13:01,214] [INFO] [logging.py:93:log_dist] [Rank 0] step=31, skipped=0, lr=[2.3060637077513695e-05, 2.3060637077513695e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:13:01,214] [INFO] [timer.py:198:stop] epoch=0/micro_step=31/global_step=31, RunningAvgSamplesPerSec=680.2372445274768, CurrSamplesPerSec=678.4120986044348, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:13:01,215] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.35 | backward_microstep: 214.83 | backward_inner_microstep: 179.81 | backward_allreduce_microstep: 34.22 | step_microstep: 3.97
[2023-03-17 12:13:01,215] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.26 | backward: 214.81 | backward_inner: 179.96 | backward_allreduce: 34.20 | step: 3.98
 iteration       31/      50 | consumed samples:         7936 | consumed tokens:      8126464 | elapsed time per iteration (ms): 357.6 | learning rate: 2.306E-05 | global batch size:    32 | lm loss: 1.043682E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.489 | TFLOPs: 4.89 |
time (ms) | forward-compute: 136.09 | backward-compute: 215.04 | backward-embedding-all-reduce: 0.01 | optimizer: 4.68 | batch-generator: 4.68
[2023-03-17 12:13:01,573] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.68
[2023-03-17 12:13:01,573] [INFO] [logging.py:93:log_dist] [Rank 0] step=32, skipped=0, lr=[2.150395912774304e-05, 2.150395912774304e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:13:01,573] [INFO] [timer.py:198:stop] epoch=0/micro_step=32/global_step=32, RunningAvgSamplesPerSec=680.2099148532243, CurrSamplesPerSec=679.4183084617409, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:13:01,574] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.61 | backward_microstep: 214.68 | backward_inner_microstep: 180.31 | backward_allreduce_microstep: 33.62 | step_microstep: 4.05
[2023-03-17 12:13:01,574] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.53 | backward: 214.65 | backward_inner: 180.41 | backward_allreduce: 33.61 | step: 4.06
 iteration       32/      50 | consumed samples:         8192 | consumed tokens:      8388608 | elapsed time per iteration (ms): 358.7 | learning rate: 2.150E-05 | global batch size:    32 | lm loss: 1.040837E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.217 | TFLOPs: 4.87 |
time (ms) | forward-compute: 137.30 | backward-compute: 215.00 | backward-embedding-all-reduce: 0.01 | optimizer: 4.66 | batch-generator: 4.76
[2023-03-17 12:13:01,931] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.68
[2023-03-17 12:13:01,931] [INFO] [logging.py:93:log_dist] [Rank 0] step=33, skipped=0, lr=[1.999265079925368e-05, 1.999265079925368e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:13:01,932] [INFO] [timer.py:198:stop] epoch=0/micro_step=33/global_step=33, RunningAvgSamplesPerSec=680.1078574430361, CurrSamplesPerSec=677.0603119514114, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:13:01,932] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.73 | backward_microstep: 214.47 | backward_inner_microstep: 180.11 | backward_allreduce_microstep: 33.60 | step_microstep: 4.54
[2023-03-17 12:13:01,933] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.64 | backward: 214.45 | backward_inner: 180.22 | backward_allreduce: 33.57 | step: 4.55
 iteration       33/      50 | consumed samples:         8448 | consumed tokens:      8650752 | elapsed time per iteration (ms): 358.0 | learning rate: 1.999E-05 | global batch size:    32 | lm loss: 1.038328E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.398 | TFLOPs: 4.88 |
time (ms) | forward-compute: 136.81 | backward-compute: 214.80 | backward-embedding-all-reduce: 0.01 | optimizer: 4.64 | batch-generator: 4.68
[2023-03-17 12:13:02,289] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.68
[2023-03-17 12:13:02,289] [INFO] [logging.py:93:log_dist] [Rank 0] step=34, skipped=0, lr=[1.853267653556708e-05, 1.853267653556708e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:13:02,289] [INFO] [timer.py:198:stop] epoch=0/micro_step=34/global_step=34, RunningAvgSamplesPerSec=679.919196805891, CurrSamplesPerSec=674.1221898543446, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:13:02,290] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.31 | backward_microstep: 214.55 | backward_inner_microstep: 180.02 | backward_allreduce_microstep: 33.76 | step_microstep: 4.32
[2023-03-17 12:13:02,290] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.23 | backward: 214.53 | backward_inner: 180.15 | backward_allreduce: 33.74 | step: 4.33
 iteration       34/      50 | consumed samples:         8704 | consumed tokens:      8912896 | elapsed time per iteration (ms): 358.1 | learning rate: 1.853E-05 | global batch size:    32 | lm loss: 1.034367E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.371 | TFLOPs: 4.88 |
time (ms) | forward-compute: 136.89 | backward-compute: 214.62 | backward-embedding-all-reduce: 0.01 | optimizer: 4.86 | batch-generator: 4.75
[2023-03-17 12:13:02,646] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.67
[2023-03-17 12:13:02,646] [INFO] [logging.py:93:log_dist] [Rank 0] step=35, skipped=0, lr=[1.712979818810323e-05, 1.712979818810323e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:13:02,647] [INFO] [timer.py:198:stop] epoch=0/micro_step=35/global_step=35, RunningAvgSamplesPerSec=679.9341714897657, CurrSamplesPerSec=680.4137098940986, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:13:02,647] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.32 | backward_microstep: 214.71 | backward_inner_microstep: 179.97 | backward_allreduce_microstep: 33.98 | step_microstep: 4.24
[2023-03-17 12:13:02,648] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.24 | backward: 214.68 | backward_inner: 180.09 | backward_allreduce: 33.96 | step: 4.24
 iteration       35/      50 | consumed samples:         8960 | consumed tokens:      9175040 | elapsed time per iteration (ms): 357.4 | learning rate: 1.713E-05 | global batch size:    32 | lm loss: 1.031691E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.526 | TFLOPs: 4.89 |
time (ms) | forward-compute: 136.28 | backward-compute: 214.67 | backward-embedding-all-reduce: 0.01 | optimizer: 4.82 | batch-generator: 4.69
[2023-03-17 12:13:03,004] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.68
[2023-03-17 12:13:03,004] [INFO] [logging.py:93:log_dist] [Rank 0] step=36, skipped=0, lr=[1.5789552276785377e-05, 1.5789552276785377e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:13:03,004] [INFO] [timer.py:198:stop] epoch=0/micro_step=36/global_step=36, RunningAvgSamplesPerSec=679.8569956220657, CurrSamplesPerSec=677.3199838514332, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:13:03,005] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.49 | backward_microstep: 214.42 | backward_inner_microstep: 180.00 | backward_allreduce_microstep: 33.63 | step_microstep: 4.21
[2023-03-17 12:13:03,005] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.41 | backward: 214.38 | backward_inner: 180.13 | backward_allreduce: 33.61 | step: 4.21
 iteration       36/      50 | consumed samples:         9216 | consumed tokens:      9437184 | elapsed time per iteration (ms): 357.5 | learning rate: 1.579E-05 | global batch size:    32 | lm loss: 1.029127E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.512 | TFLOPs: 4.89 |
time (ms) | forward-compute: 136.35 | backward-compute: 214.70 | backward-embedding-all-reduce: 0.01 | optimizer: 4.75 | batch-generator: 4.68
[2023-03-17 12:13:03,361] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.67
[2023-03-17 12:13:03,361] [INFO] [logging.py:93:log_dist] [Rank 0] step=37, skipped=0, lr=[1.4517228139925405e-05, 1.4517228139925405e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:13:03,362] [INFO] [timer.py:198:stop] epoch=0/micro_step=37/global_step=37, RunningAvgSamplesPerSec=679.7887709155402, CurrSamplesPerSec=677.4772504719505, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:13:03,362] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.56 | backward_microstep: 214.48 | backward_inner_microstep: 179.97 | backward_allreduce_microstep: 33.73 | step_microstep: 4.30
[2023-03-17 12:13:03,363] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.48 | backward: 214.46 | backward_inner: 180.10 | backward_allreduce: 33.70 | step: 4.30
 iteration       37/      50 | consumed samples:         9472 | consumed tokens:      9699328 | elapsed time per iteration (ms): 357.5 | learning rate: 1.452E-05 | global batch size:    32 | lm loss: 1.026351E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.504 | TFLOPs: 4.89 |
time (ms) | forward-compute: 136.27 | backward-compute: 214.72 | backward-embedding-all-reduce: 0.01 | optimizer: 4.84 | batch-generator: 4.72
[2023-03-17 12:13:03,719] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.67
[2023-03-17 12:13:03,720] [INFO] [logging.py:93:log_dist] [Rank 0] step=38, skipped=0, lr=[1.3317847059621894e-05, 1.3317847059621894e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:13:03,720] [INFO] [timer.py:198:stop] epoch=0/micro_step=38/global_step=38, RunningAvgSamplesPerSec=679.608762914511, CurrSamplesPerSec=673.3679906483446, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:13:03,720] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.65 | backward_microstep: 214.88 | backward_inner_microstep: 181.08 | backward_allreduce_microstep: 33.02 | step_microstep: 4.23
[2023-03-17 12:13:03,721] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.56 | backward: 214.85 | backward_inner: 181.21 | backward_allreduce: 33.00 | step: 4.23
 iteration       38/      50 | consumed samples:         9728 | consumed tokens:      9961472 | elapsed time per iteration (ms): 358.0 | learning rate: 1.332E-05 | global batch size:    32 | lm loss: 1.024347E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.389 | TFLOPs: 4.88 |
time (ms) | forward-compute: 136.54 | backward-compute: 214.98 | backward-embedding-all-reduce: 0.01 | optimizer: 4.77 | batch-generator: 4.74
[2023-03-17 12:13:04,076] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.67
[2023-03-17 12:13:04,076] [INFO] [logging.py:93:log_dist] [Rank 0] step=39, skipped=0, lr=[1.2196142445053694e-05, 1.2196142445053694e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:13:04,077] [INFO] [timer.py:198:stop] epoch=0/micro_step=39/global_step=39, RunningAvgSamplesPerSec=679.6248195750545, CurrSamplesPerSec=680.2033650922359, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:13:04,077] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.23 | backward_microstep: 214.14 | backward_inner_microstep: 180.34 | backward_allreduce_microstep: 33.04 | step_microstep: 4.17
[2023-03-17 12:13:04,078] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.15 | backward: 214.12 | backward_inner: 180.47 | backward_allreduce: 33.01 | step: 4.18
 iteration       39/      50 | consumed samples:         9984 | consumed tokens:     10223616 | elapsed time per iteration (ms): 356.9 | learning rate: 1.220E-05 | global batch size:    32 | lm loss: 1.021817E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.667 | TFLOPs: 4.90 |
time (ms) | forward-compute: 136.32 | backward-compute: 214.09 | backward-embedding-all-reduce: 0.01 | optimizer: 4.81 | batch-generator: 4.66
[2023-03-17 12:13:04,434] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.67
[2023-03-17 12:13:04,434] [INFO] [logging.py:93:log_dist] [Rank 0] step=40, skipped=0, lr=[1.1156541151876422e-05, 1.1156541151876422e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:13:04,435] [INFO] [timer.py:198:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=679.6945568582631, CurrSamplesPerSec=682.2849357964193, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:13:04,435] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.90 | backward_microstep: 214.93 | backward_inner_microstep: 180.32 | backward_allreduce_microstep: 33.82 | step_microstep: 4.20
[2023-03-17 12:13:04,436] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.81 | backward: 214.91 | backward_inner: 180.46 | backward_allreduce: 33.80 | step: 4.21
 iteration       40/      50 | consumed samples:        10240 | consumed tokens:     10485760 | elapsed time per iteration (ms): 358.1 | learning rate: 1.116E-05 | global batch size:    32 | lm loss: 1.019481E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.365 | TFLOPs: 4.88 |
time (ms) | forward-compute: 136.34 | backward-compute: 215.35 | backward-embedding-all-reduce: 0.01 | optimizer: 4.70 | batch-generator: 4.86
[2023-03-17 12:13:04,791] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.67
[2023-03-17 12:13:04,792] [INFO] [logging.py:93:log_dist] [Rank 0] step=41, skipped=0, lr=[1.0203146011445599e-05, 1.0203146011445599e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:13:04,792] [INFO] [timer.py:198:stop] epoch=0/micro_step=41/global_step=41, RunningAvgSamplesPerSec=679.6050496899934, CurrSamplesPerSec=676.2211585937264, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:13:04,793] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.38 | backward_microstep: 214.56 | backward_inner_microstep: 180.31 | backward_allreduce_microstep: 33.50 | step_microstep: 3.98
[2023-03-17 12:13:04,793] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.30 | backward: 214.54 | backward_inner: 180.42 | backward_allreduce: 33.49 | step: 3.99
 iteration       41/      50 | consumed samples:        10496 | consumed tokens:     10747904 | elapsed time per iteration (ms): 357.2 | learning rate: 1.020E-05 | global batch size:    32 | lm loss: 1.017328E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.586 | TFLOPs: 4.89 |
time (ms) | forward-compute: 136.27 | backward-compute: 214.33 | backward-embedding-all-reduce: 0.01 | optimizer: 4.91 | batch-generator: 4.77
[2023-03-17 12:13:05,149] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.67
[2023-03-17 12:13:05,149] [INFO] [logging.py:93:log_dist] [Rank 0] step=42, skipped=0, lr=[9.33971963881569e-06, 9.33971963881569e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:13:05,150] [INFO] [timer.py:198:stop] epoch=0/micro_step=42/global_step=42, RunningAvgSamplesPerSec=679.6033908843905, CurrSamplesPerSec=679.5387037815233, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:13:05,150] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.67 | backward_microstep: 214.95 | backward_inner_microstep: 180.08 | backward_allreduce_microstep: 34.11 | step_microstep: 4.14
[2023-03-17 12:13:05,151] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.60 | backward: 214.92 | backward_inner: 180.20 | backward_allreduce: 34.10 | step: 4.14
 iteration       42/      50 | consumed samples:        10752 | consumed tokens:     11010048 | elapsed time per iteration (ms): 357.5 | learning rate: 9.340E-06 | global batch size:    32 | lm loss: 1.014726E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.504 | TFLOPs: 4.89 |
time (ms) | forward-compute: 136.26 | backward-compute: 214.83 | backward-embedding-all-reduce: 0.01 | optimizer: 4.76 | batch-generator: 4.71
[2023-03-17 12:13:05,506] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.67
[2023-03-17 12:13:05,506] [INFO] [logging.py:93:log_dist] [Rank 0] step=43, skipped=0, lr=[8.569669583417477e-06, 8.569669583417477e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:13:05,507] [INFO] [timer.py:198:stop] epoch=0/micro_step=43/global_step=43, RunningAvgSamplesPerSec=679.6117168445294, CurrSamplesPerSec=679.9449226170875, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:13:05,507] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.35 | backward_microstep: 214.62 | backward_inner_microstep: 180.69 | backward_allreduce_microstep: 33.16 | step_microstep: 4.09
[2023-03-17 12:13:05,508] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.27 | backward: 214.59 | backward_inner: 180.80 | backward_allreduce: 33.14 | step: 4.10
 iteration       43/      50 | consumed samples:        11008 | consumed tokens:     11272192 | elapsed time per iteration (ms): 357.1 | learning rate: 8.570E-06 | global batch size:    32 | lm loss: 1.013476E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.617 | TFLOPs: 4.89 |
time (ms) | forward-compute: 136.12 | backward-compute: 214.48 | backward-embedding-all-reduce: 0.01 | optimizer: 4.78 | batch-generator: 4.66
[2023-03-17 12:13:05,864] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.66
[2023-03-17 12:13:05,864] [INFO] [logging.py:93:log_dist] [Rank 0] step=44, skipped=0, lr=[7.896034881017213e-06, 7.896034881017213e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:13:05,865] [INFO] [timer.py:198:stop] epoch=0/micro_step=44/global_step=44, RunningAvgSamplesPerSec=679.4598277420057, CurrSamplesPerSec=673.2902992786411, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:13:05,865] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.46 | backward_microstep: 214.70 | backward_inner_microstep: 179.94 | backward_allreduce_microstep: 34.02 | step_microstep: 4.24
[2023-03-17 12:13:05,866] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.37 | backward: 214.68 | backward_inner: 180.05 | backward_allreduce: 34.00 | step: 4.25
 iteration       44/      50 | consumed samples:        11264 | consumed tokens:     11534336 | elapsed time per iteration (ms): 358.2 | learning rate: 7.896E-06 | global batch size:    32 | lm loss: 1.011820E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.336 | TFLOPs: 4.88 |
time (ms) | forward-compute: 136.42 | backward-compute: 215.37 | backward-embedding-all-reduce: 0.01 | optimizer: 4.71 | batch-generator: 4.57
[2023-03-17 12:13:06,222] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.68
[2023-03-17 12:13:06,222] [INFO] [logging.py:93:log_dist] [Rank 0] step=45, skipped=0, lr=[7.3214740600308545e-06, 7.3214740600308545e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:13:06,222] [INFO] [timer.py:198:stop] epoch=0/micro_step=45/global_step=45, RunningAvgSamplesPerSec=679.4391845923085, CurrSamplesPerSec=678.5733035380244, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:13:06,223] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.41 | backward_microstep: 214.63 | backward_inner_microstep: 180.09 | backward_allreduce_microstep: 33.81 | step_microstep: 4.18
[2023-03-17 12:13:06,223] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.32 | backward: 214.61 | backward_inner: 180.20 | backward_allreduce: 33.80 | step: 4.18
 iteration       45/      50 | consumed samples:        11520 | consumed tokens:     11796480 | elapsed time per iteration (ms): 357.4 | learning rate: 7.321E-06 | global batch size:    32 | lm loss: 1.009276E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.524 | TFLOPs: 4.89 |
time (ms) | forward-compute: 136.25 | backward-compute: 214.92 | backward-embedding-all-reduce: 0.01 | optimizer: 4.59 | batch-generator: 4.62
[2023-03-17 12:13:06,579] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.68
[2023-03-17 12:13:06,579] [INFO] [logging.py:93:log_dist] [Rank 0] step=46, skipped=0, lr=[6.848254649526961e-06, 6.848254649526961e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:13:06,579] [INFO] [timer.py:198:stop] epoch=0/micro_step=46/global_step=46, RunningAvgSamplesPerSec=679.474435516328, CurrSamplesPerSec=680.9936932685916, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:13:06,580] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.29 | backward_microstep: 214.49 | backward_inner_microstep: 180.55 | backward_allreduce_microstep: 33.20 | step_microstep: 4.09
[2023-03-17 12:13:06,580] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.21 | backward: 214.47 | backward_inner: 180.66 | backward_allreduce: 33.20 | step: 4.10
 iteration       46/      50 | consumed samples:        11776 | consumed tokens:     12058624 | elapsed time per iteration (ms): 357.2 | learning rate: 6.848E-06 | global batch size:    32 | lm loss: 1.008490E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.587 | TFLOPs: 4.89 |
time (ms) | forward-compute: 136.16 | backward-compute: 214.67 | backward-embedding-all-reduce: 0.01 | optimizer: 4.68 | batch-generator: 4.52
[2023-03-17 12:13:06,936] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.68
[2023-03-17 12:13:06,936] [INFO] [logging.py:93:log_dist] [Rank 0] step=47, skipped=0, lr=[6.478244230325408e-06, 6.478244230325408e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:13:06,936] [INFO] [timer.py:198:stop] epoch=0/micro_step=47/global_step=47, RunningAvgSamplesPerSec=679.5188260581695, CurrSamplesPerSec=681.4777685820331, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:13:06,937] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.47 | backward_microstep: 214.32 | backward_inner_microstep: 180.15 | backward_allreduce_microstep: 33.38 | step_microstep: 4.12
[2023-03-17 12:13:06,937] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.39 | backward: 214.29 | backward_inner: 180.29 | backward_allreduce: 33.36 | step: 4.12
 iteration       47/      50 | consumed samples:        12032 | consumed tokens:     12320768 | elapsed time per iteration (ms): 356.7 | learning rate: 6.478E-06 | global batch size:    32 | lm loss: 1.006718E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.717 | TFLOPs: 4.90 |
time (ms) | forward-compute: 135.63 | backward-compute: 214.74 | backward-embedding-all-reduce: 0.01 | optimizer: 4.62 | batch-generator: 4.59
[2023-03-17 12:13:07,293] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.67
[2023-03-17 12:13:07,293] [INFO] [logging.py:93:log_dist] [Rank 0] step=48, skipped=0, lr=[6.2129030645091e-06, 6.2129030645091e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:13:07,293] [INFO] [timer.py:198:stop] epoch=0/micro_step=48/global_step=48, RunningAvgSamplesPerSec=679.5802164062404, CurrSamplesPerSec=682.3543096523605, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:13:07,294] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.69 | backward_microstep: 214.63 | backward_inner_microstep: 180.08 | backward_allreduce_microstep: 33.81 | step_microstep: 4.03
[2023-03-17 12:13:07,294] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.61 | backward: 214.61 | backward_inner: 180.19 | backward_allreduce: 33.80 | step: 4.04
 iteration       48/      50 | consumed samples:        12288 | consumed tokens:     12582912 | elapsed time per iteration (ms): 357.1 | learning rate: 6.213E-06 | global batch size:    32 | lm loss: 1.005616E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.619 | TFLOPs: 4.89 |
time (ms) | forward-compute: 136.10 | backward-compute: 214.60 | backward-embedding-all-reduce: 0.01 | optimizer: 4.62 | batch-generator: 4.48
[2023-03-17 12:13:07,650] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.68
[2023-03-17 12:13:07,650] [INFO] [logging.py:93:log_dist] [Rank 0] step=49, skipped=0, lr=[6.053278332436668e-06, 6.053278332436668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:13:07,650] [INFO] [timer.py:198:stop] epoch=0/micro_step=49/global_step=49, RunningAvgSamplesPerSec=679.6603134208556, CurrSamplesPerSec=683.3652975708605, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:13:07,651] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.44 | backward_microstep: 214.69 | backward_inner_microstep: 180.16 | backward_allreduce_microstep: 33.77 | step_microstep: 4.16
[2023-03-17 12:13:07,651] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.36 | backward: 214.66 | backward_inner: 180.27 | backward_allreduce: 33.74 | step: 4.17
 iteration       49/      50 | consumed samples:        12544 | consumed tokens:     12845056 | elapsed time per iteration (ms): 356.9 | learning rate: 6.053E-06 | global batch size:    32 | lm loss: 1.004196E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.651 | TFLOPs: 4.89 |
time (ms) | forward-compute: 136.39 | backward-compute: 214.17 | backward-embedding-all-reduce: 0.01 | optimizer: 4.69 | batch-generator: 4.58
[2023-03-17 12:13:08,007] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.68
[2023-03-17 12:13:08,008] [INFO] [logging.py:93:log_dist] [Rank 0] step=50, skipped=0, lr=[6e-06, 6e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:13:08,008] [INFO] [timer.py:198:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=679.6163383860041, CurrSamplesPerSec=677.555911172138, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:13:08,009] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 127.25 | backward_microstep: 215.03 | backward_inner_microstep: 180.77 | backward_allreduce_microstep: 33.47 | step_microstep: 4.17
[2023-03-17 12:13:08,009] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 127.16 | backward: 215.01 | backward_inner: 180.90 | backward_allreduce: 33.46 | step: 4.18
 iteration       50/      50 | consumed samples:        12800 | consumed tokens:     13107200 | elapsed time per iteration (ms): 357.8 | learning rate: 6.000E-06 | global batch size:    32 | lm loss: 1.002793E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 89.432 | TFLOPs: 4.88 |
time (ms) | forward-compute: 136.64 | backward-compute: 214.74 | backward-embedding-all-reduce: 0.01 | optimizer: 4.74 | batch-generator: 4.68
[after training is done] datetime: 2023-03-17 12:13:08 
------------------------------------------------------------------------------------------------------------------
 validation loss at the end of training for val data | lm loss value: 9.955453E+00 | lm loss PPL: 2.106679E+04 | 
------------------------------------------------------------------------------------------------------------------
gpu108-23-l:42418:42886 [1] NCCL INFO [Service thread] Connection closed by localRank 1
gpu108-23-r:64498:64907 [2] NCCL INFO [Service thread] Connection closed by localRank 2
gpu108-23-l:42420:42885 [3] NCCL INFO [Service thread] Connection closed by localRank 3
gpu108-23-r:64496:64906 [0] NCCL INFO [Service thread] Connection closed by localRank 0
gpu108-23-r:64499:64904 [3] NCCL INFO [Service thread] Connection closed by localRank 3
gpu108-23-l:42417:42888 [0] NCCL INFO [Service thread] Connection closed by localRank 0
gpu108-23-r:64497:64905 [1] NCCL INFO [Service thread] Connection closed by localRank 1
gpu108-23-l:42419:42887 [2] NCCL INFO [Service thread] Connection closed by localRank 2
gpu108-23-l:42418:42418 [1] NCCL INFO comm 0x3bcfac90 rank 1 nranks 8 cudaDev 1 busId 46000 - Abort COMPLETE
gpu108-23-r:64498:64498 [2] NCCL INFO comm 0x3b226090 rank 6 nranks 8 cudaDev 2 busId 85000 - Abort COMPLETE
gpu108-23-r:64497:64497 [1] NCCL INFO comm 0x3ad9f8b0 rank 5 nranks 8 cudaDev 1 busId 46000 - Abort COMPLETE
gpu108-23-l:42419:42419 [2] NCCL INFO comm 0x38dd5980 rank 2 nranks 8 cudaDev 2 busId 85000 - Abort COMPLETE
gpu108-23-l:42417:42417 [0] NCCL INFO comm 0x39d76030 rank 0 nranks 8 cudaDev 0 busId 7000 - Abort COMPLETE
gpu108-23-r:64499:64499 [3] NCCL INFO comm 0x3af4da90 rank 7 nranks 8 cudaDev 3 busId c7000 - Abort COMPLETE
gpu108-23-l:42420:42420 [3] NCCL INFO comm 0x396d4c70 rank 3 nranks 8 cudaDev 3 busId c7000 - Abort COMPLETE
gpu108-23-r:64496:64496 [0] NCCL INFO comm 0x3b19a830 rank 4 nranks 8 cudaDev 0 busId 7000 - Abort COMPLETE
