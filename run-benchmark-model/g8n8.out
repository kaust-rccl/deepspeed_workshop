------- JOB Configuration ---------
scontrol show job 24296273
JobId=24296273 JobName=g4
   UserId=shaima0d(174988) GroupId=g-shaima0d(1174988) MCS_label=N/A
   Priority=9704 Nice=0 Account=a100_training_acc QOS=a100_training_qos
   JobState=RUNNING Reason=None Dependency=(null)
   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=00:00:00 TimeLimit=00:15:00 TimeMin=N/A
   SubmitTime=2023-03-17T12:15:15 EligibleTime=2023-03-17T12:15:15
   AccrueTime=2023-03-17T12:15:15
   StartTime=2023-03-17T12:15:16 EndTime=2023-03-17T12:30:16 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2023-03-17T12:15:16 Scheduler=Main
   Partition=a100_training AllocNode:Sid=login510-27:152451
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=gpu108-23-r,gpu109-02-l,gpu109-02-r,gpu109-16-l,gpu109-16-r,gpu109-23-r,gpu201-02-l,gpu201-02-r
   BatchHost=gpu108-23-r
   NumNodes=8 NumCPUs=120 NumTasks=8 CPUs/Task=15 ReqB:S:C:T=0:0:*:*
   TRES=cpu=120,mem=240G,node=8,billing=120,gres/gpu=8
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
   MinCPUsNode=15 MinMemoryCPU=2G MinTmpDiskNode=0
   Features=(a100)&el7 DelayBoot=00:00:00
   Reservation=A100
   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
   Command=/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/G8N8.slurm
   WorkDir=/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed
   StdErr=/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/slurm-24296273.out
   StdIn=/dev/null
   StdOut=/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/slurm-24296273.out
   Power=
   TresPerJob=gres:gpu:8
   TresPerNode=gres:gpu:1
   

------- GPU Configuration ---------
nvidia-smi -L
GPU 0: NVIDIA A100-SXM4-80GB (UUID: GPU-b213bccc-3ee2-861c-2151-d700ecb7a781)
------- NVLink Configuration ------
nvidia-smi topo -m
	[4mGPU0	mlx5_0	mlx5_1	CPU Affinity	NUMA Affinity[0m
GPU0	 X 	SYS	SYS	0-14	0-1
mlx5_0	SYS	 X 	SYS		
mlx5_1	SYS	SYS	 X 		

Legend:

  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing at most a single PCIe bridge
  NV#  = Connection traversing a bonded set of # NVLinks
------- Infiniband Configuration --
ibv_devinfo
hca_id:	mlx5_0
	transport:			InfiniBand (0)
	fw_ver:				20.34.1002
	node_guid:			88e9:a4ff:ff1a:6ec8
	sys_image_guid:			88e9:a4ff:ff1a:6ec8
	vendor_id:			0x02c9
	vendor_part_id:			4123
	hw_ver:				0x0
	board_id:			MT_0000000451
	phys_port_cnt:			1
		port:	1
			state:			PORT_ACTIVE (4)
			max_mtu:		4096 (5)
			active_mtu:		4096 (5)
			sm_lid:			1
			port_lid:		664
			port_lmc:		0x00
			link_layer:		InfiniBand

hca_id:	mlx5_1
	transport:			InfiniBand (0)
	fw_ver:				20.34.1002
	node_guid:			88e9:a4ff:ff1a:6ec4
	sys_image_guid:			88e9:a4ff:ff1a:6ec4
	vendor_id:			0x02c9
	vendor_part_id:			4123
	hw_ver:				0x0
	board_id:			MT_0000000451
	phys_port_cnt:			1
		port:	1
			state:			PORT_ACTIVE (4)
			max_mtu:		4096 (5)
			active_mtu:		4096 (5)
			sm_lid:			1
			port_lid:		692
			port_lmc:		0x00
			link_layer:		InfiniBand

Loading module for CUDA 11.7.0
CUDA 11.7.0 is now loaded
GNU 11.1.0 is now loaded
Loading module for nccl-2.17.1.1
nccl-2.17.1.1 modules now loaded
Loading module for gdrcopy 2.0_cuda11.7.0
gdrcopy 2.0_cuda11.7.0 modules now loaded
Loading module for ucx-gpu 1.14.0
ucx-gpu 1.14.0 modules now loaded
Loading module for OPENMPI 4.1.4
OPENMPI 4.1.4 modules now loaded
Loading module for pytorch-1.13.1_cuda11.7.0
pytorch-1.13.1_cuda11.7.0 modules now loaded
Loading module for deepspeed-0.8.3
deepspeed-0.8.3 modules now loaded
Loading module for apex-22.03
apex-22.03 modules now loaded
Currently Loaded Modulefiles:
  1) dl/2023             5) nccl/2.17.1.1       9) pytorch/1.13.1
  2) python/3.9.16       6) gdrcopy/2.0        10) deepspeed/0.8.3
  3) cuda/11.7.0         7) ucx/1.14.0         11) apex/22.03
  4) gcc/11.1.0          8) openmpi-gpu/4.1.4
[2023-03-17 12:15:19,905] [INFO] [runner.py:550:main] cmd = srun -n 8 --nodes 8 --gpus 8 /sw/csgv/dl/apps/python/3.9.16/bin/python3.9 -u pretrain_gpt.py --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 1 --hidden-size 1024 --num-attention-heads 32 --seq-length 1024 --loss-scale 15 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 32 --train-iters 50 --lr 6.0e-5 --min-lr 6.0e-6 --lr-decay-style cosine --log-interval 1 --eval-iters 40 --eval-interval 1000 --data-path /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document --vocab-file /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json --merge-file /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt --save-interval 1000 --split 98,2,0 --clip-grad 1.0 --weight-decay 0.1 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.006 --fp16 --checkpoint-activations --tensorboard-dir ds_z_off-none_stage_3_nl1_hs1024_mb4_seq1024_gb32_nodes8 --deepspeed-activation-checkpointing --zero-stage=3 --deepspeed_config=ds_config.json --no-pipeline-parallel --deepspeed --no-masked-softmax-fusion --no-bias-dropout-fusion --no-bias-gelu-fusion --exit-interval 5000
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. False
  bias_gelu_fusion ................................ False
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  data_parallel_size .............................. 1
  data_path ....................................... ['/ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 40
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 5000
  expert_interval ................................. 2
  ffn_hidden_size ................................. 4096
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 1024
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 32
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 15.0
  loss_scale_window ............................... 1000
  lr .............................................. 6e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ None
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... False
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 32
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 1
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. ds_z_off-none_stage_3_nl1_hs1024_mb4_seq1024_gb32_nodes8
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 50
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 1
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 3
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> setting tensorboard ...
[2023-03-17 12:15:22,801] [INFO] [comm.py:634:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. False
  bias_gelu_fusion ................................ False
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  data_parallel_size .............................. 1
  data_path ....................................... ['/ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 40
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 5000
  expert_interval ................................. 2
  ffn_hidden_size ................................. 4096
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 1024
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 32
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 15.0
  loss_scale_window ............................... 1000
  lr .............................................. 6e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ None
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... False
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 32
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 1
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. ds_z_off-none_stage_3_nl1_hs1024_mb4_seq1024_gb32_nodes8
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 50
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 1
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 3
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. False
  bias_gelu_fusion ................................ False
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  data_parallel_size .............................. 1
  data_path ....................................... ['/ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 40
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 5000
  expert_interval ................................. 2
  ffn_hidden_size ................................. 4096
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 1024
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 32
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 15.0
  loss_scale_window ............................... 1000
  lr .............................................. 6e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ None
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... False
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 32
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 1
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. ds_z_off-none_stage_3_nl1_hs1024_mb4_seq1024_gb32_nodes8
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 50
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 1
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 3
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. False
  bias_gelu_fusion ................................ False
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  data_parallel_size .............................. 1
  data_path ....................................... ['/ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 40
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 5000
  expert_interval ................................. 2
  ffn_hidden_size ................................. 4096
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 1024
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 32
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 15.0
  loss_scale_window ............................... 1000
  lr .............................................. 6e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ None
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... False
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 32
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 1
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. ds_z_off-none_stage_3_nl1_hs1024_mb4_seq1024_gb32_nodes8
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 50
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 1
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 3
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. False
  bias_gelu_fusion ................................ False
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  data_parallel_size .............................. 1
  data_path ....................................... ['/ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 40
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 5000
  expert_interval ................................. 2
  ffn_hidden_size ................................. 4096
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 1024
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 32
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 15.0
  loss_scale_window ............................... 1000
  lr .............................................. 6e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ None
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... False
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 32
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 1
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. ds_z_off-none_stage_3_nl1_hs1024_mb4_seq1024_gb32_nodes8
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 50
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 1
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 3
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. False
  bias_gelu_fusion ................................ False
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  data_parallel_size .............................. 1
  data_path ....................................... ['/ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 40
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 5000
  expert_interval ................................. 2
  ffn_hidden_size ................................. 4096
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 1024
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 32
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 15.0
  loss_scale_window ............................... 1000
  lr .............................................. 6e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ None
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... False
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 32
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 1
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. ds_z_off-none_stage_3_nl1_hs1024_mb4_seq1024_gb32_nodes8
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 50
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 1
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 3
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
> setting tensorboard ...
[2023-03-17 12:15:23,689] [INFO] [comm.py:634:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. False
  bias_gelu_fusion ................................ False
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  data_parallel_size .............................. 1
  data_path ....................................... ['/ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 40
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 5000
  expert_interval ................................. 2
  ffn_hidden_size ................................. 4096
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 1024
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 32
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 15.0
  loss_scale_window ............................... 1000
  lr .............................................. 6e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ None
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... False
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 32
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 1
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. ds_z_off-none_stage_3_nl1_hs1024_mb4_seq1024_gb32_nodes8
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 50
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 1
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 3
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch']
torch version .................... 1.13.0a0+git49444c3
deepspeed install path ........... ['/sw/csgv/dl/apps/deepspeed/0.8.3/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.8.3+bbfd0a6, bbfd0a6, master
torch cuda version ............... 11.7
torch hip version ................ None
nvcc version ..................... 11.7
deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
> setting tensorboard ...
> setting tensorboard ...
[2023-03-17 12:15:23,749] [INFO] [comm.py:634:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-03-17 12:15:23,750] [INFO] [comm.py:634:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
> setting tensorboard ...
[2023-03-17 12:15:23,757] [INFO] [comm.py:634:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
**** Git info for Megatron: git_hash=57e6439 git_branch=main ****
using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. False
  bias_gelu_fusion ................................ False
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning_legacy ...................... False
  custom_token_counting ........................... False
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  data_parallel_size .............................. 1
  data_path ....................................... ['/ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  enable_expert_tensor_parallelism ................ False
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 40
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... 5000
  expert_interval ................................. 2
  ffn_hidden_size ................................. 4096
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 1024
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 32
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... 15.0
  loss_scale_window ............................... 1000
  lr .............................................. 6e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ None
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... False
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 32
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 1
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  return_data_index ............................... False
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 98,2,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. ds_z_off-none_stage_3_nl1_hs1024_mb4_seq1024_gb32_nodes8
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 50
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 1
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 3
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> setting tensorboard ...
[2023-03-17 12:15:23,900] [INFO] [comm.py:634:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
> setting tensorboard ...
[2023-03-17 12:15:23,946] [INFO] [comm.py:634:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
> setting tensorboard ...
[2023-03-17 12:15:24,029] [INFO] [comm.py:634:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-03-17 12:15:24,584] [INFO] [comm.py:688:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=0, world_size=8, master_addr=10.109.8.134, master_port=29500
[2023-03-17 12:15:24,584] [INFO] [comm.py:688:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=0, world_size=8, master_addr=10.109.8.134, master_port=29500
[2023-03-17 12:15:24,583] [INFO] [comm.py:688:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=0, world_size=8, master_addr=10.109.8.134, master_port=29500
[2023-03-17 12:15:24,584] [INFO] [comm.py:688:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=0, world_size=8, master_addr=10.109.8.134, master_port=29500
[2023-03-17 12:15:24,584] [INFO] [comm.py:688:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=0, world_size=8, master_addr=10.109.8.134, master_port=29500
[2023-03-17 12:15:24,584] [INFO] [comm.py:688:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=0, world_size=8, master_addr=10.109.8.134, master_port=29500
[2023-03-17 12:15:24,584] [INFO] [comm.py:688:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=8, master_addr=10.109.8.134, master_port=29500
[2023-03-17 12:15:24,584] [INFO] [comm.py:652:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-03-17 12:15:24,585] [INFO] [comm.py:688:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=0, world_size=8, master_addr=10.109.8.134, master_port=29500
Hi, I am 0 and pinning GPU 0
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
Hi, I am 4 and pinning GPU 0
Hi, I am 5 and pinning GPU 0
Hi, I am 3 and pinning GPU 0
Hi, I am 1 and pinning GPU 0
Hi, I am 6 and pinning GPU 0
Hi, I am 2 and pinning GPU 0
Hi, I am 7 and pinning GPU 0
Helloworld from 6 0
> setting random seeds to 1234 ...
[2023-03-17 12:15:25,878] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> compiling dataset index builder ...
Helloworld from 3 0
Helloworld from 7 0
Helloworld from 1 0
Helloworld from 4 0
Helloworld from 2 0
Helloworld from 5 0
make: Entering directory `/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/megatron/data'
make: Nothing to be done for `default'.
make: Leaving directory `/ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/megatron/data'
>>> done with dataset index builder. Compilation time: 0.030 seconds
WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /ibex/user/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (15) as the number of workers...
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
gpu108-23-r:1142:1142 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-23-r:1142:1142 [0] NCCL INFO Bootstrap : Using ib0:10.109.136.134<0>
gpu108-23-r:1142:1142 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu108-23-r:1142:1142 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu108-23-r:1142:1142 [0] NCCL INFO cudaDriverVersion 11080
NCCL version 2.17.1+cuda11.7
gpu109-16-l:53482:53482 [0] NCCL INFO cudaDriverVersion 11080
gpu109-02-r:49201:49201 [0] NCCL INFO cudaDriverVersion 11080
gpu201-02-r:15165:15165 [0] NCCL INFO cudaDriverVersion 11080
gpu109-23-r:58710:58710 [0] NCCL INFO cudaDriverVersion 11080
gpu109-02-l:65178:65178 [0] NCCL INFO cudaDriverVersion 11080
gpu201-02-l:12282:12282 [0] NCCL INFO cudaDriverVersion 11080
gpu109-16-r:24876:24876 [0] NCCL INFO cudaDriverVersion 11080
gpu109-02-r:49201:49201 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu109-23-r:58710:58710 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu201-02-l:12282:12282 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu109-02-l:65178:65178 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu109-16-l:53482:53482 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu201-02-r:15165:15165 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu109-16-r:24876:24876 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu109-02-r:49201:49201 [0] NCCL INFO Bootstrap : Using ib0:10.109.137.8<0>
gpu109-23-r:58710:58710 [0] NCCL INFO Bootstrap : Using ib0:10.109.137.134<0>
gpu201-02-l:12282:12282 [0] NCCL INFO Bootstrap : Using ib0:10.109.143.7<0>
gpu109-02-r:49201:49201 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu109-02-r:49201:49201 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu109-23-r:58710:58710 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu109-23-r:58710:58710 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu109-16-l:53482:53482 [0] NCCL INFO Bootstrap : Using ib0:10.109.137.91<0>
gpu201-02-l:12282:12282 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu201-02-l:12282:12282 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu109-02-l:65178:65178 [0] NCCL INFO Bootstrap : Using ib0:10.109.137.7<0>
gpu201-02-r:15165:15165 [0] NCCL INFO Bootstrap : Using ib0:10.109.143.8<0>
gpu109-16-l:53482:53482 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu109-16-l:53482:53482 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu109-02-l:65178:65178 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu109-02-l:65178:65178 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu201-02-r:15165:15165 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu201-02-r:15165:15165 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu109-16-r:24876:24876 [0] NCCL INFO Bootstrap : Using ib0:10.109.137.92<0>
gpu109-16-r:24876:24876 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
gpu109-16-r:24876:24876 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
gpu109-02-r:49201:49461 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu109-02-l:65178:65256 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-23-r:1142:1287 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu109-02-r:49201:49461 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.137.8<0>
gpu109-02-r:49201:49461 [0] NCCL INFO Using network IB
gpu109-02-l:65178:65256 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.137.7<0>
gpu109-02-l:65178:65256 [0] NCCL INFO Using network IB
gpu201-02-r:15165:15275 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu109-16-r:24876:24955 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu109-23-r:58710:58787 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu108-23-r:1142:1287 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.136.134<0>
gpu108-23-r:1142:1287 [0] NCCL INFO Using network IB
gpu109-16-l:53482:53572 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu201-02-r:15165:15275 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.143.8<0>
gpu201-02-r:15165:15275 [0] NCCL INFO Using network IB
gpu109-16-r:24876:24955 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.137.92<0>
gpu109-16-r:24876:24955 [0] NCCL INFO Using network IB
gpu109-23-r:58710:58787 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.137.134<0>
gpu109-23-r:58710:58787 [0] NCCL INFO Using network IB
gpu109-16-l:53482:53572 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.137.91<0>
gpu109-16-l:53482:53572 [0] NCCL INFO Using network IB
gpu201-02-l:12282:12441 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0
gpu201-02-l:12282:12441 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.109.143.7<0>
gpu201-02-l:12282:12441 [0] NCCL INFO Using network IB
gpu109-02-l:65178:65256 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu109-16-r:24876:24955 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu109-16-l:53482:53572 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu109-02-r:49201:49461 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu109-02-r:49201:49461 [0] NCCL INFO Setting affinity for GPU 0 to 3f00ff80
gpu201-02-l:12282:12441 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu108-23-r:1142:1287 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu201-02-r:15165:15275 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu108-23-r:1142:1287 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu109-23-r:58710:58787 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to SYS
gpu109-23-r:58710:58787 [0] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] 6/4/-1->5->3
gpu109-23-r:58710:58787 [0] NCCL INFO P2P Chunksize set to 131072
gpu201-02-l:12282:12441 [0] NCCL INFO Trees [0] 5/7/-1->6->4 [1] -1/-1/-1->6->5
gpu201-02-l:12282:12441 [0] NCCL INFO P2P Chunksize set to 131072
gpu201-02-r:15165:15275 [0] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 3/-1/-1->7->-1
gpu201-02-r:15165:15275 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:1142:1287 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7
gpu108-23-r:1142:1287 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7
gpu108-23-r:1142:1287 [0] NCCL INFO Trees [0] 4/-1/-1->0->-1 [1] -1/-1/-1->0->1
gpu108-23-r:1142:1287 [0] NCCL INFO P2P Chunksize set to 131072
gpu109-02-l:65178:65256 [0] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] 2/0/-1->1->3
gpu109-02-l:65178:65256 [0] NCCL INFO P2P Chunksize set to 131072
gpu109-02-r:49201:49461 [0] NCCL INFO Trees [0] 1/3/-1->2->4 [1] -1/-1/-1->2->1
gpu109-02-r:49201:49461 [0] NCCL INFO P2P Chunksize set to 131072
gpu109-16-l:53482:53572 [0] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 5/1/-1->3->7
gpu109-16-l:53482:53572 [0] NCCL INFO P2P Chunksize set to 131072
gpu109-16-r:24876:24955 [0] NCCL INFO Trees [0] 2/6/-1->4->0 [1] -1/-1/-1->4->5
gpu109-16-r:24876:24955 [0] NCCL INFO P2P Chunksize set to 131072
gpu109-02-r:49201:49461 [0] NCCL INFO Channel 00/0 : 1[7000] -> 2[c7000] [receive] via NET/IB/1/GDRDMA
gpu109-16-r:24876:24955 [0] NCCL INFO Channel 00/0 : 3[7000] -> 4[46000] [receive] via NET/IB/0/GDRDMA
gpu109-16-l:53482:53572 [0] NCCL INFO Channel 00/0 : 2[c7000] -> 3[7000] [receive] via NET/IB/0/GDRDMA
gpu109-02-l:65178:65256 [0] NCCL INFO Channel 00/0 : 0[7000] -> 1[7000] [receive] via NET/IB/0/GDRDMA
gpu201-02-r:15165:15275 [0] NCCL INFO Channel 00/0 : 6[7000] -> 7[85000] [receive] via NET/IB/1/GDRDMA
gpu108-23-r:1142:1287 [0] NCCL INFO Channel 00/0 : 7[85000] -> 0[7000] [receive] via NET/IB/0/GDRDMA
gpu109-23-r:58710:58787 [0] NCCL INFO Channel 00/0 : 4[46000] -> 5[7000] [receive] via NET/IB/0/GDRDMA
gpu201-02-l:12282:12441 [0] NCCL INFO Channel 00/0 : 5[7000] -> 6[7000] [receive] via NET/IB/0/GDRDMA
gpu109-02-r:49201:49461 [0] NCCL INFO Channel 01/0 : 1[7000] -> 2[c7000] [receive] via NET/IB/1/GDRDMA
gpu109-16-r:24876:24955 [0] NCCL INFO Channel 01/0 : 3[7000] -> 4[46000] [receive] via NET/IB/0/GDRDMA
gpu109-16-l:53482:53572 [0] NCCL INFO Channel 01/0 : 2[c7000] -> 3[7000] [receive] via NET/IB/0/GDRDMA
gpu108-23-r:1142:1287 [0] NCCL INFO Channel 01/0 : 7[85000] -> 0[7000] [receive] via NET/IB/0/GDRDMA
gpu201-02-r:15165:15275 [0] NCCL INFO Channel 01/0 : 6[7000] -> 7[85000] [receive] via NET/IB/1/GDRDMA
gpu109-23-r:58710:58787 [0] NCCL INFO Channel 01/0 : 4[46000] -> 5[7000] [receive] via NET/IB/0/GDRDMA
gpu201-02-l:12282:12441 [0] NCCL INFO Channel 01/0 : 5[7000] -> 6[7000] [receive] via NET/IB/0/GDRDMA
gpu109-02-l:65178:65256 [0] NCCL INFO Channel 01/0 : 0[7000] -> 1[7000] [receive] via NET/IB/0/GDRDMA
gpu109-02-r:49201:49461 [0] NCCL INFO Channel 00/0 : 2[c7000] -> 3[7000] [send] via NET/IB/1/GDRDMA
gpu109-16-r:24876:24955 [0] NCCL INFO Channel 00/0 : 4[46000] -> 5[7000] [send] via NET/IB/0/GDRDMA
gpu109-16-l:53482:53572 [0] NCCL INFO Channel 00/0 : 3[7000] -> 4[46000] [send] via NET/IB/0/GDRDMA
gpu108-23-r:1142:1287 [0] NCCL INFO Channel 00/0 : 0[7000] -> 1[7000] [send] via NET/IB/0/GDRDMA
gpu201-02-r:15165:15275 [0] NCCL INFO Channel 00/0 : 7[85000] -> 0[7000] [send] via NET/IB/1/GDRDMA
gpu109-23-r:58710:58787 [0] NCCL INFO Channel 00/0 : 5[7000] -> 6[7000] [send] via NET/IB/0/GDRDMA
gpu201-02-l:12282:12441 [0] NCCL INFO Channel 00/0 : 6[7000] -> 7[85000] [send] via NET/IB/0/GDRDMA
gpu109-02-l:65178:65256 [0] NCCL INFO Channel 00/0 : 1[7000] -> 2[c7000] [send] via NET/IB/0/GDRDMA
gpu109-02-r:49201:49461 [0] NCCL INFO Channel 01/0 : 2[c7000] -> 3[7000] [send] via NET/IB/1/GDRDMA
gpu109-16-r:24876:24955 [0] NCCL INFO Channel 01/0 : 4[46000] -> 5[7000] [send] via NET/IB/0/GDRDMA
gpu109-16-l:53482:53572 [0] NCCL INFO Channel 01/0 : 3[7000] -> 4[46000] [send] via NET/IB/0/GDRDMA
gpu108-23-r:1142:1287 [0] NCCL INFO Channel 01/0 : 0[7000] -> 1[7000] [send] via NET/IB/0/GDRDMA
gpu201-02-r:15165:15275 [0] NCCL INFO Channel 01/0 : 7[85000] -> 0[7000] [send] via NET/IB/1/GDRDMA
gpu109-23-r:58710:58787 [0] NCCL INFO Channel 01/0 : 5[7000] -> 6[7000] [send] via NET/IB/0/GDRDMA
gpu201-02-l:12282:12441 [0] NCCL INFO Channel 01/0 : 6[7000] -> 7[85000] [send] via NET/IB/0/GDRDMA
gpu109-16-r:24876:24955 [0] NCCL INFO Connected all rings
gpu109-23-r:58710:58787 [0] NCCL INFO Connected all rings
gpu201-02-l:12282:12441 [0] NCCL INFO Connected all rings
gpu109-16-r:24876:24955 [0] NCCL INFO Channel 00/0 : 2[c7000] -> 4[46000] [receive] via NET/IB/0/GDRDMA
gpu109-23-r:58710:58787 [0] NCCL INFO Channel 01/0 : 3[7000] -> 5[7000] [receive] via NET/IB/0/GDRDMA
gpu201-02-l:12282:12441 [0] NCCL INFO Channel 00/0 : 4[46000] -> 6[7000] [receive] via NET/IB/0/GDRDMA
gpu109-02-l:65178:65256 [0] NCCL INFO Channel 01/0 : 1[7000] -> 2[c7000] [send] via NET/IB/0/GDRDMA
gpu109-16-l:53482:53572 [0] NCCL INFO Connected all rings
gpu201-02-r:15165:15275 [0] NCCL INFO Connected all rings
gpu109-02-r:49201:49461 [0] NCCL INFO Connected all rings
gpu108-23-r:1142:1287 [0] NCCL INFO Connected all rings
gpu109-02-l:65178:65256 [0] NCCL INFO Connected all rings
gpu109-16-r:24876:24955 [0] NCCL INFO Channel 00/0 : 4[46000] -> 6[7000] [send] via NET/IB/0/GDRDMA
gpu109-16-l:53482:53572 [0] NCCL INFO Channel 01/0 : 1[7000] -> 3[7000] [receive] via NET/IB/0/GDRDMA
gpu201-02-r:15165:15275 [0] NCCL INFO Channel 01/0 : 3[7000] -> 7[85000] [receive] via NET/IB/1/GDRDMA
gpu201-02-l:12282:12441 [0] NCCL INFO Channel 00/0 : 6[7000] -> 4[46000] [send] via NET/IB/0/GDRDMA
gpu109-02-r:49201:49461 [0] NCCL INFO Channel 00/0 : 2[c7000] -> 4[46000] [send] via NET/IB/1/GDRDMA
gpu108-23-r:1142:1287 [0] NCCL INFO Channel 00/0 : 4[46000] -> 0[7000] [receive] via NET/IB/0/GDRDMA
gpu109-16-r:24876:24955 [0] NCCL INFO Channel 00/0 : 0[7000] -> 4[46000] [receive] via NET/IB/0/GDRDMA
gpu109-16-l:53482:53572 [0] NCCL INFO Channel 01/0 : 3[7000] -> 5[7000] [send] via NET/IB/0/GDRDMA
gpu201-02-r:15165:15275 [0] NCCL INFO Channel 01/0 : 7[85000] -> 3[7000] [send] via NET/IB/1/GDRDMA
gpu109-02-r:49201:49461 [0] NCCL INFO Channel 00/0 : 4[46000] -> 2[c7000] [receive] via NET/IB/1/GDRDMA
gpu109-02-l:65178:65256 [0] NCCL INFO Channel 01/0 : 1[7000] -> 3[7000] [send] via NET/IB/0/GDRDMA
gpu109-23-r:58710:58787 [0] NCCL INFO Channel 01/0 : 5[7000] -> 3[7000] [send] via NET/IB/0/GDRDMA
gpu108-23-r:1142:1287 [0] NCCL INFO Channel 00/0 : 0[7000] -> 4[46000] [send] via NET/IB/0/GDRDMA
gpu109-16-r:24876:24955 [0] NCCL INFO Channel 00/0 : 4[46000] -> 0[7000] [send] via NET/IB/0/GDRDMA
gpu109-16-l:53482:53572 [0] NCCL INFO Channel 01/0 : 7[85000] -> 3[7000] [receive] via NET/IB/0/GDRDMA
gpu108-23-r:1142:1287 [0] NCCL INFO Channel 01/0 : 1[7000] -> 0[7000] [receive] via NET/IB/0/GDRDMA
gpu109-16-r:24876:24955 [0] NCCL INFO Channel 00/0 : 6[7000] -> 4[46000] [receive] via NET/IB/0/GDRDMA
gpu109-02-l:65178:65256 [0] NCCL INFO Channel 01/0 : 3[7000] -> 1[7000] [receive] via NET/IB/0/GDRDMA
gpu109-16-l:53482:53572 [0] NCCL INFO Channel 01/0 : 3[7000] -> 7[85000] [send] via NET/IB/0/GDRDMA
gpu109-16-r:24876:24955 [0] NCCL INFO Channel 00/0 : 4[46000] -> 2[c7000] [send] via NET/IB/0/GDRDMA
gpu201-02-r:15165:15275 [0] NCCL INFO Channel 00/0 : 7[85000] -> 6[7000] [send] via NET/IB/1/GDRDMA
gpu201-02-l:12282:12441 [0] NCCL INFO Channel 00/0 : 7[85000] -> 6[7000] [receive] via NET/IB/0/GDRDMA
gpu109-02-r:49201:49461 [0] NCCL INFO Channel 00/0 : 3[7000] -> 2[c7000] [receive] via NET/IB/1/GDRDMA
gpu109-16-l:53482:53572 [0] NCCL INFO Channel 01/0 : 5[7000] -> 3[7000] [receive] via NET/IB/0/GDRDMA
gpu109-16-r:24876:24955 [0] NCCL INFO Channel 01/0 : 5[7000] -> 4[46000] [receive] via NET/IB/0/GDRDMA
gpu201-02-l:12282:12441 [0] NCCL INFO Channel 00/0 : 6[7000] -> 5[7000] [send] via NET/IB/0/GDRDMA
gpu109-02-r:49201:49461 [0] NCCL INFO Channel 00/0 : 2[c7000] -> 1[7000] [send] via NET/IB/1/GDRDMA
gpu109-16-l:53482:53572 [0] NCCL INFO Channel 01/0 : 3[7000] -> 1[7000] [send] via NET/IB/0/GDRDMA
gpu109-23-r:58710:58787 [0] NCCL INFO Channel 00/0 : 6[7000] -> 5[7000] [receive] via NET/IB/0/GDRDMA
gpu201-02-l:12282:12441 [0] NCCL INFO Channel 01/0 : 6[7000] -> 5[7000] [send] via NET/IB/0/GDRDMA
gpu109-02-r:49201:49461 [0] NCCL INFO Channel 01/0 : 2[c7000] -> 1[7000] [send] via NET/IB/1/GDRDMA
gpu109-16-l:53482:53572 [0] NCCL INFO Channel 00/0 : 3[7000] -> 2[c7000] [send] via NET/IB/0/GDRDMA
gpu109-02-l:65178:65256 [0] NCCL INFO Channel 00/0 : 2[c7000] -> 1[7000] [receive] via NET/IB/0/GDRDMA
gpu109-23-r:58710:58787 [0] NCCL INFO Channel 01/0 : 6[7000] -> 5[7000] [receive] via NET/IB/0/GDRDMA
gpu109-23-r:58710:58787 [0] NCCL INFO Channel 01/0 : 5[7000] -> 4[46000] [send] via NET/IB/0/GDRDMA
gpu201-02-r:15165:15275 [0] NCCL INFO Connected all trees
gpu201-02-r:15165:15275 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu201-02-r:15165:15275 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
gpu109-02-l:65178:65256 [0] NCCL INFO Channel 01/0 : 2[c7000] -> 1[7000] [receive] via NET/IB/0/GDRDMA
gpu201-02-l:12282:12441 [0] NCCL INFO Connected all trees
gpu201-02-l:12282:12441 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu201-02-l:12282:12441 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
gpu109-23-r:58710:58787 [0] NCCL INFO Connected all trees
gpu109-23-r:58710:58787 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu109-23-r:58710:58787 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
gpu201-02-r:15165:15275 [0] NCCL INFO comm 0x39a525c0 rank 7 nranks 8 cudaDev 0 busId 85000 commId 0x7804668fe7810aa8 - Init COMPLETE
gpu201-02-l:12282:12441 [0] NCCL INFO comm 0x38e1db40 rank 6 nranks 8 cudaDev 0 busId 7000 commId 0x7804668fe7810aa8 - Init COMPLETE
gpu109-23-r:58710:58787 [0] NCCL INFO comm 0x3b803cd0 rank 5 nranks 8 cudaDev 0 busId 7000 commId 0x7804668fe7810aa8 - Init COMPLETE
gpu109-02-l:65178:65256 [0] NCCL INFO Channel 01/0 : 1[7000] -> 0[7000] [send] via NET/IB/0/GDRDMA
gpu109-16-r:24876:24955 [0] NCCL INFO Connected all trees
gpu109-16-r:24876:24955 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu109-16-r:24876:24955 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
gpu109-16-l:53482:53572 [0] NCCL INFO Connected all trees
gpu109-16-l:53482:53572 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu109-16-l:53482:53572 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
gpu108-23-r:1142:1287 [0] NCCL INFO Connected all trees
gpu109-02-r:49201:49461 [0] NCCL INFO Connected all trees
gpu108-23-r:1142:1287 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-23-r:1142:1287 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
gpu109-02-r:49201:49461 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu109-02-r:49201:49461 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
gpu109-02-l:65178:65256 [0] NCCL INFO Connected all trees
gpu109-02-l:65178:65256 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu109-02-l:65178:65256 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
gpu109-16-r:24876:24955 [0] NCCL INFO comm 0x39d09b90 rank 4 nranks 8 cudaDev 0 busId 46000 commId 0x7804668fe7810aa8 - Init COMPLETE
gpu109-16-l:53482:53572 [0] NCCL INFO comm 0x3a1d37c0 rank 3 nranks 8 cudaDev 0 busId 7000 commId 0x7804668fe7810aa8 - Init COMPLETE
gpu108-23-r:1142:1287 [0] NCCL INFO comm 0x3b497d00 rank 0 nranks 8 cudaDev 0 busId 7000 commId 0x7804668fe7810aa8 - Init COMPLETE
gpu109-02-r:49201:49461 [0] NCCL INFO comm 0x3cd24180 rank 2 nranks 8 cudaDev 0 busId c7000 commId 0x7804668fe7810aa8 - Init COMPLETE
gpu109-02-l:65178:65256 [0] NCCL INFO comm 0x3a3c8a40 rank 1 nranks 8 cudaDev 0 busId 7000 commId 0x7804668fe7810aa8 - Init COMPLETE
0 0 Helloworld here 1
1 0 Helloworld here 1
3 0 Helloworld here 1
6 0 Helloworld here 1
5 0 Helloworld here 1
4 0 Helloworld here 1
2 0 Helloworld here 1
7 0 Helloworld here 1
0 0 Helloworld here 2
4 0 Helloworld here 2
2 0 Helloworld here 2
6 0 Helloworld here 2
5 0 Helloworld here 2
7 0 Helloworld here 2
>>> done with compiling and loading fused kernels. Compilation time: 3.747 seconds
3 0 Helloworld here 2
1 0 Helloworld here 2
time to initialize megatron (seconds): 49.656
[after megatron is initialized] datetime: 2023-03-17 12:15:29 
building GPT model ...
[2023-03-17 12:15:29,695] [INFO] [utils.py:829:see_memory_usage] Before Building Model
[2023-03-17 12:15:29,696] [INFO] [utils.py:830:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2023-03-17 12:15:29,696] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 29.28 GB, percent = 5.8%
gpu108-23-r:1142:1301 [0] NCCL INFO Using network IB
gpu109-23-r:58710:58795 [0] NCCL INFO Using network IB
gpu201-02-l:12282:12473 [0] NCCL INFO Using network IB
gpu109-16-l:53482:53580 [0] NCCL INFO Using network IB
gpu109-02-r:49201:49533 [0] NCCL INFO Using network IB
gpu109-16-r:24876:24973 [0] NCCL INFO Using network IB
gpu109-02-l:65178:65269 [0] NCCL INFO Using network IB
gpu201-02-r:15165:15284 [0] NCCL INFO Using network IB
gpu108-23-r:1142:1301 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu109-02-r:49201:49533 [0] NCCL INFO Setting affinity for GPU 0 to 3f00ff80
gpu109-02-r:49201:49533 [0] NCCL INFO Trees [0] 1/3/-1->2->4 [1] -1/-1/-1->2->1
gpu109-02-r:49201:49533 [0] NCCL INFO P2P Chunksize set to 131072
gpu109-16-l:53482:53580 [0] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 5/1/-1->3->7
gpu109-16-l:53482:53580 [0] NCCL INFO P2P Chunksize set to 131072
gpu109-16-r:24876:24973 [0] NCCL INFO Trees [0] 2/6/-1->4->0 [1] -1/-1/-1->4->5
gpu109-16-r:24876:24973 [0] NCCL INFO P2P Chunksize set to 131072
gpu109-23-r:58710:58795 [0] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] 6/4/-1->5->3
gpu109-23-r:58710:58795 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:1142:1301 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7
gpu108-23-r:1142:1301 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7
gpu201-02-r:15165:15284 [0] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 3/-1/-1->7->-1
gpu201-02-r:15165:15284 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:1142:1301 [0] NCCL INFO Trees [0] 4/-1/-1->0->-1 [1] -1/-1/-1->0->1
gpu108-23-r:1142:1301 [0] NCCL INFO P2P Chunksize set to 131072
gpu201-02-l:12282:12473 [0] NCCL INFO Trees [0] 5/7/-1->6->4 [1] -1/-1/-1->6->5
gpu201-02-l:12282:12473 [0] NCCL INFO P2P Chunksize set to 131072
gpu109-02-l:65178:65269 [0] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] 2/0/-1->1->3
gpu109-02-l:65178:65269 [0] NCCL INFO P2P Chunksize set to 131072
gpu109-02-r:49201:49533 [0] NCCL INFO Channel 00/0 : 1[7000] -> 2[c7000] [receive] via NET/IB/1/GDRDMA
gpu109-16-r:24876:24973 [0] NCCL INFO Channel 00/0 : 3[7000] -> 4[46000] [receive] via NET/IB/0/GDRDMA
gpu109-02-l:65178:65269 [0] NCCL INFO Channel 00/0 : 0[7000] -> 1[7000] [receive] via NET/IB/0/GDRDMA
gpu201-02-r:15165:15284 [0] NCCL INFO Channel 00/0 : 6[7000] -> 7[85000] [receive] via NET/IB/1/GDRDMA
gpu109-23-r:58710:58795 [0] NCCL INFO Channel 00/0 : 4[46000] -> 5[7000] [receive] via NET/IB/0/GDRDMA
gpu201-02-l:12282:12473 [0] NCCL INFO Channel 00/0 : 5[7000] -> 6[7000] [receive] via NET/IB/0/GDRDMA
gpu109-16-l:53482:53580 [0] NCCL INFO Channel 00/0 : 2[c7000] -> 3[7000] [receive] via NET/IB/0/GDRDMA
gpu108-23-r:1142:1301 [0] NCCL INFO Channel 00/0 : 7[85000] -> 0[7000] [receive] via NET/IB/0/GDRDMA
gpu109-02-r:49201:49533 [0] NCCL INFO Channel 01/0 : 1[7000] -> 2[c7000] [receive] via NET/IB/1/GDRDMA
gpu109-16-r:24876:24973 [0] NCCL INFO Channel 01/0 : 3[7000] -> 4[46000] [receive] via NET/IB/0/GDRDMA
gpu201-02-r:15165:15284 [0] NCCL INFO Channel 01/0 : 6[7000] -> 7[85000] [receive] via NET/IB/1/GDRDMA
gpu109-23-r:58710:58795 [0] NCCL INFO Channel 01/0 : 4[46000] -> 5[7000] [receive] via NET/IB/0/GDRDMA
gpu201-02-l:12282:12473 [0] NCCL INFO Channel 01/0 : 5[7000] -> 6[7000] [receive] via NET/IB/0/GDRDMA
gpu109-16-l:53482:53580 [0] NCCL INFO Channel 01/0 : 2[c7000] -> 3[7000] [receive] via NET/IB/0/GDRDMA
gpu108-23-r:1142:1301 [0] NCCL INFO Channel 01/0 : 7[85000] -> 0[7000] [receive] via NET/IB/0/GDRDMA
gpu109-02-l:65178:65269 [0] NCCL INFO Channel 01/0 : 0[7000] -> 1[7000] [receive] via NET/IB/0/GDRDMA
gpu109-02-r:49201:49533 [0] NCCL INFO Channel 00/0 : 2[c7000] -> 3[7000] [send] via NET/IB/1/GDRDMA
gpu109-16-r:24876:24973 [0] NCCL INFO Channel 00/0 : 4[46000] -> 5[7000] [send] via NET/IB/0/GDRDMA
gpu201-02-r:15165:15284 [0] NCCL INFO Channel 00/0 : 7[85000] -> 0[7000] [send] via NET/IB/1/GDRDMA
gpu109-23-r:58710:58795 [0] NCCL INFO Channel 00/0 : 5[7000] -> 6[7000] [send] via NET/IB/0/GDRDMA
gpu201-02-l:12282:12473 [0] NCCL INFO Channel 00/0 : 6[7000] -> 7[85000] [send] via NET/IB/0/GDRDMA
gpu109-16-l:53482:53580 [0] NCCL INFO Channel 00/0 : 3[7000] -> 4[46000] [send] via NET/IB/0/GDRDMA
gpu108-23-r:1142:1301 [0] NCCL INFO Channel 00/0 : 0[7000] -> 1[7000] [send] via NET/IB/0/GDRDMA
gpu109-02-l:65178:65269 [0] NCCL INFO Channel 00/0 : 1[7000] -> 2[c7000] [send] via NET/IB/0/GDRDMA
gpu109-02-r:49201:49533 [0] NCCL INFO Channel 01/0 : 2[c7000] -> 3[7000] [send] via NET/IB/1/GDRDMA
gpu109-16-r:24876:24973 [0] NCCL INFO Channel 01/0 : 4[46000] -> 5[7000] [send] via NET/IB/0/GDRDMA
gpu201-02-r:15165:15284 [0] NCCL INFO Channel 01/0 : 7[85000] -> 0[7000] [send] via NET/IB/1/GDRDMA
gpu109-23-r:58710:58795 [0] NCCL INFO Channel 01/0 : 5[7000] -> 6[7000] [send] via NET/IB/0/GDRDMA
gpu201-02-l:12282:12473 [0] NCCL INFO Channel 01/0 : 6[7000] -> 7[85000] [send] via NET/IB/0/GDRDMA
gpu109-16-l:53482:53580 [0] NCCL INFO Channel 01/0 : 3[7000] -> 4[46000] [send] via NET/IB/0/GDRDMA
gpu108-23-r:1142:1301 [0] NCCL INFO Channel 01/0 : 0[7000] -> 1[7000] [send] via NET/IB/0/GDRDMA
gpu109-23-r:58710:58795 [0] NCCL INFO Connected all rings
gpu109-16-r:24876:24973 [0] NCCL INFO Connected all rings
gpu201-02-l:12282:12473 [0] NCCL INFO Connected all rings
gpu109-02-l:65178:65269 [0] NCCL INFO Channel 01/0 : 1[7000] -> 2[c7000] [send] via NET/IB/0/GDRDMA
gpu109-16-l:53482:53580 [0] NCCL INFO Connected all rings
gpu201-02-r:15165:15284 [0] NCCL INFO Connected all rings
gpu109-23-r:58710:58795 [0] NCCL INFO Channel 01/0 : 3[7000] -> 5[7000] [receive] via NET/IB/0/GDRDMA
gpu109-16-r:24876:24973 [0] NCCL INFO Channel 00/0 : 2[c7000] -> 4[46000] [receive] via NET/IB/0/GDRDMA
gpu201-02-l:12282:12473 [0] NCCL INFO Channel 00/0 : 4[46000] -> 6[7000] [receive] via NET/IB/0/GDRDMA
gpu109-02-r:49201:49533 [0] NCCL INFO Connected all rings
gpu108-23-r:1142:1301 [0] NCCL INFO Connected all rings
gpu109-02-l:65178:65269 [0] NCCL INFO Connected all rings
gpu109-16-l:53482:53580 [0] NCCL INFO Channel 01/0 : 1[7000] -> 3[7000] [receive] via NET/IB/0/GDRDMA
gpu201-02-r:15165:15284 [0] NCCL INFO Channel 01/0 : 3[7000] -> 7[85000] [receive] via NET/IB/1/GDRDMA
gpu109-16-r:24876:24973 [0] NCCL INFO Channel 00/0 : 4[46000] -> 6[7000] [send] via NET/IB/0/GDRDMA
gpu109-02-r:49201:49533 [0] NCCL INFO Channel 00/0 : 2[c7000] -> 4[46000] [send] via NET/IB/1/GDRDMA
gpu108-23-r:1142:1301 [0] NCCL INFO Channel 00/0 : 4[46000] -> 0[7000] [receive] via NET/IB/0/GDRDMA
gpu201-02-l:12282:12473 [0] NCCL INFO Channel 00/0 : 6[7000] -> 4[46000] [send] via NET/IB/0/GDRDMA
gpu109-16-l:53482:53580 [0] NCCL INFO Channel 01/0 : 3[7000] -> 5[7000] [send] via NET/IB/0/GDRDMA
gpu201-02-r:15165:15284 [0] NCCL INFO Channel 01/0 : 7[85000] -> 3[7000] [send] via NET/IB/1/GDRDMA
gpu109-02-r:49201:49533 [0] NCCL INFO Channel 00/0 : 4[46000] -> 2[c7000] [receive] via NET/IB/1/GDRDMA
gpu109-16-r:24876:24973 [0] NCCL INFO Channel 00/0 : 0[7000] -> 4[46000] [receive] via NET/IB/0/GDRDMA
gpu109-02-l:65178:65269 [0] NCCL INFO Channel 01/0 : 1[7000] -> 3[7000] [send] via NET/IB/0/GDRDMA
gpu109-23-r:58710:58795 [0] NCCL INFO Channel 01/0 : 5[7000] -> 3[7000] [send] via NET/IB/0/GDRDMA
gpu108-23-r:1142:1301 [0] NCCL INFO Channel 00/0 : 0[7000] -> 4[46000] [send] via NET/IB/0/GDRDMA
gpu109-16-r:24876:24973 [0] NCCL INFO Channel 00/0 : 4[46000] -> 0[7000] [send] via NET/IB/0/GDRDMA
gpu109-16-l:53482:53580 [0] NCCL INFO Channel 01/0 : 7[85000] -> 3[7000] [receive] via NET/IB/0/GDRDMA
gpu108-23-r:1142:1301 [0] NCCL INFO Channel 01/0 : 1[7000] -> 0[7000] [receive] via NET/IB/0/GDRDMA
gpu109-02-l:65178:65269 [0] NCCL INFO Channel 01/0 : 3[7000] -> 1[7000] [receive] via NET/IB/0/GDRDMA
gpu109-16-r:24876:24973 [0] NCCL INFO Channel 00/0 : 6[7000] -> 4[46000] [receive] via NET/IB/0/GDRDMA
gpu109-16-l:53482:53580 [0] NCCL INFO Channel 01/0 : 3[7000] -> 7[85000] [send] via NET/IB/0/GDRDMA
gpu201-02-r:15165:15284 [0] NCCL INFO Channel 00/0 : 7[85000] -> 6[7000] [send] via NET/IB/1/GDRDMA
gpu109-16-r:24876:24973 [0] NCCL INFO Channel 00/0 : 4[46000] -> 2[c7000] [send] via NET/IB/0/GDRDMA
gpu109-16-l:53482:53580 [0] NCCL INFO Channel 01/0 : 5[7000] -> 3[7000] [receive] via NET/IB/0/GDRDMA
gpu201-02-l:12282:12473 [0] NCCL INFO Channel 00/0 : 7[85000] -> 6[7000] [receive] via NET/IB/0/GDRDMA
gpu109-02-r:49201:49533 [0] NCCL INFO Channel 00/0 : 3[7000] -> 2[c7000] [receive] via NET/IB/1/GDRDMA
gpu109-16-r:24876:24973 [0] NCCL INFO Channel 01/0 : 5[7000] -> 4[46000] [receive] via NET/IB/0/GDRDMA
gpu109-16-l:53482:53580 [0] NCCL INFO Channel 01/0 : 3[7000] -> 1[7000] [send] via NET/IB/0/GDRDMA
gpu201-02-l:12282:12473 [0] NCCL INFO Channel 00/0 : 6[7000] -> 5[7000] [send] via NET/IB/0/GDRDMA
gpu109-02-r:49201:49533 [0] NCCL INFO Channel 00/0 : 2[c7000] -> 1[7000] [send] via NET/IB/1/GDRDMA
gpu109-23-r:58710:58795 [0] NCCL INFO Channel 00/0 : 6[7000] -> 5[7000] [receive] via NET/IB/0/GDRDMA
gpu109-16-l:53482:53580 [0] NCCL INFO Channel 00/0 : 3[7000] -> 2[c7000] [send] via NET/IB/0/GDRDMA
gpu109-02-l:65178:65269 [0] NCCL INFO Channel 00/0 : 2[c7000] -> 1[7000] [receive] via NET/IB/0/GDRDMA
gpu201-02-l:12282:12473 [0] NCCL INFO Channel 01/0 : 6[7000] -> 5[7000] [send] via NET/IB/0/GDRDMA
gpu109-02-r:49201:49533 [0] NCCL INFO Channel 01/0 : 2[c7000] -> 1[7000] [send] via NET/IB/1/GDRDMA
gpu109-23-r:58710:58795 [0] NCCL INFO Channel 01/0 : 6[7000] -> 5[7000] [receive] via NET/IB/0/GDRDMA
gpu109-23-r:58710:58795 [0] NCCL INFO Channel 01/0 : 5[7000] -> 4[46000] [send] via NET/IB/0/GDRDMA
gpu201-02-r:15165:15284 [0] NCCL INFO Connected all trees
gpu201-02-r:15165:15284 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu201-02-r:15165:15284 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
gpu201-02-l:12282:12473 [0] NCCL INFO Connected all trees
gpu201-02-l:12282:12473 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu201-02-l:12282:12473 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
gpu109-02-l:65178:65269 [0] NCCL INFO Channel 01/0 : 2[c7000] -> 1[7000] [receive] via NET/IB/0/GDRDMA
gpu109-23-r:58710:58795 [0] NCCL INFO Connected all trees
gpu109-23-r:58710:58795 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu109-23-r:58710:58795 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
gpu201-02-r:15165:15284 [0] NCCL INFO comm 0x35234990 rank 7 nranks 8 cudaDev 0 busId 85000 commId 0x1cc952cc155a674f - Init COMPLETE
gpu201-02-l:12282:12473 [0] NCCL INFO comm 0x34f33070 rank 6 nranks 8 cudaDev 0 busId 7000 commId 0x1cc952cc155a674f - Init COMPLETE
gpu109-23-r:58710:58795 [0] NCCL INFO comm 0x37971350 rank 5 nranks 8 cudaDev 0 busId 7000 commId 0x1cc952cc155a674f - Init COMPLETE
gpu109-02-l:65178:65269 [0] NCCL INFO Channel 01/0 : 1[7000] -> 0[7000] [send] via NET/IB/0/GDRDMA
gpu109-16-r:24876:24973 [0] NCCL INFO Connected all trees
gpu109-16-r:24876:24973 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu109-16-r:24876:24973 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
gpu109-16-l:53482:53580 [0] NCCL INFO Connected all trees
gpu109-16-l:53482:53580 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu109-16-l:53482:53580 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
gpu108-23-r:1142:1301 [0] NCCL INFO Connected all trees
gpu108-23-r:1142:1301 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu108-23-r:1142:1301 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
gpu109-02-r:49201:49533 [0] NCCL INFO Connected all trees
gpu109-02-r:49201:49533 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu109-02-r:49201:49533 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
gpu109-02-l:65178:65269 [0] NCCL INFO Connected all trees
gpu109-02-l:65178:65269 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
gpu109-02-l:65178:65269 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
gpu109-16-r:24876:24973 [0] NCCL INFO comm 0x35e70d30 rank 4 nranks 8 cudaDev 0 busId 46000 commId 0x1cc952cc155a674f - Init COMPLETE
gpu109-16-l:53482:53580 [0] NCCL INFO comm 0x35f9af60 rank 3 nranks 8 cudaDev 0 busId 7000 commId 0x1cc952cc155a674f - Init COMPLETE
gpu109-02-r:49201:49533 [0] NCCL INFO comm 0x388500a0 rank 2 nranks 8 cudaDev 0 busId c7000 commId 0x1cc952cc155a674f - Init COMPLETE
gpu108-23-r:1142:1301 [0] NCCL INFO comm 0x36d1d940 rank 0 nranks 8 cudaDev 0 busId 7000 commId 0x1cc952cc155a674f - Init COMPLETE
[2023-03-17 12:15:31,151] [INFO] [partition_parameters.py:415:__exit__] finished initializing model with 0.07B parameters
[2023-03-17 12:15:31,184] [INFO] [utils.py:829:see_memory_usage] After Building Model
[2023-03-17 12:15:31,185] [INFO] [utils.py:830:see_memory_usage] MA 0.02 GB         Max_MA 0.11 GB         CA 0.15 GB         Max_CA 0 GB 
[2023-03-17 12:15:31,185] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 29.34 GB, percent = 5.8%
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 65158144
gpu109-02-l:65178:65269 [0] NCCL INFO comm 0x34e731a0 rank 1 nranks 8 cudaDev 0 busId 7000 commId 0x1cc952cc155a674f - Init COMPLETE
> learning rate decay style: cosine
DeepSpeed is enabled.
[2023-03-17 12:15:31,243] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed info: version=0.8.3+bbfd0a6, git-hash=bbfd0a6, git-branch=master
[2023-03-17 12:15:31,245] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-03-17 12:15:31,246] [INFO] [logging.py:93:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-03-17 12:15:31,246] [INFO] [logging.py:93:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-03-17 12:15:31,246] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2023-03-17 12:15:31,246] [INFO] [utils.py:55:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'apex.optimizers.fused_adam.FusedAdam'>
[2023-03-17 12:15:31,246] [INFO] [logging.py:93:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[2023-03-17 12:15:31,282] [INFO] [utils.py:829:see_memory_usage] Stage 3 initialize beginning
[2023-03-17 12:15:31,282] [INFO] [utils.py:830:see_memory_usage] MA 0.02 GB         Max_MA 0.02 GB         CA 0.15 GB         Max_CA 0 GB 
[2023-03-17 12:15:31,282] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 29.34 GB, percent = 5.8%
[2023-03-17 12:15:31,283] [INFO] [stage3.py:113:__init__] Reduce bucket size 90000000
[2023-03-17 12:15:31,283] [INFO] [stage3.py:114:__init__] Prefetch bucket size 50000000
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Emitting ninja build file /home/shaima0d/.cache/torch_extensions/py39_cu117/utils/build.ninja...
Building extension module utils...
Using envvar MAX_JOBS (15) as the number of workers...
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.14678192138671875 seconds
Loading extension module utils...
Loading extension module utils...
Loading extension module utils...
Loading extension module utils...
Loading extension module utils...
Loading extension module utils...
Time to load utils op: 3.0157201290130615 seconds
Time to load utils op: 3.015407085418701 seconds
Time to load utils op: 3.016066789627075 seconds
Time to load utils op: 3.015347719192505 seconds
Time to load utils op: 3.0157554149627686 seconds
Time to load utils op: 3.0148537158966064 seconds
Loading extension module utils...
Time to load utils op: 3.01509952545166 seconds
[2023-03-17 12:15:34,325] [INFO] [utils.py:829:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2023-03-17 12:15:34,326] [INFO] [utils.py:830:see_memory_usage] MA 0.02 GB         Max_MA 0.02 GB         CA 0.15 GB         Max_CA 0 GB 
[2023-03-17 12:15:34,326] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 29.34 GB, percent = 5.8%
Parameter Offload: Total persistent parameters: 15360 in 10 params
[2023-03-17 12:15:34,351] [INFO] [utils.py:829:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2023-03-17 12:15:34,352] [INFO] [utils.py:830:see_memory_usage] MA 0.02 GB         Max_MA 0.02 GB         CA 0.15 GB         Max_CA 0 GB 
[2023-03-17 12:15:34,352] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 29.34 GB, percent = 5.8%
[2023-03-17 12:15:34,377] [INFO] [utils.py:829:see_memory_usage] Before creating fp16 partitions
[2023-03-17 12:15:34,377] [INFO] [utils.py:830:see_memory_usage] MA 0.02 GB         Max_MA 0.02 GB         CA 0.15 GB         Max_CA 0 GB 
[2023-03-17 12:15:34,378] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 29.34 GB, percent = 5.8%
[2023-03-17 12:15:34,440] [INFO] [utils.py:829:see_memory_usage] After creating fp16 partitions: 2
[2023-03-17 12:15:34,441] [INFO] [utils.py:830:see_memory_usage] MA 0.02 GB         Max_MA 0.02 GB         CA 0.02 GB         Max_CA 0 GB 
[2023-03-17 12:15:34,441] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 29.35 GB, percent = 5.8%
[2023-03-17 12:15:34,465] [INFO] [utils.py:829:see_memory_usage] Before creating fp32 partitions
[2023-03-17 12:15:34,466] [INFO] [utils.py:830:see_memory_usage] MA 0.02 GB         Max_MA 0.02 GB         CA 0.02 GB         Max_CA 0 GB 
[2023-03-17 12:15:34,466] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 29.35 GB, percent = 5.8%
[2023-03-17 12:15:34,491] [INFO] [utils.py:829:see_memory_usage] After creating fp32 partitions
[2023-03-17 12:15:34,492] [INFO] [utils.py:830:see_memory_usage] MA 0.05 GB         Max_MA 0.06 GB         CA 0.06 GB         Max_CA 0 GB 
[2023-03-17 12:15:34,492] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 29.35 GB, percent = 5.8%
[2023-03-17 12:15:34,517] [INFO] [utils.py:829:see_memory_usage] Before initializing optimizer states
[2023-03-17 12:15:34,517] [INFO] [utils.py:830:see_memory_usage] MA 0.05 GB         Max_MA 0.05 GB         CA 0.06 GB         Max_CA 0 GB 
[2023-03-17 12:15:34,518] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 29.35 GB, percent = 5.8%
[2023-03-17 12:15:34,519] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | init_optimizer_state: 0.74
[2023-03-17 12:15:34,544] [INFO] [utils.py:829:see_memory_usage] After initializing optimizer states
[2023-03-17 12:15:34,544] [INFO] [utils.py:830:see_memory_usage] MA 0.11 GB         Max_MA 0.14 GB         CA 0.16 GB         Max_CA 0 GB 
[2023-03-17 12:15:34,544] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 29.35 GB, percent = 5.8%
[2023-03-17 12:15:34,545] [INFO] [stage3.py:376:_setup_for_real_optimizer] optimizer state initialized
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Time to load utils op: 0.00035071372985839844 seconds
No modifications detected for re-loaded extension module utils, skipping build step...
Time to load utils op: 0.0003383159637451172 seconds
No modifications detected for re-loaded extension module utils, skipping build step...
Time to load utils op: 0.0003714561462402344 seconds
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Time to load utils op: 0.0003285408020019531 seconds
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Time to load utils op: 0.0003540515899658203 seconds
Loading extension module utils...
Time to load utils op: 0.00037932395935058594 seconds
No modifications detected for re-loaded extension module utils, skipping build step...
Time to load utils op: 0.0004923343658447266 seconds
Loading extension module utils...
Loading extension module utils...
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Loading extension module utils...
No modifications detected for re-loaded extension module utils, skipping build step...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Loading extension module utils...
[2023-03-17 12:15:34,577] [INFO] [utils.py:829:see_memory_usage] After initializing ZeRO optimizer
[2023-03-17 12:15:34,578] [INFO] [utils.py:830:see_memory_usage] MA 0.29 GB         Max_MA 0.48 GB         CA 0.62 GB         Max_CA 1 GB 
[2023-03-17 12:15:34,578] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 29.35 GB, percent = 5.8%
[2023-03-17 12:15:34,578] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[2023-03-17 12:15:34,578] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-03-17 12:15:34,578] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x2b31e3fd3520>
[2023-03-17 12:15:34,578] [INFO] [logging.py:93:log_dist] [Rank 0] step=0, skipped=0, lr=[5.9999999999999995e-05, 5.9999999999999995e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:34,578] [INFO] [config.py:1018:print] DeepSpeedEngine configuration:
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 16, 'thread_count': 2, 'single_submit': False, 'overlap_events': True}
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   amp_enabled .................. False
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   amp_params ................... False
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   bfloat16_enabled ............. False
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   checkpoint_parallel_write_pipeline  False
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   checkpoint_tag_validation_enabled  True
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   checkpoint_tag_validation_fail  False
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x2b31e3fd3a90>
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   communication_data_type ...... None
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   curriculum_enabled_legacy .... False
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   curriculum_params_legacy ..... False
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   data_efficiency_enabled ...... False
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   dataloader_drop_last ......... False
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   disable_allgather ............ False
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   dump_state ................... False
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   dynamic_loss_scale_args ...... {'init_scale': 32768, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   eigenvalue_enabled ........... False
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   eigenvalue_gas_boundary_resolution  1
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   eigenvalue_layer_num ......... 0
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   eigenvalue_max_iter .......... 100
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   eigenvalue_stability ......... 1e-06
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   eigenvalue_tol ............... 0.01
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   eigenvalue_verbose ........... False
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   elasticity_enabled ........... False
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   fp16_auto_cast ............... False
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   fp16_enabled ................. True
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   fp16_master_weights_and_gradients  False
[2023-03-17 12:15:34,579] [INFO] [config.py:1022:print]   global_rank .................. 0
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   grad_accum_dtype ............. None
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   gradient_accumulation_steps .. 1
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   gradient_clipping ............ 1
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   gradient_predivide_factor .... 1.0
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   initial_dynamic_scale ........ 32768
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   load_universal_checkpoint .... False
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   loss_scale ................... 0
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   memory_breakdown ............. False
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   optimizer_legacy_fusion ...... False
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   optimizer_name ............... None
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   optimizer_params ............. None
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   pld_enabled .................. False
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   pld_params ................... False
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   prescale_gradients ........... False
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   scheduler_name ............... None
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   scheduler_params ............. None
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   sparse_attention ............. None
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   sparse_gradients_enabled ..... False
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   steps_per_print .............. 1
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   train_batch_size ............. 32
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   train_micro_batch_size_per_gpu  4
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   use_node_local_storage ....... False
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   wall_clock_breakdown ......... True
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   world_size ................... 8
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   zero_allow_untested_optimizer  False
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=90000000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=sys.maxsize max_live_parameters=3000000000 max_reuse_distance=3000000000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   zero_enabled ................. True
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   zero_force_ds_cpu_optimizer .. True
[2023-03-17 12:15:34,580] [INFO] [config.py:1022:print]   zero_optimization_stage ...... 3
[2023-03-17 12:15:34,580] [INFO] [config.py:1007:print_user_config]   json = {
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 4, 
    "steps_per_print": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 3, 
        "stage3_max_live_parameters": 3.000000e+09, 
        "stage3_max_reuse_distance": 3.000000e+09, 
        "stage3_param_persistence_threshold": 1.000000e+05, 
        "stage3_prefetch_bucket_size": 5.000000e+07, 
        "contiguous_gradients": true, 
        "overlap_comm": true, 
        "reduce_bucket_size": 9.000000e+07, 
        "sub_group_size": 1.000000e+09, 
        "offload_optimizer": {
            "device": "none", 
            "buffer_count": 4, 
            "pipeline_read": false, 
            "pipeline_write": false, 
            "pin_memory": true
        }
    }, 
    "gradient_clipping": 1, 
    "fp16": {
        "enabled": true, 
        "initial_scale_power": 15, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "wall_clock_breakdown": true, 
    "zero_allow_untested_optimizer": false, 
    "aio": {
        "block_size": 1.048576e+06, 
        "queue_depth": 16, 
        "single_submit": false, 
        "overlap_events": true, 
        "thread_count": 2
    }
}
Using /home/shaima0d/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0003006458282470703 seconds
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-03-17 12:15:34 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      1600
    validation: 1280
    test:       1280
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.000773 seconds
    number of documents: 17868
 > dataset split:
    train:
     document indices in [0, 17511) total of 17511 documents
    validation:
     document indices in [17511, 17868) total of 357 documents
    test:
     document indices in [17868, 17868) total of 0 documents
NCCL version 2.17.1+cuda11.7
NCCL version 2.17.1+cuda11.7
NCCL version 2.17.1+cuda11.7
NCCL version 2.17.1+cuda11.7
NCCL version 2.17.1+cuda11.7
gpu109-23-r:58710:58804 [0] NCCL INFO Using network IB
gpu201-02-l:12282:12550 [0] NCCL INFO Using network IB
gpu109-16-r:24876:24986 [0] NCCL INFO Using network IB
gpu108-23-r:1142:1311 [0] NCCL INFO Using network IB
gpu201-02-r:15165:15295 [0] NCCL INFO Using network IB
gpu109-02-l:65178:65284 [0] NCCL INFO Using network IB
NCCL version 2.17.1+cuda11.7
gpu109-16-l:53482:53591 [0] NCCL INFO Using network IB
gpu201-02-r:15165:15295 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu109-16-r:24876:24986 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 00/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 01/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 02/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 03/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 04/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 05/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 06/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 07/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 08/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 09/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 10/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 11/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 12/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 13/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 14/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 15/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 16/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 17/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 18/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 19/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 20/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 21/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 22/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 23/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 24/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 25/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 26/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 27/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 28/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 29/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 30/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Channel 31/32 :    0
gpu201-02-r:15165:15295 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu201-02-r:15165:15295 [0] NCCL INFO P2P Chunksize set to 131072
gpu201-02-r:15165:15295 [0] NCCL INFO Connected all rings
gpu201-02-r:15165:15295 [0] NCCL INFO Connected all trees
gpu201-02-r:15165:15295 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 00/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 01/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 02/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 03/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 04/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 05/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 06/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 07/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 08/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 09/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 10/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 11/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 12/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 13/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 14/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 15/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 16/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 17/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 18/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 19/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 20/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 21/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 22/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 23/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 24/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 25/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 26/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 27/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 28/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 29/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 30/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Channel 31/32 :    0
gpu109-16-r:24876:24986 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu109-16-r:24876:24986 [0] NCCL INFO P2P Chunksize set to 131072
gpu109-16-r:24876:24986 [0] NCCL INFO Connected all rings
gpu109-16-r:24876:24986 [0] NCCL INFO Connected all trees
gpu109-16-r:24876:24986 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:1142:1311 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 00/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 01/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 02/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 03/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 04/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 05/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 06/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 07/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 08/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 09/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 10/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 11/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 12/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 13/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 14/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 15/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 16/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 17/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 18/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 19/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 20/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 21/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 22/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 23/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 24/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 25/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 26/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 27/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 28/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 29/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 30/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Channel 31/32 :    0
gpu108-23-r:1142:1311 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-r:1142:1311 [0] NCCL INFO P2P Chunksize set to 131072
gpu108-23-r:1142:1311 [0] NCCL INFO Connected all rings
gpu108-23-r:1142:1311 [0] NCCL INFO Connected all trees
gpu108-23-r:1142:1311 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu109-16-l:53482:53591 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 00/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 01/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 02/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 03/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 04/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 05/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 06/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 07/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 08/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 09/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 10/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 11/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 12/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 13/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 14/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 15/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 16/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 17/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 18/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 19/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 20/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 21/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 22/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 23/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 24/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 25/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 26/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 27/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 28/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 29/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 30/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Channel 31/32 :    0
gpu109-16-l:53482:53591 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu109-16-l:53482:53591 [0] NCCL INFO P2P Chunksize set to 131072
gpu109-16-l:53482:53591 [0] NCCL INFO Connected all rings
gpu109-16-l:53482:53591 [0] NCCL INFO Connected all trees
gpu109-16-l:53482:53591 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 00/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 01/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 02/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 03/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 04/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 05/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 06/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 07/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 08/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 09/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 10/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 11/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 12/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 13/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 14/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 15/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 16/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 17/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 18/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 19/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 20/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 21/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 22/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 23/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 24/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 25/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 26/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 27/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 28/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 29/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 30/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Channel 31/32 :    0
gpu109-02-l:65178:65284 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu109-02-l:65178:65284 [0] NCCL INFO P2P Chunksize set to 131072
gpu109-02-l:65178:65284 [0] NCCL INFO Connected all rings
gpu109-02-l:65178:65284 [0] NCCL INFO Connected all trees
gpu109-02-l:65178:65284 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu109-23-r:58710:58804 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 00/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 01/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 02/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 03/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 04/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 05/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 06/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 07/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 08/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 09/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 10/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 11/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 12/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 13/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 14/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 15/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 16/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 17/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 18/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 19/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 20/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 21/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 22/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 23/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 24/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 25/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 26/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 27/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 28/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 29/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 30/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Channel 31/32 :    0
gpu109-23-r:58710:58804 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu109-23-r:58710:58804 [0] NCCL INFO P2P Chunksize set to 131072
gpu109-23-r:58710:58804 [0] NCCL INFO Connected all rings
gpu109-23-r:58710:58804 [0] NCCL INFO Connected all trees
gpu109-23-r:58710:58804 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu201-02-l:12282:12550 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 00/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 01/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 02/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 03/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 04/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 05/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 06/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 07/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 08/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 09/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 10/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 11/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 12/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 13/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 14/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 15/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 16/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 17/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 18/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 19/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 20/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 21/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 22/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 23/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 24/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 25/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 26/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 27/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 28/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 29/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 30/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Channel 31/32 :    0
gpu201-02-l:12282:12550 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu201-02-l:12282:12550 [0] NCCL INFO P2P Chunksize set to 131072
gpu201-02-l:12282:12550 [0] NCCL INFO Connected all rings
gpu201-02-l:12282:12550 [0] NCCL INFO Connected all trees
gpu201-02-l:12282:12550 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu201-02-r:15165:15295 [0] NCCL INFO comm 0x35258cc0 rank 0 nranks 1 cudaDev 0 busId 85000 commId 0x404e2ea0d00b6707 - Init COMPLETE
gpu109-16-r:24876:24986 [0] NCCL INFO comm 0x35e95060 rank 0 nranks 1 cudaDev 0 busId 46000 commId 0xfbca48b7ff6fe9a9 - Init COMPLETE
gpu108-23-r:1142:1311 [0] NCCL INFO comm 0x36d47580 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0xf17c3a31433ca3ee - Init COMPLETE
gpu109-16-l:53482:53591 [0] NCCL INFO comm 0x35fbf290 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0x92b77d59bb5ef3b0 - Init COMPLETE
gpu109-02-l:65178:65284 [0] NCCL INFO comm 0x34e9b3e0 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0x63c00907d52b255 - Init COMPLETE
gpu109-23-r:58710:58804 [0] NCCL INFO comm 0x37995680 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0x162d7e9748531720 - Init COMPLETE
gpu201-02-l:12282:12550 [0] NCCL INFO comm 0x34f573a0 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0x9bcce238938c6ed0 - Init COMPLETE
 > loading doc-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_train_indexmap_1600ns_1024sl_1234s_doc_idx.npy
NCCL version 2.17.1+cuda11.7
 > loading sample-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_train_indexmap_1600ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_train_indexmap_1600ns_1024sl_1234s_shuffle_idx.npy
gpu109-02-r:49201:49702 [0] NCCL INFO Using network IB
    loaded indexed file in 0.001 seconds
    total number of samples: 1544006
    total number of epochs: 1
gpu109-02-r:49201:49702 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu109-02-r:49201:49702 [0] NCCL INFO Setting affinity for GPU 0 to 3f00ff80
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 00/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 01/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 02/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 03/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 04/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 05/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 06/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 07/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 08/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 09/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 10/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 11/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 12/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 13/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 14/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 15/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 16/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 17/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 18/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 19/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 20/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 21/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 22/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 23/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 24/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 25/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 26/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 27/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 28/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 29/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 30/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Channel 31/32 :    0
gpu109-02-r:49201:49702 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu109-02-r:49201:49702 [0] NCCL INFO P2P Chunksize set to 131072
gpu109-02-r:49201:49702 [0] NCCL INFO Connected all rings
gpu109-02-r:49201:49702 [0] NCCL INFO Connected all trees
gpu109-02-r:49201:49702 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu109-02-r:49201:49702 [0] NCCL INFO comm 0x388743d0 rank 0 nranks 1 cudaDev 0 busId c7000 commId 0xee7cf342d5e09f70 - Init COMPLETE
 > loading doc-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_valid_indexmap_1280ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_valid_indexmap_1280ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /ibex/ai/home/shaima0d/KSL_Trainings/DeepSpeed_workshop/ibex_benchmarking/deepspeed_bench/Megatron-DeepSpeed/dataset//BookCorpusDataset_text_document_valid_indexmap_1280ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.001 seconds
    total number of samples: 31426
    total number of epochs: 1
> finished creating GPT datasets ...
gpu108-23-r:1142:1316 [0] NCCL INFO Using network IB
gpu109-16-l:53482:53595 [0] NCCL INFO Using network IB
gpu109-23-r:58710:58809 [0] NCCL INFO Using network IB
gpu109-02-l:65178:65303 [0] NCCL INFO Using network IB
gpu201-02-l:12282:12555 [0] NCCL INFO Using network IB
gpu109-16-r:24876:24990 [0] NCCL INFO Using network IB
gpu109-02-r:49201:49708 [0] NCCL INFO Using network IB
gpu201-02-r:15165:15300 [0] NCCL INFO Using network IB
gpu109-16-l:53482:53595 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu109-23-r:58710:58809 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu109-16-r:24876:24990 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu201-02-l:12282:12555 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu109-02-l:65178:65303 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:1142:1316 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 00/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 01/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 02/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 03/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 04/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 05/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 06/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 07/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 08/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 09/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 10/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 11/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 12/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 13/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 14/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 15/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 16/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 17/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 18/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 19/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 20/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 21/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 22/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 23/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 24/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 25/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 26/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 27/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 28/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 29/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 30/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Channel 31/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu109-16-l:53482:53595 [0] NCCL INFO P2P Chunksize set to 131072
gpu109-02-r:49201:49708 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu109-16-l:53482:53595 [0] NCCL INFO Connected all rings
gpu109-16-l:53482:53595 [0] NCCL INFO Connected all trees
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 00/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 01/32 :    0
gpu109-16-l:53482:53595 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 02/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 03/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 04/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 05/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 06/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 07/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 08/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 09/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 10/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 11/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 12/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 13/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 14/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 15/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 16/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 17/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 18/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 19/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 20/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 21/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 22/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 23/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 24/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 25/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 26/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 27/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 28/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 29/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 30/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Channel 31/32 :    0
gpu109-23-r:58710:58809 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu109-23-r:58710:58809 [0] NCCL INFO P2P Chunksize set to 131072
gpu201-02-r:15165:15300 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu109-23-r:58710:58809 [0] NCCL INFO Connected all rings
gpu109-23-r:58710:58809 [0] NCCL INFO Connected all trees
gpu109-23-r:58710:58809 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 00/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 01/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 02/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 03/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 04/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 05/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 06/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 07/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 08/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 09/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 10/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 11/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 12/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 13/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 14/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 15/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 16/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 17/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 18/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 19/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 20/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 21/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 22/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 23/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 24/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 25/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 26/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 27/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 28/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 29/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 30/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Channel 31/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-r:1142:1316 [0] NCCL INFO P2P Chunksize set to 131072
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 00/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 01/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 02/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 03/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 04/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 05/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 06/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 07/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 08/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 09/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 10/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 11/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 12/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 13/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 14/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 15/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 16/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 17/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 18/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 19/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 20/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 21/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 22/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 23/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 24/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 25/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 26/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 27/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 28/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 29/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 30/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Channel 31/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu201-02-l:12282:12555 [0] NCCL INFO P2P Chunksize set to 131072
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 00/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 01/32 :    0
gpu108-23-r:1142:1316 [0] NCCL INFO Connected all rings
gpu108-23-r:1142:1316 [0] NCCL INFO Connected all trees
gpu108-23-r:1142:1316 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 02/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 03/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 04/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 05/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 06/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 07/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 08/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 09/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 10/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 11/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 12/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 13/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 14/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 15/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 16/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 17/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 18/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 19/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 20/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 00/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 01/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 21/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 22/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 23/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 24/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 25/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 26/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 27/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 28/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 29/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 30/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Channel 31/32 :    0
gpu109-16-r:24876:24990 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu109-16-r:24876:24990 [0] NCCL INFO P2P Chunksize set to 131072
gpu201-02-l:12282:12555 [0] NCCL INFO Connected all rings
gpu201-02-l:12282:12555 [0] NCCL INFO Connected all trees
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 02/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 03/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 04/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 05/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 06/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 07/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 08/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 09/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 10/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 11/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 12/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 13/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 14/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 15/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 16/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 17/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 18/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 19/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 20/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 21/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 22/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 23/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 24/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 25/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 26/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 27/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 28/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 29/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 30/32 :    0
gpu109-02-l:65178:65303 [0] NCCL INFO Channel 31/32 :    0
gpu201-02-l:12282:12555 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu109-02-l:65178:65303 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu109-02-l:65178:65303 [0] NCCL INFO P2P Chunksize set to 131072
gpu109-16-r:24876:24990 [0] NCCL INFO Connected all rings
gpu109-16-r:24876:24990 [0] NCCL INFO Connected all trees
gpu109-16-r:24876:24990 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu109-02-l:65178:65303 [0] NCCL INFO Connected all rings
gpu109-02-l:65178:65303 [0] NCCL INFO Connected all trees
gpu109-02-l:65178:65303 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu109-02-r:49201:49708 [0] NCCL INFO Setting affinity for GPU 0 to 3f00ff80
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 00/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 01/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 02/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 03/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 04/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 05/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 06/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 07/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 08/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 09/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 10/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 11/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 12/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 13/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 14/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 15/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 16/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 17/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 18/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 19/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 20/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 21/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 22/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 23/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 24/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 25/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 26/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 27/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 28/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 29/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 30/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Channel 31/32 :    0
gpu109-02-r:49201:49708 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu109-02-r:49201:49708 [0] NCCL INFO P2P Chunksize set to 131072
gpu109-02-r:49201:49708 [0] NCCL INFO Connected all rings
gpu109-02-r:49201:49708 [0] NCCL INFO Connected all trees
gpu109-02-r:49201:49708 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 00/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 01/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 02/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 03/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 04/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 05/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 06/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 07/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 08/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 09/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 10/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 11/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 12/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 13/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 14/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 15/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 16/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 17/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 18/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 19/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 20/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 21/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 22/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 23/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 24/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 25/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 26/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 27/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 28/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 29/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 30/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Channel 31/32 :    0
gpu201-02-r:15165:15300 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu201-02-r:15165:15300 [0] NCCL INFO P2P Chunksize set to 131072
gpu201-02-r:15165:15300 [0] NCCL INFO Connected all rings
gpu201-02-r:15165:15300 [0] NCCL INFO Connected all trees
gpu201-02-r:15165:15300 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu109-16-l:53482:53595 [0] NCCL INFO comm 0x35fc2400 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0xdace91cbc8389d1d - Init COMPLETE
gpu109-16-r:24876:24990 [0] NCCL INFO comm 0x35e981d0 rank 0 nranks 1 cudaDev 0 busId 46000 commId 0xfcb9aee07c6686f0 - Init COMPLETE
gpu109-23-r:58710:58809 [0] NCCL INFO comm 0x379987f0 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0x2f9dc18f2a879c0f - Init COMPLETE
gpu108-23-r:1142:1316 [0] NCCL INFO comm 0x36d363d0 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0x395478ba68987db6 - Init COMPLETE
gpu109-02-r:49201:49708 [0] NCCL INFO comm 0x38877540 rank 0 nranks 1 cudaDev 0 busId c7000 commId 0xb4fdfcdc28f9644f - Init COMPLETE
gpu201-02-l:12282:12555 [0] NCCL INFO comm 0x34f5a510 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0xb53d4c1e50f527f8 - Init COMPLETE
gpu201-02-r:15165:15300 [0] NCCL INFO comm 0x3525be30 rank 0 nranks 1 cudaDev 0 busId 85000 commId 0x8a4d2a5963af609f - Init COMPLETE
gpu109-02-l:65178:65303 [0] NCCL INFO comm 0x34e9e550 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0x826ed31e4620fcfe - Init COMPLETE
[after dataloaders are built] datetime: 2023-03-17 12:15:35 
done with setup ...
training ...
time (ms) | model-and-optimizer-setup: 4901.30 | train/valid/test-data-iterators-setup: 777.62
[before the start of training step] datetime: 2023-03-17 12:15:35 
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
[2023-03-17 12:15:35,542] [INFO] [checkpointing.py:553:forward] Activation Checkpointing Information
[2023-03-17 12:15:35,542] [INFO] [checkpointing.py:554:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2023-03-17 12:15:35,542] [INFO] [checkpointing.py:557:forward] ----contiguous Memory Checkpointing False with 1 total layers
[2023-03-17 12:15:35,542] [INFO] [checkpointing.py:560:forward] ----Synchronization False
[2023-03-17 12:15:35,542] [INFO] [checkpointing.py:561:forward] ----Profiling time in checkpointing False
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
/sw/csgv/dl/apps/pytorch/1.13.1_cuda11.7.0/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
  warnings.warn(
gpu109-23-r:58710:59028 [0] NCCL INFO Using network IB
gpu109-16-r:24876:25205 [0] NCCL INFO Using network IB
gpu201-02-l:12282:12792 [0] NCCL INFO Using network IB
gpu109-16-l:53482:53809 [0] NCCL INFO Using network IB
gpu109-02-r:49201:49989 [0] NCCL INFO Using network IB
gpu108-23-r:1142:1530 [0] NCCL INFO Using network IB
gpu109-02-l:65178:424 [0] NCCL INFO Using network IB
gpu201-02-r:15165:15515 [0] NCCL INFO Using network IB
gpu109-23-r:58710:59028 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu201-02-r:15165:15515 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu201-02-l:12282:12792 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu109-16-r:24876:25205 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu108-23-r:1142:1530 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu109-16-l:53482:53809 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 00/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 01/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 02/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 03/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 04/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 05/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 06/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 07/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 08/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 09/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 10/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 11/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 12/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 13/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 14/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 15/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 16/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 17/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 18/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 19/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 20/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 21/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 22/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 23/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 24/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 25/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 26/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 27/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 28/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 29/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 30/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Channel 31/32 :    0
gpu109-23-r:58710:59028 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu109-23-r:58710:59028 [0] NCCL INFO P2P Chunksize set to 131072
gpu109-23-r:58710:59028 [0] NCCL INFO Connected all rings
gpu109-23-r:58710:59028 [0] NCCL INFO Connected all trees
gpu109-23-r:58710:59028 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 00/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 01/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 02/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 03/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 04/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 05/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 06/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 07/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 08/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 09/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 10/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 11/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 12/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 13/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 14/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 15/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 16/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 17/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 18/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 19/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 20/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 21/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 22/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 23/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 24/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 25/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 00/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 01/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 26/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 27/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 28/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 29/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 30/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Channel 31/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 02/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 03/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 04/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 05/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 06/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 07/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 08/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 09/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 10/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 11/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 12/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 13/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 14/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 15/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 16/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 17/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 18/32 :    0
gpu201-02-l:12282:12792 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu201-02-l:12282:12792 [0] NCCL INFO P2P Chunksize set to 131072
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 19/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 20/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 21/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 22/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 23/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 24/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 25/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 26/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 27/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 28/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 29/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 30/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Channel 31/32 :    0
gpu109-16-r:24876:25205 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu109-16-r:24876:25205 [0] NCCL INFO P2P Chunksize set to 131072
gpu109-02-r:49201:49989 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu201-02-l:12282:12792 [0] NCCL INFO Connected all rings
gpu201-02-l:12282:12792 [0] NCCL INFO Connected all trees
gpu201-02-l:12282:12792 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu109-16-r:24876:25205 [0] NCCL INFO Connected all rings
gpu109-16-r:24876:25205 [0] NCCL INFO Connected all trees
gpu109-16-r:24876:25205 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 00/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 01/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 02/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 00/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 01/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 02/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 03/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 04/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 05/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 06/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 07/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 08/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 09/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 10/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 11/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 12/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 13/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 14/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 15/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 16/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 17/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 18/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 19/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 20/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 21/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 22/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 23/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 24/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 25/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 26/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 27/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 28/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 29/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 30/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Channel 31/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 03/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 04/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 05/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 06/32 :    0
gpu108-23-r:1142:1530 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu108-23-r:1142:1530 [0] NCCL INFO P2P Chunksize set to 131072
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 07/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 08/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 09/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 10/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 11/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 12/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 13/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 14/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 15/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 16/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 17/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 18/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 19/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 20/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 21/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 22/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 23/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 24/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 25/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 26/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 27/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 28/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 29/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 30/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Channel 31/32 :    0
gpu109-16-l:53482:53809 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu109-16-l:53482:53809 [0] NCCL INFO P2P Chunksize set to 131072
gpu109-16-l:53482:53809 [0] NCCL INFO Connected all rings
gpu109-16-l:53482:53809 [0] NCCL INFO Connected all trees
gpu108-23-r:1142:1530 [0] NCCL INFO Connected all rings
gpu108-23-r:1142:1530 [0] NCCL INFO Connected all trees
gpu108-23-r:1142:1530 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu109-16-l:53482:53809 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 00/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 01/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 02/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 03/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 04/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 05/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 06/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 07/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 08/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 09/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 10/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 11/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 12/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 13/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 14/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 15/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 16/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 17/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 18/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 19/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 20/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 21/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Setting affinity for GPU 0 to 3f00ff80
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 22/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 23/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 24/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 25/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 26/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 27/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 28/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 29/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 30/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Channel 31/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu201-02-r:15165:15515 [0] NCCL INFO P2P Chunksize set to 131072
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 00/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 01/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 02/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 03/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 04/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 05/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 06/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 07/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 08/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 09/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 10/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 11/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 12/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 13/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 14/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 15/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 16/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 17/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 18/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 19/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 20/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 21/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 22/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 23/32 :    0
gpu201-02-r:15165:15515 [0] NCCL INFO Connected all rings
gpu201-02-r:15165:15515 [0] NCCL INFO Connected all trees
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 24/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 25/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 26/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 27/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 28/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 29/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 30/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Channel 31/32 :    0
gpu109-02-r:49201:49989 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu109-02-r:49201:49989 [0] NCCL INFO P2P Chunksize set to 131072
gpu201-02-r:15165:15515 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu109-02-r:49201:49989 [0] NCCL INFO Connected all rings
gpu109-02-r:49201:49989 [0] NCCL INFO Connected all trees
gpu109-02-r:49201:49989 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu109-02-l:65178:424 [0] NCCL INFO NCCL_TOPO_DUMP_FILE set by environment to ./nccl_dump.log.g4
gpu109-02-l:65178:424 [0] NCCL INFO Channel 00/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 01/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 02/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 03/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 04/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 05/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 06/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 07/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 08/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 09/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 10/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 11/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 12/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 13/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 14/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 15/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 16/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 17/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 18/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 19/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 20/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 21/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 22/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 23/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 24/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 25/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 26/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 27/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 28/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 29/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 30/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Channel 31/32 :    0
gpu109-02-l:65178:424 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
gpu109-02-l:65178:424 [0] NCCL INFO P2P Chunksize set to 131072
gpu109-02-l:65178:424 [0] NCCL INFO Connected all rings
gpu109-02-l:65178:424 [0] NCCL INFO Connected all trees
gpu109-02-l:65178:424 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
gpu109-23-r:58710:59028 [0] NCCL INFO comm 0x70bf86e0 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0xcc3d0819e827ac5e - Init COMPLETE
gpu108-23-r:1142:1530 [0] NCCL INFO comm 0x6dca4a00 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0x52ce3fa8cd88ab53 - Init COMPLETE
gpu201-02-r:15165:15515 [0] NCCL INFO comm 0x6e8a6130 rank 0 nranks 1 cudaDev 0 busId 85000 commId 0x84a59c0d63db975f - Init COMPLETE
gpu201-02-l:12282:12792 [0] NCCL INFO comm 0x6e24dff0 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0xfc353deb71e07684 - Init COMPLETE
gpu109-16-l:53482:53809 [0] NCCL INFO comm 0x6eb437a0 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0xb2d874f4ca9d14a7 - Init COMPLETE
gpu109-02-r:49201:49989 [0] NCCL INFO comm 0x71dd3950 rank 0 nranks 1 cudaDev 0 busId c7000 commId 0x2df5c76cb573c184 - Init COMPLETE
gpu109-16-r:24876:25205 [0] NCCL INFO comm 0x6ebf1bb0 rank 0 nranks 1 cudaDev 0 busId 46000 commId 0xf40d232dc4f5d5f5 - Init COMPLETE
gpu109-02-l:65178:424 [0] NCCL INFO comm 0x6f8ed010 rank 0 nranks 1 cudaDev 0 busId 7000 commId 0x820a48f9395cbdf2 - Init COMPLETE
[2023-03-17 12:15:36,881] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.91
[2023-03-17 12:15:36,881] [INFO] [logging.py:93:log_dist] [Rank 0] step=1, skipped=0, lr=[5.9946721667563326e-05, 5.9946721667563326e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:36,881] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 907.01 | backward_microstep: 280.43 | backward_inner_microstep: 191.89 | backward_allreduce_microstep: 88.03 | step_microstep: 149.21
[2023-03-17 12:15:36,882] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 906.93 | backward: 280.41 | backward_inner: 191.85 | backward_allreduce: 88.04 | step: 149.21
 iteration        1/      50 | consumed samples:          256 | consumed tokens:       262144 | elapsed time per iteration (ms): 1350.0 | learning rate: 5.995E-05 | global batch size:    32 | lm loss: 1.084315E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.704 | TFLOPs: 1.29 |
[Rank 0] (after 1 iterations) memory (MB) | allocated: 300.0537109375 | max allocated: 2629.6298828125 | reserved: 3816.0 | max reserved: 3816.0
time (ms) | forward-compute: 916.37 | backward-compute: 281.39 | backward-embedding-all-reduce: 0.01 | optimizer: 149.74 | batch-generator: 5.72
[2023-03-17 12:15:37,296] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:37,296] [INFO] [logging.py:93:log_dist] [Rank 0] step=2, skipped=0, lr=[5.97870969354909e-05, 5.97870969354909e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:37,297] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.60 | backward_microstep: 264.55 | backward_inner_microstep: 178.79 | backward_allreduce_microstep: 85.31 | step_microstep: 4.10
[2023-03-17 12:15:37,297] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.53 | backward: 264.53 | backward_inner: 178.74 | backward_allreduce: 85.33 | step: 4.10
 iteration        2/      50 | consumed samples:          512 | consumed tokens:       524288 | elapsed time per iteration (ms): 415.3 | learning rate: 5.979E-05 | global batch size:    32 | lm loss: 1.084111E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.046 | TFLOPs: 4.21 |
time (ms) | forward-compute: 142.47 | backward-compute: 266.44 | backward-embedding-all-reduce: 0.01 | optimizer: 4.37 | batch-generator: 4.55
[2023-03-17 12:15:37,710] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.64
[2023-03-17 12:15:37,710] [INFO] [logging.py:93:log_dist] [Rank 0] step=3, skipped=0, lr=[5.95217557696746e-05, 5.95217557696746e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:37,710] [INFO] [timer.py:198:stop] epoch=0/micro_step=3/global_step=3, RunningAvgSamplesPerSec=600.9650349024121, CurrSamplesPerSec=600.9650349024121, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:37,711] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.36 | backward_microstep: 265.65 | backward_inner_microstep: 178.45 | backward_allreduce_microstep: 86.74 | step_microstep: 4.16
[2023-03-17 12:15:37,711] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.29 | backward: 265.63 | backward_inner: 178.40 | backward_allreduce: 86.76 | step: 4.16
 iteration        3/      50 | consumed samples:          768 | consumed tokens:       786432 | elapsed time per iteration (ms): 413.7 | learning rate: 5.952E-05 | global batch size:    32 | lm loss: 1.083868E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.351 | TFLOPs: 4.22 |
time (ms) | forward-compute: 140.83 | backward-compute: 266.95 | backward-embedding-all-reduce: 0.01 | optimizer: 4.30 | batch-generator: 4.61
[2023-03-17 12:15:38,124] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:38,124] [INFO] [logging.py:93:log_dist] [Rank 0] step=4, skipped=0, lr=[5.9151745350473036e-05, 5.9151745350473036e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:38,124] [INFO] [timer.py:198:stop] epoch=0/micro_step=4/global_step=4, RunningAvgSamplesPerSec=592.5060445733483, CurrSamplesPerSec=584.2818809476131, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:38,124] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.15 | backward_microstep: 265.42 | backward_inner_microstep: 178.39 | backward_allreduce_microstep: 86.59 | step_microstep: 4.01
[2023-03-17 12:15:38,125] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.08 | backward: 265.39 | backward_inner: 178.33 | backward_allreduce: 86.60 | step: 4.01
 iteration        4/      50 | consumed samples:         1024 | consumed tokens:      1048576 | elapsed time per iteration (ms): 413.7 | learning rate: 5.915E-05 | global batch size:    32 | lm loss: 1.083617E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.355 | TFLOPs: 4.22 |
time (ms) | forward-compute: 141.14 | backward-compute: 266.54 | backward-embedding-all-reduce: 0.01 | optimizer: 4.38 | batch-generator: 4.44
[2023-03-17 12:15:38,538] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:38,538] [INFO] [logging.py:93:log_dist] [Rank 0] step=5, skipped=0, lr=[5.8678525939969144e-05, 5.8678525939969144e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:38,539] [INFO] [timer.py:198:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=590.9672677318132, CurrSamplesPerSec=587.9135679712653, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:38,539] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.81 | backward_microstep: 264.52 | backward_inner_microstep: 178.63 | backward_allreduce_microstep: 85.42 | step_microstep: 4.91
[2023-03-17 12:15:38,539] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.73 | backward: 264.50 | backward_inner: 178.58 | backward_allreduce: 85.44 | step: 4.91
 iteration        5/      50 | consumed samples:         1280 | consumed tokens:      1310720 | elapsed time per iteration (ms): 414.6 | learning rate: 5.868E-05 | global batch size:    32 | lm loss: 1.083390E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.186 | TFLOPs: 4.21 |
time (ms) | forward-compute: 142.12 | backward-compute: 265.69 | backward-embedding-all-reduce: 0.01 | optimizer: 5.15 | batch-generator: 4.53
[2023-03-17 12:15:38,951] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.66
[2023-03-17 12:15:38,951] [INFO] [logging.py:93:log_dist] [Rank 0] step=6, skipped=0, lr=[5.810396511898279e-05, 5.810396511898279e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:38,952] [INFO] [timer.py:198:stop] epoch=0/micro_step=6/global_step=6, RunningAvgSamplesPerSec=595.8208290004783, CurrSamplesPerSec=610.8719386477937, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:38,952] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.84 | backward_microstep: 263.58 | backward_inner_microstep: 178.33 | backward_allreduce_microstep: 84.80 | step_microstep: 4.14
[2023-03-17 12:15:38,952] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.77 | backward: 263.55 | backward_inner: 178.28 | backward_allreduce: 84.82 | step: 4.14
 iteration        6/      50 | consumed samples:         1536 | consumed tokens:      1572864 | elapsed time per iteration (ms): 412.8 | learning rate: 5.810E-05 | global batch size:    32 | lm loss: 1.083029E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.516 | TFLOPs: 4.23 |
time (ms) | forward-compute: 142.18 | backward-compute: 264.71 | backward-embedding-all-reduce: 0.01 | optimizer: 4.35 | batch-generator: 4.47
[2023-03-17 12:15:39,364] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:39,364] [INFO] [logging.py:93:log_dist] [Rank 0] step=7, skipped=0, lr=[5.743033041658253e-05, 5.743033041658253e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:39,365] [INFO] [timer.py:198:stop] epoch=0/micro_step=7/global_step=7, RunningAvgSamplesPerSec=597.3680467220754, CurrSamplesPerSec=603.6381168258765, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:39,365] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.42 | backward_microstep: 264.66 | backward_inner_microstep: 178.63 | backward_allreduce_microstep: 85.60 | step_microstep: 4.11
[2023-03-17 12:15:39,365] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.35 | backward: 264.64 | backward_inner: 178.58 | backward_allreduce: 85.62 | step: 4.11
 iteration        7/      50 | consumed samples:         1792 | consumed tokens:      1835008 | elapsed time per iteration (ms): 413.1 | learning rate: 5.743E-05 | global batch size:    32 | lm loss: 1.082633E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.467 | TFLOPs: 4.23 |
time (ms) | forward-compute: 141.11 | backward-compute: 265.99 | backward-embedding-all-reduce: 0.01 | optimizer: 4.40 | batch-generator: 4.47
[2023-03-17 12:15:39,777] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:39,777] [INFO] [logging.py:93:log_dist] [Rank 0] step=8, skipped=0, lr=[5.666028036118432e-05, 5.666028036118432e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:39,777] [INFO] [timer.py:198:stop] epoch=0/micro_step=8/global_step=8, RunningAvgSamplesPerSec=597.6614485809642, CurrSamplesPerSec=599.1327917150254, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:39,777] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 131.85 | backward_microstep: 264.56 | backward_inner_microstep: 178.46 | backward_allreduce_microstep: 85.65 | step_microstep: 4.50
[2023-03-17 12:15:39,778] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 131.77 | backward: 264.53 | backward_inner: 178.42 | backward_allreduce: 85.68 | step: 4.50
 iteration        8/      50 | consumed samples:         2048 | consumed tokens:      2097152 | elapsed time per iteration (ms): 412.5 | learning rate: 5.666E-05 | global batch size:    32 | lm loss: 1.082305E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.583 | TFLOPs: 4.24 |
time (ms) | forward-compute: 140.37 | backward-compute: 265.89 | backward-embedding-all-reduce: 0.01 | optimizer: 4.65 | batch-generator: 4.48
[2023-03-17 12:15:40,190] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.64
[2023-03-17 12:15:40,190] [INFO] [logging.py:93:log_dist] [Rank 0] step=9, skipped=0, lr=[5.579685398855441e-05, 5.579685398855441e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:40,190] [INFO] [timer.py:198:stop] epoch=0/micro_step=9/global_step=9, RunningAvgSamplesPerSec=597.5658456786825, CurrSamplesPerSec=596.9928699465802, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:40,191] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.64 | backward_microstep: 265.20 | backward_inner_microstep: 178.39 | backward_allreduce_microstep: 86.35 | step_microstep: 4.11
[2023-03-17 12:15:40,191] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.56 | backward: 265.17 | backward_inner: 178.34 | backward_allreduce: 86.39 | step: 4.12
 iteration        9/      50 | consumed samples:         2304 | consumed tokens:      2359296 | elapsed time per iteration (ms): 413.2 | learning rate: 5.580E-05 | global batch size:    32 | lm loss: 1.081878E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.452 | TFLOPs: 4.23 |
time (ms) | forward-compute: 140.88 | backward-compute: 266.31 | backward-embedding-all-reduce: 0.01 | optimizer: 4.31 | batch-generator: 4.51
[2023-03-17 12:15:40,606] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:40,607] [INFO] [logging.py:93:log_dist] [Rank 0] step=10, skipped=0, lr=[5.4843458848123576e-05, 5.4843458848123576e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:40,607] [INFO] [timer.py:198:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=597.8428074856336, CurrSamplesPerSec=599.788752094738, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:40,607] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.00 | backward_microstep: 265.89 | backward_inner_microstep: 178.36 | backward_allreduce_microstep: 87.09 | step_microstep: 4.01
[2023-03-17 12:15:40,608] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 131.92 | backward: 265.87 | backward_inner: 178.31 | backward_allreduce: 87.14 | step: 4.01
 iteration       10/      50 | consumed samples:         2560 | consumed tokens:      2621440 | elapsed time per iteration (ms): 416.7 | learning rate: 5.484E-05 | global batch size:    32 | lm loss: 1.081315E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 76.795 | TFLOPs: 4.19 |
time (ms) | forward-compute: 143.70 | backward-compute: 267.11 | backward-embedding-all-reduce: 0.01 | optimizer: 4.27 | batch-generator: 4.41
[2023-03-17 12:15:41,019] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:41,020] [INFO] [logging.py:93:log_dist] [Rank 0] step=11, skipped=0, lr=[5.380385755494631e-05, 5.380385755494631e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:41,020] [INFO] [timer.py:198:stop] epoch=0/micro_step=11/global_step=11, RunningAvgSamplesPerSec=597.6089656219868, CurrSamplesPerSec=595.7447956891883, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:41,020] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.10 | backward_microstep: 265.31 | backward_inner_microstep: 178.40 | backward_allreduce_microstep: 86.45 | step_microstep: 4.10
[2023-03-17 12:15:41,021] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.02 | backward: 265.28 | backward_inner: 178.35 | backward_allreduce: 86.48 | step: 4.10
 iteration       11/      50 | consumed samples:         2816 | consumed tokens:      2883584 | elapsed time per iteration (ms): 412.9 | learning rate: 5.380E-05 | global batch size:    32 | lm loss: 1.080804E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.496 | TFLOPs: 4.23 |
time (ms) | forward-compute: 140.68 | backward-compute: 266.40 | backward-embedding-all-reduce: 0.01 | optimizer: 4.24 | batch-generator: 4.53
[2023-03-17 12:15:41,431] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:41,431] [INFO] [logging.py:93:log_dist] [Rank 0] step=12, skipped=0, lr=[5.2682152940378117e-05, 5.2682152940378117e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:41,431] [INFO] [timer.py:198:stop] epoch=0/micro_step=12/global_step=12, RunningAvgSamplesPerSec=597.795876554977, CurrSamplesPerSec=599.4833511248878, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:41,432] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 131.91 | backward_microstep: 263.68 | backward_inner_microstep: 178.28 | backward_allreduce_microstep: 84.95 | step_microstep: 4.26
[2023-03-17 12:15:41,432] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 131.84 | backward: 263.64 | backward_inner: 178.23 | backward_allreduce: 84.98 | step: 4.26
 iteration       12/      50 | consumed samples:         3072 | consumed tokens:      3145728 | elapsed time per iteration (ms): 411.4 | learning rate: 5.268E-05 | global batch size:    32 | lm loss: 1.080132E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.778 | TFLOPs: 4.25 |
time (ms) | forward-compute: 140.42 | backward-compute: 264.85 | backward-embedding-all-reduce: 0.01 | optimizer: 4.51 | batch-generator: 4.48
[2023-03-17 12:15:41,842] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:41,843] [INFO] [logging.py:93:log_dist] [Rank 0] step=13, skipped=0, lr=[5.14827718600746e-05, 5.14827718600746e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:41,843] [INFO] [timer.py:198:stop] epoch=0/micro_step=13/global_step=13, RunningAvgSamplesPerSec=597.9576090201019, CurrSamplesPerSec=599.5797599317409, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:41,843] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.07 | backward_microstep: 263.61 | backward_inner_microstep: 178.31 | backward_allreduce_microstep: 84.87 | step_microstep: 4.01
[2023-03-17 12:15:41,844] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.00 | backward: 263.58 | backward_inner: 178.26 | backward_allreduce: 84.90 | step: 4.02
 iteration       13/      50 | consumed samples:         3328 | consumed tokens:      3407872 | elapsed time per iteration (ms): 411.6 | learning rate: 5.148E-05 | global batch size:    32 | lm loss: 1.079512E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.751 | TFLOPs: 4.24 |
time (ms) | forward-compute: 140.85 | backward-compute: 264.84 | backward-embedding-all-reduce: 0.01 | optimizer: 4.25 | batch-generator: 4.65
[2023-03-17 12:15:42,256] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.64
[2023-03-17 12:15:42,256] [INFO] [logging.py:93:log_dist] [Rank 0] step=14, skipped=0, lr=[5.021044772321462e-05, 5.021044772321462e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:42,256] [INFO] [timer.py:198:stop] epoch=0/micro_step=14/global_step=14, RunningAvgSamplesPerSec=597.617539672645, CurrSamplesPerSec=593.9021474116455, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:42,257] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.95 | backward_microstep: 264.06 | backward_inner_microstep: 178.42 | backward_allreduce_microstep: 85.19 | step_microstep: 4.57
[2023-03-17 12:15:42,257] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.88 | backward: 264.04 | backward_inner: 178.37 | backward_allreduce: 85.21 | step: 4.57
 iteration       14/      50 | consumed samples:         3584 | consumed tokens:      3670016 | elapsed time per iteration (ms): 413.5 | learning rate: 5.021E-05 | global batch size:    32 | lm loss: 1.078673E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.388 | TFLOPs: 4.22 |
time (ms) | forward-compute: 141.92 | backward-compute: 265.28 | backward-embedding-all-reduce: 0.01 | optimizer: 4.72 | batch-generator: 4.54
[2023-03-17 12:15:42,671] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:42,671] [INFO] [logging.py:93:log_dist] [Rank 0] step=15, skipped=0, lr=[4.887020181189677e-05, 4.887020181189677e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:42,672] [INFO] [timer.py:198:stop] epoch=0/micro_step=15/global_step=15, RunningAvgSamplesPerSec=596.6019666185008, CurrSamplesPerSec=584.6789395272655, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:42,672] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.12 | backward_microstep: 263.37 | backward_inner_microstep: 178.20 | backward_allreduce_microstep: 84.73 | step_microstep: 4.04
[2023-03-17 12:15:42,672] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.04 | backward: 263.35 | backward_inner: 178.15 | backward_allreduce: 84.75 | step: 4.04
 iteration       15/      50 | consumed samples:         3840 | consumed tokens:      3932160 | elapsed time per iteration (ms): 415.2 | learning rate: 4.887E-05 | global batch size:    32 | lm loss: 1.077656E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.078 | TFLOPs: 4.21 |
time (ms) | forward-compute: 144.66 | backward-compute: 264.52 | backward-embedding-all-reduce: 0.01 | optimizer: 4.36 | batch-generator: 4.56
[2023-03-17 12:15:43,084] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.64
[2023-03-17 12:15:43,084] [INFO] [logging.py:93:log_dist] [Rank 0] step=16, skipped=0, lr=[4.74673234644329e-05, 4.74673234644329e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:43,084] [INFO] [timer.py:198:stop] epoch=0/micro_step=16/global_step=16, RunningAvgSamplesPerSec=597.1666589334667, CurrSamplesPerSec=604.6061479692962, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:43,085] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.23 | backward_microstep: 263.93 | backward_inner_microstep: 178.37 | backward_allreduce_microstep: 85.11 | step_microstep: 4.03
[2023-03-17 12:15:43,085] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.15 | backward: 263.90 | backward_inner: 178.32 | backward_allreduce: 85.13 | step: 4.03
 iteration       16/      50 | consumed samples:         4096 | consumed tokens:      4194304 | elapsed time per iteration (ms): 412.7 | learning rate: 4.747E-05 | global batch size:    32 | lm loss: 1.076603E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.537 | TFLOPs: 4.23 |
time (ms) | forward-compute: 141.56 | backward-compute: 265.22 | backward-embedding-all-reduce: 0.01 | optimizer: 4.32 | batch-generator: 4.72
[2023-03-17 12:15:43,495] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:43,495] [INFO] [logging.py:93:log_dist] [Rank 0] step=17, skipped=0, lr=[4.6007349200746303e-05, 4.6007349200746303e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:43,495] [INFO] [timer.py:198:stop] epoch=0/micro_step=17/global_step=17, RunningAvgSamplesPerSec=597.5604233956301, CurrSamplesPerSec=603.1281590394364, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:43,496] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 131.71 | backward_microstep: 263.49 | backward_inner_microstep: 178.46 | backward_allreduce_microstep: 84.59 | step_microstep: 4.10
[2023-03-17 12:15:43,496] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 131.64 | backward: 263.46 | backward_inner: 178.41 | backward_allreduce: 84.62 | step: 4.11
 iteration       17/      50 | consumed samples:         4352 | consumed tokens:      4456448 | elapsed time per iteration (ms): 411.1 | learning rate: 4.601E-05 | global batch size:    32 | lm loss: 1.075349E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.840 | TFLOPs: 4.25 |
time (ms) | forward-compute: 140.56 | backward-compute: 264.61 | backward-embedding-all-reduce: 0.01 | optimizer: 4.45 | batch-generator: 4.29
[2023-03-17 12:15:43,906] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.66
[2023-03-17 12:15:43,906] [INFO] [logging.py:93:log_dist] [Rank 0] step=18, skipped=0, lr=[4.4496040872256956e-05, 4.4496040872256956e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:43,907] [INFO] [timer.py:198:stop] epoch=0/micro_step=18/global_step=18, RunningAvgSamplesPerSec=597.7321488265572, CurrSamplesPerSec=600.3199255737397, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:43,907] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.48 | backward_microstep: 264.01 | backward_inner_microstep: 178.43 | backward_allreduce_microstep: 85.14 | step_microstep: 3.96
[2023-03-17 12:15:43,907] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.40 | backward: 263.99 | backward_inner: 178.37 | backward_allreduce: 85.17 | step: 3.97
 iteration       18/      50 | consumed samples:         4608 | consumed tokens:      4718592 | elapsed time per iteration (ms): 411.2 | learning rate: 4.450E-05 | global batch size:    32 | lm loss: 1.074199E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.814 | TFLOPs: 4.25 |
time (ms) | forward-compute: 140.59 | backward-compute: 264.98 | backward-embedding-all-reduce: 0.01 | optimizer: 4.18 | batch-generator: 4.32
[2023-03-17 12:15:44,318] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:44,319] [INFO] [logging.py:93:log_dist] [Rank 0] step=19, skipped=0, lr=[4.293936292248631e-05, 4.293936292248631e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:44,319] [INFO] [timer.py:198:stop] epoch=0/micro_step=19/global_step=19, RunningAvgSamplesPerSec=597.6806265099501, CurrSamplesPerSec=596.857475741971, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:44,319] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.35 | backward_microstep: 265.04 | backward_inner_microstep: 178.38 | backward_allreduce_microstep: 86.22 | step_microstep: 3.98
[2023-03-17 12:15:44,320] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.27 | backward: 265.02 | backward_inner: 178.33 | backward_allreduce: 86.25 | step: 3.99
 iteration       19/      50 | consumed samples:         4864 | consumed tokens:      4980736 | elapsed time per iteration (ms): 412.1 | learning rate: 4.294E-05 | global batch size:    32 | lm loss: 1.072808E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.645 | TFLOPs: 4.24 |
time (ms) | forward-compute: 140.25 | backward-compute: 266.21 | backward-embedding-all-reduce: 0.01 | optimizer: 4.16 | batch-generator: 4.40
[2023-03-17 12:15:44,734] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:44,734] [INFO] [logging.py:93:log_dist] [Rank 0] step=20, skipped=0, lr=[4.134345884812357e-05, 4.134345884812357e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:44,734] [INFO] [timer.py:198:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=596.993017468314, CurrSamplesPerSec=585.5410871651688, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:44,734] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 131.77 | backward_microstep: 264.51 | backward_inner_microstep: 178.40 | backward_allreduce_microstep: 85.68 | step_microstep: 5.67
[2023-03-17 12:15:44,735] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 131.69 | backward: 264.49 | backward_inner: 178.35 | backward_allreduce: 85.69 | step: 5.67
 iteration       20/      50 | consumed samples:         5120 | consumed tokens:      5242880 | elapsed time per iteration (ms): 415.2 | learning rate: 4.134E-05 | global batch size:    32 | lm loss: 1.071156E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.078 | TFLOPs: 4.21 |
time (ms) | forward-compute: 142.40 | backward-compute: 265.40 | backward-embedding-all-reduce: 0.01 | optimizer: 5.89 | batch-generator: 4.26
[2023-03-17 12:15:45,145] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.64
[2023-03-17 12:15:45,145] [INFO] [logging.py:93:log_dist] [Rank 0] step=21, skipped=0, lr=[3.971462695345109e-05, 3.971462695345109e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:45,146] [INFO] [timer.py:198:stop] epoch=0/micro_step=21/global_step=21, RunningAvgSamplesPerSec=597.1374140841468, CurrSamplesPerSec=599.7485499798919, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:45,146] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 131.99 | backward_microstep: 264.35 | backward_inner_microstep: 178.52 | backward_allreduce_microstep: 85.36 | step_microstep: 4.30
[2023-03-17 12:15:45,146] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 131.91 | backward: 264.33 | backward_inner: 178.48 | backward_allreduce: 85.37 | step: 4.31
 iteration       21/      50 | consumed samples:         5376 | consumed tokens:      5505024 | elapsed time per iteration (ms): 411.5 | learning rate: 3.971E-05 | global batch size:    32 | lm loss: 1.069096E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.763 | TFLOPs: 4.25 |
time (ms) | forward-compute: 140.24 | backward-compute: 264.95 | backward-embedding-all-reduce: 0.01 | optimizer: 4.76 | batch-generator: 4.37
[2023-03-17 12:15:45,556] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.64
[2023-03-17 12:15:45,556] [INFO] [logging.py:93:log_dist] [Rank 0] step=22, skipped=0, lr=[3.805929549381457e-05, 3.805929549381457e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:45,557] [INFO] [timer.py:198:stop] epoch=0/micro_step=22/global_step=22, RunningAvgSamplesPerSec=597.282182114963, CurrSamplesPerSec=600.0461733108607, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:45,557] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.60 | backward_microstep: 264.16 | backward_inner_microstep: 178.75 | backward_allreduce_microstep: 84.96 | step_microstep: 4.06
[2023-03-17 12:15:45,557] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.53 | backward: 264.14 | backward_inner: 178.71 | backward_allreduce: 84.98 | step: 4.07
 iteration       22/      50 | consumed samples:         5632 | consumed tokens:      5767168 | elapsed time per iteration (ms): 411.1 | learning rate: 3.806E-05 | global batch size:    32 | lm loss: 1.067315E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.844 | TFLOPs: 4.25 |
time (ms) | forward-compute: 140.26 | backward-compute: 265.01 | backward-embedding-all-reduce: 0.01 | optimizer: 4.32 | batch-generator: 4.44
[2023-03-17 12:15:45,968] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:45,968] [INFO] [logging.py:93:log_dist] [Rank 0] step=23, skipped=0, lr=[3.638399730623622e-05, 3.638399730623622e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:45,969] [INFO] [timer.py:198:stop] epoch=0/micro_step=23/global_step=23, RunningAvgSamplesPerSec=597.3568802684636, CurrSamplesPerSec=598.8547768199746, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:45,969] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 131.89 | backward_microstep: 265.04 | backward_inner_microstep: 178.39 | backward_allreduce_microstep: 86.20 | step_microstep: 3.94
[2023-03-17 12:15:45,969] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 131.81 | backward: 265.02 | backward_inner: 178.34 | backward_allreduce: 86.22 | step: 3.94
 iteration       23/      50 | consumed samples:         5888 | consumed tokens:      6029312 | elapsed time per iteration (ms): 412.0 | learning rate: 3.638E-05 | global batch size:    32 | lm loss: 1.065398E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.671 | TFLOPs: 4.24 |
time (ms) | forward-compute: 140.17 | backward-compute: 266.11 | backward-embedding-all-reduce: 0.01 | optimizer: 4.23 | batch-generator: 4.29
[2023-03-17 12:15:46,380] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:46,380] [INFO] [logging.py:93:log_dist] [Rank 0] step=24, skipped=0, lr=[3.469534402729146e-05, 3.469534402729146e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:46,381] [INFO] [timer.py:198:stop] epoch=0/micro_step=24/global_step=24, RunningAvgSamplesPerSec=597.8141518118621, CurrSamplesPerSec=607.5812136438741, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:46,381] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.70 | backward_microstep: 264.96 | backward_inner_microstep: 178.50 | backward_allreduce_microstep: 86.01 | step_microstep: 4.00
[2023-03-17 12:15:46,381] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.62 | backward: 264.93 | backward_inner: 178.45 | backward_allreduce: 86.05 | step: 4.00
 iteration       24/      50 | consumed samples:         6144 | consumed tokens:      6291456 | elapsed time per iteration (ms): 411.9 | learning rate: 3.470E-05 | global batch size:    32 | lm loss: 1.062967E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.694 | TFLOPs: 4.24 |
time (ms) | forward-compute: 140.29 | backward-compute: 265.87 | backward-embedding-all-reduce: 0.01 | optimizer: 4.23 | batch-generator: 4.27
[2023-03-17 12:15:46,792] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.66
[2023-03-17 12:15:46,793] [INFO] [logging.py:93:log_dist] [Rank 0] step=25, skipped=0, lr=[3.3e-05, 3.3e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:46,793] [INFO] [timer.py:198:stop] epoch=0/micro_step=25/global_step=25, RunningAvgSamplesPerSec=597.8405642423295, CurrSamplesPerSec=598.4222287614921, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:46,793] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.31 | backward_microstep: 264.78 | backward_inner_microstep: 178.42 | backward_allreduce_microstep: 85.91 | step_microstep: 3.84
[2023-03-17 12:15:46,794] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.24 | backward: 264.76 | backward_inner: 178.37 | backward_allreduce: 85.94 | step: 3.84
 iteration       25/      50 | consumed samples:         6400 | consumed tokens:      6553600 | elapsed time per iteration (ms): 412.3 | learning rate: 3.300E-05 | global batch size:    32 | lm loss: 1.060745E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.619 | TFLOPs: 4.24 |
time (ms) | forward-compute: 141.07 | backward-compute: 265.62 | backward-embedding-all-reduce: 0.01 | optimizer: 4.12 | batch-generator: 4.27
[2023-03-17 12:15:47,206] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.64
[2023-03-17 12:15:47,206] [INFO] [logging.py:93:log_dist] [Rank 0] step=26, skipped=0, lr=[3.1304655972708536e-05, 3.1304655972708536e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:47,206] [INFO] [timer.py:198:stop] epoch=0/micro_step=26/global_step=26, RunningAvgSamplesPerSec=597.8968482034837, CurrSamplesPerSec=599.194310638089, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:47,207] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.42 | backward_microstep: 266.56 | backward_inner_microstep: 178.20 | backward_allreduce_microstep: 87.92 | step_microstep: 3.94
[2023-03-17 12:15:47,207] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.35 | backward: 266.54 | backward_inner: 178.15 | backward_allreduce: 87.94 | step: 3.94
 iteration       26/      50 | consumed samples:         6656 | consumed tokens:      6815744 | elapsed time per iteration (ms): 413.5 | learning rate: 3.130E-05 | global batch size:    32 | lm loss: 1.058197E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.394 | TFLOPs: 4.23 |
time (ms) | forward-compute: 140.58 | backward-compute: 267.30 | backward-embedding-all-reduce: 0.01 | optimizer: 4.14 | batch-generator: 4.24
[2023-03-17 12:15:47,617] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.64
[2023-03-17 12:15:47,617] [INFO] [logging.py:93:log_dist] [Rank 0] step=27, skipped=0, lr=[2.961600269376378e-05, 2.961600269376378e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:47,617] [INFO] [timer.py:198:stop] epoch=0/micro_step=27/global_step=27, RunningAvgSamplesPerSec=597.8080179785499, CurrSamplesPerSec=595.6839830106028, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:47,618] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.28 | backward_microstep: 264.71 | backward_inner_microstep: 179.10 | backward_allreduce_microstep: 85.16 | step_microstep: 3.86
[2023-03-17 12:15:47,618] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.21 | backward: 264.69 | backward_inner: 179.06 | backward_allreduce: 85.17 | step: 3.86
 iteration       27/      50 | consumed samples:         6912 | consumed tokens:      7077888 | elapsed time per iteration (ms): 410.9 | learning rate: 2.962E-05 | global batch size:    32 | lm loss: 1.055830E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.876 | TFLOPs: 4.25 |
time (ms) | forward-compute: 139.56 | backward-compute: 265.73 | backward-embedding-all-reduce: 0.01 | optimizer: 4.15 | batch-generator: 4.40
[2023-03-17 12:15:48,028] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:48,028] [INFO] [logging.py:93:log_dist] [Rank 0] step=28, skipped=0, lr=[2.7940704506185428e-05, 2.7940704506185428e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:48,029] [INFO] [timer.py:198:stop] epoch=0/micro_step=28/global_step=28, RunningAvgSamplesPerSec=598.1763495801131, CurrSamplesPerSec=607.5344601261984, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:48,029] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.30 | backward_microstep: 264.64 | backward_inner_microstep: 178.41 | backward_allreduce_microstep: 85.77 | step_microstep: 4.03
[2023-03-17 12:15:48,029] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.23 | backward: 264.61 | backward_inner: 178.35 | backward_allreduce: 85.79 | step: 4.03
 iteration       28/      50 | consumed samples:         7168 | consumed tokens:      7340032 | elapsed time per iteration (ms): 411.3 | learning rate: 2.794E-05 | global batch size:    32 | lm loss: 1.052796E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.795 | TFLOPs: 4.25 |
time (ms) | forward-compute: 139.99 | backward-compute: 265.68 | backward-embedding-all-reduce: 0.01 | optimizer: 4.22 | batch-generator: 4.28
[2023-03-17 12:15:48,440] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:48,440] [INFO] [logging.py:93:log_dist] [Rank 0] step=29, skipped=0, lr=[2.6285373046548923e-05, 2.6285373046548923e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:48,440] [INFO] [timer.py:198:stop] epoch=0/micro_step=29/global_step=29, RunningAvgSamplesPerSec=598.0967962294874, CurrSamplesPerSec=596.0358107147932, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:48,441] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.02 | backward_microstep: 264.73 | backward_inner_microstep: 178.43 | backward_allreduce_microstep: 85.84 | step_microstep: 3.86
[2023-03-17 12:15:48,441] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 131.95 | backward: 264.70 | backward_inner: 178.38 | backward_allreduce: 85.86 | step: 3.86
 iteration       29/      50 | consumed samples:         7424 | consumed tokens:      7602176 | elapsed time per iteration (ms): 411.9 | learning rate: 2.629E-05 | global batch size:    32 | lm loss: 1.049798E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.694 | TFLOPs: 4.24 |
time (ms) | forward-compute: 140.35 | backward-compute: 265.89 | backward-embedding-all-reduce: 0.01 | optimizer: 4.16 | batch-generator: 4.25
[2023-03-17 12:15:48,854] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.64
[2023-03-17 12:15:48,854] [INFO] [logging.py:93:log_dist] [Rank 0] step=30, skipped=0, lr=[2.465654115187642e-05, 2.465654115187642e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:48,854] [INFO] [timer.py:198:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=598.4220381814679, CurrSamplesPerSec=607.3392731896486, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:48,855] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.33 | backward_microstep: 265.81 | backward_inner_microstep: 178.29 | backward_allreduce_microstep: 87.07 | step_microstep: 4.06
[2023-03-17 12:15:48,855] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.26 | backward: 265.78 | backward_inner: 178.24 | backward_allreduce: 87.11 | step: 4.06
 iteration       30/      50 | consumed samples:         7680 | consumed tokens:      7864320 | elapsed time per iteration (ms): 413.7 | learning rate: 2.466E-05 | global batch size:    32 | lm loss: 1.046920E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.352 | TFLOPs: 4.22 |
time (ms) | forward-compute: 141.49 | backward-compute: 266.40 | backward-embedding-all-reduce: 0.01 | optimizer: 4.29 | batch-generator: 4.40
[2023-03-17 12:15:49,266] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:49,266] [INFO] [logging.py:93:log_dist] [Rank 0] step=31, skipped=0, lr=[2.3060637077513695e-05, 2.3060637077513695e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:49,267] [INFO] [timer.py:198:stop] epoch=0/micro_step=31/global_step=31, RunningAvgSamplesPerSec=598.4633574559795, CurrSamplesPerSec=599.6226182445262, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:49,267] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 131.82 | backward_microstep: 266.50 | backward_inner_microstep: 178.25 | backward_allreduce_microstep: 87.81 | step_microstep: 4.07
[2023-03-17 12:15:49,267] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 131.75 | backward: 266.48 | backward_inner: 178.19 | backward_allreduce: 87.83 | step: 4.08
 iteration       31/      50 | consumed samples:         7936 | consumed tokens:      8126464 | elapsed time per iteration (ms): 412.5 | learning rate: 2.306E-05 | global batch size:    32 | lm loss: 1.043683E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.577 | TFLOPs: 4.24 |
time (ms) | forward-compute: 139.73 | backward-compute: 267.02 | backward-embedding-all-reduce: 0.01 | optimizer: 4.26 | batch-generator: 4.15
[2023-03-17 12:15:49,677] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:49,677] [INFO] [logging.py:93:log_dist] [Rank 0] step=32, skipped=0, lr=[2.150395912774304e-05, 2.150395912774304e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:49,678] [INFO] [timer.py:198:stop] epoch=0/micro_step=32/global_step=32, RunningAvgSamplesPerSec=598.6880916951252, CurrSamplesPerSec=605.2796139710027, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:49,678] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.12 | backward_microstep: 264.81 | backward_inner_microstep: 178.40 | backward_allreduce_microstep: 85.96 | step_microstep: 4.02
[2023-03-17 12:15:49,678] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.05 | backward: 264.78 | backward_inner: 178.35 | backward_allreduce: 85.97 | step: 4.03
 iteration       32/      50 | consumed samples:         8192 | consumed tokens:      8388608 | elapsed time per iteration (ms): 411.0 | learning rate: 2.150E-05 | global batch size:    32 | lm loss: 1.040837E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.866 | TFLOPs: 4.25 |
time (ms) | forward-compute: 139.40 | backward-compute: 265.86 | backward-embedding-all-reduce: 0.01 | optimizer: 4.22 | batch-generator: 4.34
[2023-03-17 12:15:50,088] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:50,088] [INFO] [logging.py:93:log_dist] [Rank 0] step=33, skipped=0, lr=[1.999265079925368e-05, 1.999265079925368e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:50,089] [INFO] [timer.py:198:stop] epoch=0/micro_step=33/global_step=33, RunningAvgSamplesPerSec=598.878136005192, CurrSamplesPerSec=604.6361084957722, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:50,089] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 131.83 | backward_microstep: 265.37 | backward_inner_microstep: 178.42 | backward_allreduce_microstep: 86.51 | step_microstep: 4.26
[2023-03-17 12:15:50,090] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 131.76 | backward: 265.35 | backward_inner: 178.37 | backward_allreduce: 86.52 | step: 4.26
 iteration       33/      50 | consumed samples:         8448 | consumed tokens:      8650752 | elapsed time per iteration (ms): 411.0 | learning rate: 1.999E-05 | global batch size:    32 | lm loss: 1.038328E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.868 | TFLOPs: 4.25 |
time (ms) | forward-compute: 139.18 | backward-compute: 266.10 | backward-embedding-all-reduce: 0.01 | optimizer: 4.17 | batch-generator: 4.22
[2023-03-17 12:15:50,498] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:50,499] [INFO] [logging.py:93:log_dist] [Rank 0] step=34, skipped=0, lr=[1.853267653556708e-05, 1.853267653556708e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:50,499] [INFO] [timer.py:198:stop] epoch=0/micro_step=34/global_step=34, RunningAvgSamplesPerSec=598.9561622111148, CurrSamplesPerSec=601.3850999861099, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:50,499] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 131.62 | backward_microstep: 263.47 | backward_inner_microstep: 178.48 | backward_allreduce_microstep: 84.55 | step_microstep: 4.05
[2023-03-17 12:15:50,500] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 131.55 | backward: 263.45 | backward_inner: 178.42 | backward_allreduce: 84.58 | step: 4.06
 iteration       34/      50 | consumed samples:         8704 | consumed tokens:      8912896 | elapsed time per iteration (ms): 410.2 | learning rate: 1.853E-05 | global batch size:    32 | lm loss: 1.034368E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 78.010 | TFLOPs: 4.26 |
time (ms) | forward-compute: 139.96 | backward-compute: 264.38 | backward-embedding-all-reduce: 0.01 | optimizer: 4.35 | batch-generator: 4.29
[2023-03-17 12:15:50,909] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.64
[2023-03-17 12:15:50,909] [INFO] [logging.py:93:log_dist] [Rank 0] step=35, skipped=0, lr=[1.712979818810323e-05, 1.712979818810323e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:50,909] [INFO] [timer.py:198:stop] epoch=0/micro_step=35/global_step=35, RunningAvgSamplesPerSec=599.012141086148, CurrSamplesPerSec=600.8090064683632, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:50,910] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.31 | backward_microstep: 262.03 | backward_inner_microstep: 178.47 | backward_allreduce_microstep: 83.12 | step_microstep: 3.96
[2023-03-17 12:15:50,910] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.24 | backward: 262.01 | backward_inner: 178.42 | backward_allreduce: 83.14 | step: 3.96
 iteration       35/      50 | consumed samples:         8960 | consumed tokens:      9175040 | elapsed time per iteration (ms): 410.6 | learning rate: 1.713E-05 | global batch size:    32 | lm loss: 1.031691E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.938 | TFLOPs: 4.25 |
time (ms) | forward-compute: 141.51 | backward-compute: 263.06 | backward-embedding-all-reduce: 0.01 | optimizer: 4.37 | batch-generator: 4.38
[2023-03-17 12:15:51,318] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:51,318] [INFO] [logging.py:93:log_dist] [Rank 0] step=36, skipped=0, lr=[1.5789552276785377e-05, 1.5789552276785377e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:51,319] [INFO] [timer.py:198:stop] epoch=0/micro_step=36/global_step=36, RunningAvgSamplesPerSec=599.2952699512714, CurrSamplesPerSec=608.7910516814384, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:51,319] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.02 | backward_microstep: 261.27 | backward_inner_microstep: 177.98 | backward_allreduce_microstep: 82.84 | step_microstep: 4.01
[2023-03-17 12:15:51,319] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 131.94 | backward: 261.24 | backward_inner: 177.93 | backward_allreduce: 82.86 | step: 4.01
 iteration       36/      50 | consumed samples:         9216 | consumed tokens:      9437184 | elapsed time per iteration (ms): 409.2 | learning rate: 1.579E-05 | global batch size:    32 | lm loss: 1.029127E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 78.209 | TFLOPs: 4.27 |
time (ms) | forward-compute: 140.95 | backward-compute: 262.36 | backward-embedding-all-reduce: 0.01 | optimizer: 4.26 | batch-generator: 4.20
[2023-03-17 12:15:51,728] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:51,728] [INFO] [logging.py:93:log_dist] [Rank 0] step=37, skipped=0, lr=[1.4517228139925405e-05, 1.4517228139925405e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:51,728] [INFO] [timer.py:198:stop] epoch=0/micro_step=37/global_step=37, RunningAvgSamplesPerSec=599.3630366506042, CurrSamplesPerSec=601.676258444545, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:51,729] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 131.79 | backward_microstep: 261.74 | backward_inner_microstep: 177.80 | backward_allreduce_microstep: 83.50 | step_microstep: 4.12
[2023-03-17 12:15:51,729] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 131.71 | backward: 261.71 | backward_inner: 177.75 | backward_allreduce: 83.52 | step: 4.12
 iteration       37/      50 | consumed samples:         9472 | consumed tokens:      9699328 | elapsed time per iteration (ms): 409.7 | learning rate: 1.452E-05 | global batch size:    32 | lm loss: 1.026351E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 78.107 | TFLOPs: 4.26 |
time (ms) | forward-compute: 140.51 | backward-compute: 263.17 | backward-embedding-all-reduce: 0.01 | optimizer: 4.41 | batch-generator: 4.27
[2023-03-17 12:15:52,138] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.64
[2023-03-17 12:15:52,138] [INFO] [logging.py:93:log_dist] [Rank 0] step=38, skipped=0, lr=[1.3317847059621894e-05, 1.3317847059621894e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:52,138] [INFO] [timer.py:198:stop] epoch=0/micro_step=38/global_step=38, RunningAvgSamplesPerSec=599.5813967686544, CurrSamplesPerSec=607.3255323577589, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:52,139] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.19 | backward_microstep: 262.94 | backward_inner_microstep: 177.93 | backward_allreduce_microstep: 84.57 | step_microstep: 4.01
[2023-03-17 12:15:52,139] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.12 | backward: 262.92 | backward_inner: 177.88 | backward_allreduce: 84.60 | step: 4.03
 iteration       38/      50 | consumed samples:         9728 | consumed tokens:      9961472 | elapsed time per iteration (ms): 410.0 | learning rate: 1.332E-05 | global batch size:    32 | lm loss: 1.024347E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 78.051 | TFLOPs: 4.26 |
time (ms) | forward-compute: 140.05 | backward-compute: 263.99 | backward-embedding-all-reduce: 0.01 | optimizer: 4.28 | batch-generator: 4.37
[2023-03-17 12:15:52,547] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:52,547] [INFO] [logging.py:93:log_dist] [Rank 0] step=39, skipped=0, lr=[1.2196142445053694e-05, 1.2196142445053694e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:52,548] [INFO] [timer.py:198:stop] epoch=0/micro_step=39/global_step=39, RunningAvgSamplesPerSec=599.8172227978223, CurrSamplesPerSec=608.4322834502892, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:52,548] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.31 | backward_microstep: 261.62 | backward_inner_microstep: 177.79 | backward_allreduce_microstep: 83.38 | step_microstep: 4.03
[2023-03-17 12:15:52,548] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.24 | backward: 261.59 | backward_inner: 177.74 | backward_allreduce: 83.40 | step: 4.04
 iteration       39/      50 | consumed samples:         9984 | consumed tokens:     10223616 | elapsed time per iteration (ms): 409.4 | learning rate: 1.220E-05 | global batch size:    32 | lm loss: 1.021817E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 78.160 | TFLOPs: 4.27 |
time (ms) | forward-compute: 140.92 | backward-compute: 262.64 | backward-embedding-all-reduce: 0.01 | optimizer: 4.26 | batch-generator: 4.26
[2023-03-17 12:15:52,959] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:52,960] [INFO] [logging.py:93:log_dist] [Rank 0] step=40, skipped=0, lr=[1.1156541151876422e-05, 1.1156541151876422e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:52,960] [INFO] [timer.py:198:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=599.892526669379, CurrSamplesPerSec=602.6921242764832, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:52,960] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.51 | backward_microstep: 264.92 | backward_inner_microstep: 177.77 | backward_allreduce_microstep: 86.70 | step_microstep: 4.27
[2023-03-17 12:15:52,961] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.42 | backward: 264.90 | backward_inner: 177.72 | backward_allreduce: 86.72 | step: 4.27
 iteration       40/      50 | consumed samples:        10240 | consumed tokens:     10485760 | elapsed time per iteration (ms): 412.3 | learning rate: 1.116E-05 | global batch size:    32 | lm loss: 1.019481E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.608 | TFLOPs: 4.24 |
time (ms) | forward-compute: 141.21 | backward-compute: 264.83 | backward-embedding-all-reduce: 0.01 | optimizer: 4.73 | batch-generator: 4.25
[2023-03-17 12:15:53,372] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:53,372] [INFO] [logging.py:93:log_dist] [Rank 0] step=41, skipped=0, lr=[1.0203146011445599e-05, 1.0203146011445599e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:53,373] [INFO] [timer.py:198:stop] epoch=0/micro_step=41/global_step=41, RunningAvgSamplesPerSec=599.9691438038255, CurrSamplesPerSec=602.8951675934993, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:53,373] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.60 | backward_microstep: 263.90 | backward_inner_microstep: 177.81 | backward_allreduce_microstep: 85.64 | step_microstep: 3.89
[2023-03-17 12:15:53,373] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.52 | backward: 263.88 | backward_inner: 177.76 | backward_allreduce: 85.66 | step: 3.89
 iteration       41/      50 | consumed samples:        10496 | consumed tokens:     10747904 | elapsed time per iteration (ms): 412.6 | learning rate: 1.020E-05 | global batch size:    32 | lm loss: 1.017328E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.553 | TFLOPs: 4.23 |
time (ms) | forward-compute: 142.03 | backward-compute: 264.48 | backward-embedding-all-reduce: 0.01 | optimizer: 4.46 | batch-generator: 4.28
[2023-03-17 12:15:53,785] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:53,785] [INFO] [logging.py:93:log_dist] [Rank 0] step=42, skipped=0, lr=[9.33971963881569e-06, 9.33971963881569e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:53,786] [INFO] [timer.py:198:stop] epoch=0/micro_step=42/global_step=42, RunningAvgSamplesPerSec=599.7266420961655, CurrSamplesPerSec=590.419608843687, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:53,786] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 134.21 | backward_microstep: 262.79 | backward_inner_microstep: 177.80 | backward_allreduce_microstep: 84.55 | step_microstep: 4.70
[2023-03-17 12:15:53,786] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 134.13 | backward: 262.77 | backward_inner: 177.74 | backward_allreduce: 84.56 | step: 4.71
 iteration       42/      50 | consumed samples:        10752 | consumed tokens:     11010048 | elapsed time per iteration (ms): 412.9 | learning rate: 9.340E-06 | global batch size:    32 | lm loss: 1.014726E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.500 | TFLOPs: 4.23 |
time (ms) | forward-compute: 142.36 | backward-compute: 264.02 | backward-embedding-all-reduce: 0.01 | optimizer: 4.90 | batch-generator: 4.17
[2023-03-17 12:15:54,195] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:54,195] [INFO] [logging.py:93:log_dist] [Rank 0] step=43, skipped=0, lr=[8.569669583417477e-06, 8.569669583417477e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:54,195] [INFO] [timer.py:198:stop] epoch=0/micro_step=43/global_step=43, RunningAvgSamplesPerSec=599.714104650249, CurrSamplesPerSec=599.2130362962632, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:54,196] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.19 | backward_microstep: 263.00 | backward_inner_microstep: 178.03 | backward_allreduce_microstep: 84.51 | step_microstep: 4.75
[2023-03-17 12:15:54,196] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.11 | backward: 262.97 | backward_inner: 177.98 | backward_allreduce: 84.53 | step: 4.75
 iteration       43/      50 | consumed samples:        11008 | consumed tokens:     11272192 | elapsed time per iteration (ms): 409.8 | learning rate: 8.570E-06 | global batch size:    32 | lm loss: 1.013476E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 78.085 | TFLOPs: 4.26 |
time (ms) | forward-compute: 139.33 | backward-compute: 263.77 | backward-embedding-all-reduce: 0.01 | optimizer: 5.04 | batch-generator: 4.29
[2023-03-17 12:15:54,603] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.66
[2023-03-17 12:15:54,603] [INFO] [logging.py:93:log_dist] [Rank 0] step=44, skipped=0, lr=[7.896034881017213e-06, 7.896034881017213e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:54,603] [INFO] [timer.py:198:stop] epoch=0/micro_step=44/global_step=44, RunningAvgSamplesPerSec=599.7426796458279, CurrSamplesPerSec=600.9166036130823, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:54,604] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.21 | backward_microstep: 261.60 | backward_inner_microstep: 177.80 | backward_allreduce_microstep: 83.35 | step_microstep: 3.89
[2023-03-17 12:15:54,604] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.14 | backward: 261.58 | backward_inner: 177.76 | backward_allreduce: 83.37 | step: 3.90
 iteration       44/      50 | consumed samples:        11264 | consumed tokens:     11534336 | elapsed time per iteration (ms): 408.1 | learning rate: 7.896E-06 | global batch size:    32 | lm loss: 1.011820E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 78.413 | TFLOPs: 4.28 |
time (ms) | forward-compute: 139.74 | backward-compute: 262.43 | backward-embedding-all-reduce: 0.01 | optimizer: 4.27 | batch-generator: 4.16
[2023-03-17 12:15:55,012] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:55,013] [INFO] [logging.py:93:log_dist] [Rank 0] step=45, skipped=0, lr=[7.3214740600308545e-06, 7.3214740600308545e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:55,013] [INFO] [timer.py:198:stop] epoch=0/micro_step=45/global_step=45, RunningAvgSamplesPerSec=599.8096967269449, CurrSamplesPerSec=602.6380024874617, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:55,013] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.15 | backward_microstep: 261.78 | backward_inner_microstep: 177.87 | backward_allreduce_microstep: 83.46 | step_microstep: 4.14
[2023-03-17 12:15:55,014] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.08 | backward: 261.76 | backward_inner: 177.82 | backward_allreduce: 83.49 | step: 4.14
 iteration       45/      50 | consumed samples:        11520 | consumed tokens:     11796480 | elapsed time per iteration (ms): 409.5 | learning rate: 7.321E-06 | global batch size:    32 | lm loss: 1.009276E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 78.142 | TFLOPs: 4.27 |
time (ms) | forward-compute: 140.86 | backward-compute: 262.64 | backward-embedding-all-reduce: 0.01 | optimizer: 4.37 | batch-generator: 4.38
[2023-03-17 12:15:55,423] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.66
[2023-03-17 12:15:55,423] [INFO] [logging.py:93:log_dist] [Rank 0] step=46, skipped=0, lr=[6.848254649526961e-06, 6.848254649526961e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:55,424] [INFO] [timer.py:198:stop] epoch=0/micro_step=46/global_step=46, RunningAvgSamplesPerSec=599.9414474022395, CurrSamplesPerSec=605.6620022111415, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:55,424] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 131.94 | backward_microstep: 262.94 | backward_inner_microstep: 177.75 | backward_allreduce_microstep: 84.75 | step_microstep: 4.08
[2023-03-17 12:15:55,424] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 131.86 | backward: 262.91 | backward_inner: 177.70 | backward_allreduce: 84.77 | step: 4.08
 iteration       46/      50 | consumed samples:        11776 | consumed tokens:     12058624 | elapsed time per iteration (ms): 410.6 | learning rate: 6.848E-06 | global batch size:    32 | lm loss: 1.008490E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.927 | TFLOPs: 4.25 |
time (ms) | forward-compute: 140.74 | backward-compute: 263.70 | backward-embedding-all-reduce: 0.01 | optimizer: 4.44 | batch-generator: 4.21
[2023-03-17 12:15:55,837] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.66
[2023-03-17 12:15:55,837] [INFO] [logging.py:93:log_dist] [Rank 0] step=47, skipped=0, lr=[6.478244230325408e-06, 6.478244230325408e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:55,837] [INFO] [timer.py:198:stop] epoch=0/micro_step=47/global_step=47, RunningAvgSamplesPerSec=600.0707948283293, CurrSamplesPerSec=605.8178272879918, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:55,837] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 133.41 | backward_microstep: 264.01 | backward_inner_microstep: 177.92 | backward_allreduce_microstep: 85.65 | step_microstep: 3.95
[2023-03-17 12:15:55,838] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 133.34 | backward: 263.99 | backward_inner: 177.87 | backward_allreduce: 85.66 | step: 3.95
 iteration       47/      50 | consumed samples:        12032 | consumed tokens:     12320768 | elapsed time per iteration (ms): 413.4 | learning rate: 6.478E-06 | global batch size:    32 | lm loss: 1.006718E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 77.407 | TFLOPs: 4.23 |
time (ms) | forward-compute: 142.73 | backward-compute: 264.74 | backward-embedding-all-reduce: 0.01 | optimizer: 4.29 | batch-generator: 4.23
[2023-03-17 12:15:56,247] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:56,247] [INFO] [logging.py:93:log_dist] [Rank 0] step=48, skipped=0, lr=[6.2129030645091e-06, 6.2129030645091e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:56,247] [INFO] [timer.py:198:stop] epoch=0/micro_step=48/global_step=48, RunningAvgSamplesPerSec=600.0737006065452, CurrSamplesPerSec=600.2044897594133, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:56,247] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 131.95 | backward_microstep: 261.78 | backward_inner_microstep: 177.84 | backward_allreduce_microstep: 83.48 | step_microstep: 4.06
[2023-03-17 12:15:56,248] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 131.87 | backward: 261.76 | backward_inner: 177.79 | backward_allreduce: 83.51 | step: 4.06
 iteration       48/      50 | consumed samples:        12288 | consumed tokens:     12582912 | elapsed time per iteration (ms): 410.0 | learning rate: 6.213E-06 | global batch size:    32 | lm loss: 1.005616E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 78.041 | TFLOPs: 4.26 |
time (ms) | forward-compute: 141.05 | backward-compute: 263.00 | backward-embedding-all-reduce: 0.01 | optimizer: 4.39 | batch-generator: 4.26
[2023-03-17 12:15:56,655] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:56,655] [INFO] [logging.py:93:log_dist] [Rank 0] step=49, skipped=0, lr=[6.053278332436668e-06, 6.053278332436668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:56,656] [INFO] [timer.py:198:stop] epoch=0/micro_step=49/global_step=49, RunningAvgSamplesPerSec=600.1839889809228, CurrSamplesPerSec=605.3014517198302, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:56,656] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.35 | backward_microstep: 261.25 | backward_inner_microstep: 177.88 | backward_allreduce_microstep: 82.93 | step_microstep: 4.31
[2023-03-17 12:15:56,656] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.27 | backward: 261.23 | backward_inner: 177.83 | backward_allreduce: 82.95 | step: 4.31
 iteration       49/      50 | consumed samples:        12544 | consumed tokens:     12845056 | elapsed time per iteration (ms): 408.6 | learning rate: 6.053E-06 | global batch size:    32 | lm loss: 1.004196E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 78.312 | TFLOPs: 4.28 |
time (ms) | forward-compute: 139.70 | backward-compute: 262.75 | backward-embedding-all-reduce: 0.01 | optimizer: 4.50 | batch-generator: 4.39
[2023-03-17 12:15:57,064] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 0.65
[2023-03-17 12:15:57,064] [INFO] [logging.py:93:log_dist] [Rank 0] step=50, skipped=0, lr=[6e-06, 6e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-03-17 12:15:57,064] [INFO] [timer.py:198:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=600.3775482341916, CurrSamplesPerSec=609.6178264680901, MemAllocated=0.29GB, MaxMemAllocated=2.57GB
[2023-03-17 12:15:57,065] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 132.42 | backward_microstep: 261.86 | backward_inner_microstep: 178.06 | backward_allreduce_microstep: 83.37 | step_microstep: 4.02
[2023-03-17 12:15:57,065] [INFO] [logging.py:93:log_dist] [Rank 0] rank=0 time (ms) | forward: 132.35 | backward: 261.84 | backward_inner: 178.01 | backward_allreduce: 83.39 | step: 4.03
 iteration       50/      50 | consumed samples:        12800 | consumed tokens:     13107200 | elapsed time per iteration (ms): 408.4 | learning rate: 6.000E-06 | global batch size:    32 | lm loss: 1.002793E+01 | loss scale: 32768.0 | actual seqlen:  1024 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 78.363 | TFLOPs: 4.28 |
time (ms) | forward-compute: 139.81 | backward-compute: 262.63 | backward-embedding-all-reduce: 0.01 | optimizer: 4.28 | batch-generator: 4.40
[after training is done] datetime: 2023-03-17 12:15:57 
------------------------------------------------------------------------------------------------------------------
 validation loss at the end of training for val data | lm loss value: 9.955457E+00 | lm loss PPL: 2.106687E+04 | 
------------------------------------------------------------------------------------------------------------------
gpu109-23-r:58710:58790 [0] NCCL INFO [Service thread] Connection closed by localRank 0
gpu108-23-r:1142:1290 [0] NCCL INFO [Service thread] Connection closed by localRank 0
gpu109-02-l:65178:65260 [0] NCCL INFO [Service thread] Connection closed by localRank 0
gpu201-02-l:12282:12444 [0] NCCL INFO [Service thread] Connection closed by localRank 0
gpu201-02-r:15165:15278 [0] NCCL INFO [Service thread] Connection closed by localRank 0
gpu109-02-r:49201:49464 [0] NCCL INFO [Service thread] Connection closed by localRank 0
gpu109-16-r:24876:24958 [0] NCCL INFO [Service thread] Connection closed by localRank 0
gpu109-16-l:53482:53575 [0] NCCL INFO [Service thread] Connection closed by localRank 0
gpu109-23-r:58710:58710 [0] NCCL INFO comm 0x3b803cd0 rank 5 nranks 8 cudaDev 0 busId 7000 - Abort COMPLETE
gpu108-23-r:1142:1142 [0] NCCL INFO comm 0x3b497d00 rank 0 nranks 8 cudaDev 0 busId 7000 - Abort COMPLETE
gpu201-02-r:15165:15165 [0] NCCL INFO comm 0x39a525c0 rank 7 nranks 8 cudaDev 0 busId 85000 - Abort COMPLETE
gpu201-02-l:12282:12282 [0] NCCL INFO comm 0x38e1db40 rank 6 nranks 8 cudaDev 0 busId 7000 - Abort COMPLETE
gpu109-02-l:65178:65178 [0] NCCL INFO comm 0x3a3c8a40 rank 1 nranks 8 cudaDev 0 busId 7000 - Abort COMPLETE
gpu109-02-r:49201:49201 [0] NCCL INFO comm 0x3cd24180 rank 2 nranks 8 cudaDev 0 busId c7000 - Abort COMPLETE
gpu109-16-r:24876:24876 [0] NCCL INFO comm 0x39d09b90 rank 4 nranks 8 cudaDev 0 busId 46000 - Abort COMPLETE
gpu109-16-l:53482:53482 [0] NCCL INFO comm 0x3a1d37c0 rank 3 nranks 8 cudaDev 0 busId 7000 - Abort COMPLETE
