#!/bin/bash
#SBATCH --job-name=hello_ds
#SBATCH --gres=gpu:2
#SBATCH --cpus-per-task=4
#SBATCH --ntasks=4
#SBATCH --nodes=2
#SBATCH --time=00:15:00
##SBATCH --account=ibex-cs
##SBATCH --constraint=v100,gpu_ai
#SBATCH --constraint=a100
#SBATCH --reservation=A100
#SBATCH --account=a100_training_acc 
#SBATCH --partition=a100_training

echo "------- JOB Configuration ---------"
echo "scontrol show job ${SLURM_JOBID}"
scontrol show job ${SLURM_JOBID}

echo "------- GPU Configuration ---------"
echo "nvidia-smi -L"
nvidia-smi -L

echo "------- NVLink Configuration ------"
echo "nvidia-smi topo -m"
nvidia-smi topo -m

echo "------- Infiniband Configuration --"
echo "ibv_devinfo"
ibv_devinfo

rm -rf bert_pretrain*

module load dl
module load deepspeed/0.8.3
module load apex/22.03
module list

#export NCCL_DEBUG=INFO
#export NCCL_TREE_THRESHOLD=0 
export NCCL_SOCKET_IFNAME=ib0
#export NCCL_NET_GDR_LEVEL=4
#export NCCL_TOPO_DUMP_FILE=./nccl_dump.log.${SLURM_JOB_NAME}
#export MAX_JOBS=4

HF=hostfile-${SLURM_JOB_NAME}
rm ${HF}
nodes=$(scontrol show hostnames "$SLURM_JOB_NODELIST")
nodes_array=($nodes)

for ((i = 0; i < ${SLURM_NNODES}; i++)); do
   node_i=${nodes_array[$i]}
   echo $node_i slots=${SLURM_GPUS_ON_NODE} >> ${HF}
done

master_ip=$(srun -n 1 -N 1 --gpus=1 -w ${nodes_array[0]} /bin/hostname -I | cut -d " " -f 2)


#deepspeed --launcher='slurm' --num_nodes=${SLURM_NNODES} --num_gpus=${SLURM_GPUS}  --hostfile ${HF} --master_addr=${master_ip} \
#train_bert_ds.py --checkpoint_dir $PWD

srun -n ${SLURM_NTASKS} -N ${SLURM_NNODES} python -W ignore train_bert_ds.py  --checkpoint_dir $PWD 

